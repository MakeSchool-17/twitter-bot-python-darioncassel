Operationalism
Operationalism is based on the intuition that we do not know the meaning of a concept unless we have a method of measurement for it. It is commonly considered a theory of meaning which states that “we mean by any concept nothing more than a set of operations; the concept is synonymous with the corresponding set of operations” (Bridgman 1927, 5). That drastic statement was made in The Logic of Modern Physics, published in 1927 by the American physicist P. W. Bridgman. The operationalist point of view, first expounded at length in that book, initially found many advocates among practicing physicists and those inspired by the tradition of American pragmatism or the new philosophy of logical positivism. It is highly doubtful that Bridgman intended to advance a precise and universal theory of meaning, or any systematic philosophical theory at all. His writings were primarily “reflections of a physicist”[1] rooted in experimental practice and aimed at articulating the scientific method from a first-person point of view. However, as Bridgman's ideas gained currency they were shaped into a general philosophical doctrine of “operationalism” or “operationism”, and in that form became very influential in many areas, especially in methodological debates in psychology. Both in philosophy and in psychology operationalism is nowadays commonly regarded as an extreme and outmoded position, but that is not to say that the potential of Bridgman's original ideas has been exhausted.
This article has three sections, each of which serves a different aim. Section 1 introduces Bridgman's key ideas on operational analysis, explaining their motivations and tracing the course of their development. Section 2 summarizes various critiques of operationalism, which eventually led to a general philosophical consensus against it. Section 3 gives a view on the remaining potential of Bridgman's ideas on operational analysis for philosophy of science today.
Percy Williams Bridgman (1882–1961) was a physicist at Harvard University whose pioneering work in the physics of high pressures was rewarded with a Nobel Prize in 1946.[2] His chief scientific contribution was made possible by technical prowess: in his laboratory Bridgman created pressures nearly 100 times higher than anyone else had achieved before him, and investigated the novel behavior of various materials under such high pressures. But Bridgman was placed in a predicament by his own achievements: at such extreme pressures, all previously known pressure gauges broke down; how was he even to know what levels of pressure he had in fact reached? (see Kemble, Birch and Holton 1970) As he kept breaking his own pressure records, Bridgman had to establish a succession of new measures fit for higher and higher pressures. Therefore it is no surprise that he thought seriously about the groundlessness of concepts where no methods were available for their measurement.
Another important stimulus to his philosophical thinking was his encounter with the revolutionary new physics of the early 20th century. Bridgman's concerns about the definition and meaning of scientific concepts were forged in the general climate of shock suffered by physicists at that time from a barrage of phenomena and theoretical ideas that were entirely alien to everyday expectations, culminating with quantum mechanics and its “Copenhagen” interpretation. In a popular article, Bridgman wrote: “if we sufficiently extend our range we shall find that nature is intrinsically and in its elements neither understandable nor subject to law” (1929, 444).
Especially important for Bridgman's thinking was Albert Einstein's special theory of relativity. Bridgman credited an unexpected teaching assignment in 1914 for his first real encounter with special relativity, which gave him considerable distress as he tried to clarify the confusing conceptual situation surrounding the theory (Bridgman in Frank 1956, 76). At the heart of special relativity was Einstein's recognition that judging the simultaneity of two events separated in space required a different operation from that required for judging the simultaneity of two events happening at the same place. Fixing the latter operation was not sufficient to fix the former, so a further convention was necessary, which Einstein supplied in the form of his operation of sending light beams from each of the events in question to the midpoint between their locations, to see if they arrive there at the same time. How superior this way of thinking was, compared to Isaac Newton's declaration that he would “not define Time, Space, Place or Motion, as being well known to all” (quoted in Bridgman 1927, 4)! Bridgman felt that all physicists, including himself, had been guilty of unthinking extensions of concepts, especially on the theoretical side of physics.
Bridgman's sentiment arising out of these reflections, however, was not the familiar one of happy celebration of Einstein's genius. He rather regretted the sorry state of physics which had necessitated Einstein's revolution. Einstein showed what dangerous traps we could fall into by stepping into new domains with old concepts in an unreflective way. Anyone thinking in operational terms would have recognized from the start that the meaning of “distant simultaneity” was not fixed unless an operation for judging it was specified (Bridgman 1927, 10–16). In Bridgman's view, Einstein's revolution would never have been necessary, if classical physicists had paid operational attention to what they were doing. He thought that any future toppling of unsound structures would become unnecessary if the operational way of thinking could quietly prevent such unsound structures in the first place. Operational awareness was required if physics was not to be caught off-guard again as it had been in 1905: “We must remain aware of these joints in our conceptual structure if we hope to render unnecessary the services of the unborn Einsteins” (Bridgman 1927, 24).
Bridgman's impulse was to specify every possible detail of his operations, because any detail could make an important difference. Note the following passage, inspired by the shock of learning from the special theory of relativity that the measured length of an object was not independent of its velocity:
Now suppose we have to measure a moving street car. The simplest, and what we may call the “naïve” procedure, is to board the car with our meter stick and repeat the operations we would apply to a stationary body. Notice that this procedure reduces to that already adopted in the limiting case when the velocity of the street car vanishes. But here there may be new questions of detail. How shall we jump on to the car with our stick in hand? Shall we run and jump on from behind, or shall we let it pick us up in front? Or perhaps does now the material of which the stick is composed make a difference, although previously it did not? All these questions must be answered by experiment. (Bridgman 1927, 11; emphasis added)
Bridgman found that the challenges of the unknown were amply present even in very prosaic circumstances. Therefore he chose to open his discussion of operational analysis in The Logic of Modern Physics (Bridgman 1927) with the example of the most mundane of all scientific concepts: length. He was both fascinated and troubled by the fact that “essential physical limitations” forced scientists to use different measurement operations for the same concept in different domains of phenomena. Length is measured with a ruler only when we are dealing with dimensions that are comparable to our human bodies, and when the objects of measurement are moving slowly relative to the measurer. To measure, say, the distance to the moon, we need to infer it from the amount of time that light takes to travel that distance and return, and that is also the procedure taken up in Einstein's theorizing in special relativity; “the space of astronomy is not a physical space of meter sticks, but is a space of light waves” (Bridgman 1927, 67). For even larger distances we use the unit of “light-year,” but we cannot actually use the operation of sending off a light beam to a distant speck of light in the sky and waiting for years on end until hopefully a reflected signal comes back to us (or our descendants). Much more complex reasoning and operations are required for measuring any distances beyond the solar system:
Thus at greater and greater distances not only does experimental accuracy become less, but the very nature of the operations by which length is to be determined becomes indefinite…. To say that a certain star is 105 light years distant is actually and conceptually an entire different kind of thing from saying that a certain goal post is 100 meters distant. (Bridgman 1927, 17–18; emphasis original)
Thus operational analysis reveals that length is not one homogeneous concept that applies in the whole range in which we use it:
In principle the operations by which length is measured should be uniquely specified. If we have more than one set of operations, we have more than one concept, and strictly there should be a separate name to correspond to each different set of operations. (Bridgman 1927, 10; emphases original)
In practice scientists did not recognize multiple concepts of length, and Bridgman was willing to concede that it was allowable to use the same name to represent a series of concepts, if the different measurement operations gave mutually consistent numerical results in the areas of overlap:
If we deal with phenomena outside the domain in which we originally defined our concepts, we may find physical hindrances to performing the operations of the original definition, so that the original operations have to be replaced by others. These new operations are, of course, to be so chosen that they give, within experimental error, the same numerical results in the domain in which the two sets of operations may be both applied. (Bridgman 1927, 23)
However, such numerical convergence between the results of two different operations was regarded by Bridgman as merely a “practical justification for retaining the same name” for what the two operations measured (Bridgman 1927, 16).
Even in such convergent situations, we have to be wary of the danger of slipping into conceptual confusion through the use of the same word to refer to the subjects of different operations. If we do not temper our thoughts with the operationalist conscience always referring us back to concrete measurement operations, we may get into the sloppy habit of using one word for all sorts of different situations (without even checking for the required convergence in the overlapping domains). Bridgman warned: “our verbal machinery has no built-in cutoff” (1959a, 75). Similarly we could be misled by the representation of a concept as a number into thinking that there is naturally an infinitely extendable scale for that concept, since the real-number line continues on to infinity in both directions. It would also be easy to think that physical quantities must meaningfully exist down to infinite precision, just because the numerical scale we have pinned on them is infinitely divisible. Bridgman issued a stark reminder:
Mathematics does not recognize that as the physical range increases, the fundamental concepts become hazy, and eventually cease entirely to have physical meaning, and therefore must be replaced by other concepts which are operationally quite different. For instance, the equations of motion make no distinction between the motion of a star into our galaxy from external space, and the motion of an electron about the nucleus, although physically the meaning in terms of operations of the quantities in the equations is entirely different in the two cases. The structure of our mathematics is such that we are almost forced, whether we want to or not, to talk about the inside of an electron, although physically we cannot assign any meaning to such statements. (Bridgman 1927, 63)
Bridgman thus emphasized that our concepts did not automatically extend beyond the domains in which they were originally defined. He warned that concepts in far-out domains could easily become meaningless for lack of applicable measurement operations. The case of length in the very small scale makes that danger clear. Beyond the resolution of the eye, the ruler has to be given up in favor of various micrometers and microscopes. When we get to the realm of atoms and elementary particles, it is not clear what operations could be used to measure length, and not even clear what “length” means any more.
After introducing operational analysis with that refreshing discussion of the length concept, Bridgman published a long series of critical re-assessments of various fundamental physical concepts. His ruminations on length were extended into general commentary on the nature of space, and the concept of time received a similar treatment. His views on space and time were reminiscent of Henri Poincaré's and Pierre Duhem's: Bridgman noted that clocks had to be used in the empirical determination of the basic laws of physics, but our confidence that a clock ticks regularly was founded in the basic laws of physics governing its mechanism. Suppose we are trying to test the general theory of relativity by measuring the red-shift of light coming out of a heavy body:
If the vibrating atom is a clock, then the light of the sun is shifted toward the infra-red, but how do we know that the atom is a clock (some say yes, others no)? If we find the displacement physically have we thereby proved that general relativity is physically true, or have we proved that the atom is a clock, or have we merely proved that there is a particular kind of connection between the atom and the rest of nature, leaving the possibility open that neither is the atom a clock nor general relativity true? (Bridgman 1927, 72–73)
Bridgman found these reflections liberating as well as troubling. Basic space-time concepts are not uniquely determined a priori. For example, he noted that the definition of velocity shared in classical mechanics and special relativity was not the only one in line with our intuitions of what velocity meant. Consider this alternative: “a traveler in an automobile measures his velocity by observing the clock on his instrument board and the mile stones which he passes on the road.” If we adopt such a procedure we will find that the speed of light is infinite, if special relativity is correct about time dilation: with the car going at the speed of light according to an observer standing on the road, the clock on the car will not advance at all while the car passes any number of milestones. This alternative concept of velocity would have the advantage that “there would be no limit to the velocity which can be imparted to material bodies on giving them unlimited energy”, which seems intuitively “natural and simple”. But assigning an infinite velocity to light is also “most unnatural, particularly if we favor a medium point of view.” So there was a dilemma: “all sorts of phenomena cannot at the same time be treated simply.” (Bridgman 1927, 98–100)
In the latter parts of The Logic of Modern Physics Bridgman gave a fascinating array of discussions on the concepts of force, mass, energy, light, field, and more generally the theories of thermodynamics, relativity, and quantum mechanics. These thoughts were developed further in the remaining decades of his life, and collected in subsequent volumes including The Nature of Physical Theory (1936), The Nature of Thermodynamics (1941), Reflections of a Physicist (1950, second edition 1955), The Nature of Some of Our Physical Concepts (1952), and The Way Things Are (1959a). Bridgman made a searching examination of the familiar concepts of classical physics, checking to see if they retained operational meaning in domains of phenomena that were unfamiliar to the creators of classical physics. In some cases his analysis showed that the classical concepts were operationally unsound even in the contexts in which they were originally created. Later in life he stated that his initial foray into philosophy was motivated by his “disquietude” about physics, especially electrodynamics and thermodynamics, in which “the fundamental understanding of even the acknowledged leaders in physics were inadequate” (Bridgman 1959b, 519). In contrast, he thought that on the whole the contemporary development of quantum theory went in the right direction, especially in Werner Heisenberg's version of it, which discarded classical concepts where they did not apply (e.g., space-time orbits for electrons), and crafted new concepts with clear operational meaning in new domains of phenomena. However, he was not entirely satisfied with Niels Bohr's doctrine that all the operations of physics needed to be explained in the “macroscopic language of daily life or of present-day philosophy”; rather, he thought we needed to develop a “more adequate macroscopic language” (1959b, 526).
Interestingly, Bridgman never stopped thinking about relativity. The operationalist lesson he had taken from Einstein was so dear to him that he did not shrink from criticizing Einstein himself when the latter seemed to betray his own principles in the general theory of relativity. Already in The Logic of Modern Physics he had opined: “I personally question whether the elements of Einstein's formulation, such as curvature of space-time, are closely enough connected with immediate physical experience ever to be accepted as an ultimate in a scheme of explanation, and I very much feel the need for a formulation in more intimate physical terms” (1927, 176). Years later, when Bridgman was invited by P. A. Schilpp to contribute to the Library of Living Philosophers volume on Einstein, he issued the following “indictment” against Einstein: “he has carried into general relativity theory precisely that uncritical, pre-Einsteinian point of view which he has so convincingly shown us, in his special theory, conceals the possibility of disaster” (Bridgman in Schilpp 1949, 354; reprinted in Bridgman 1955, 337). Einstein brushed aside Bridgman's objection, merely stating that for a formal system to qualify as a physical theory it was “not necessary to demand that all of its assertions can be independently interpreted and ‘tested’ ‘operationally’” (Einstein in Schilpp 1949, 679). This exchange is reminiscent of the one that Heisenberg reported, in which Einstein responded with bemused incomprehension to Heisenberg's protest that he was following a lesson from Einstein in treating only directly observable quantities in his matrix mechanics (Heisenberg 1971, 62–69). Bridgman also carried out further operational analysis of special relativity, and his late thoughts on the subject were published posthumously in A Sophisticate's Primer on Relativity (1962).
Bridgman's critique of concepts in physics also led naturally to a philosophical critique of some general conceptions underlying physics, such as simplicity, atomism, causality, determinism, and probability. He also gave critical considerations to mathematics and its application to the physical world. There was no stone Bridgman was willing to leave unturned in his relentless critique. He went as far as to declare: “arithmetic, so far as it purports to deal with actual physical objects, is also affected with the same penumbra of uncertainty as all other empirical science” (Bridgman 1927, 34).
Among physicists Bridgman's reflections found a strong echo, especially in the early days. Perhaps that was natural: it has been said, by Bridgman himself too, that operationalism arose from observing “physicists in action” (Bridgman in Frank 1956, 80). Gerald Holton (1995a, 224) recalls what an “electrifying experience” it was for himself and many other physicists to read The Logic of Modern Physics for the first time. The explanation Holton gives of the “immense power” of Bridgman's work is “not that the work brings to the reader a message never thought of before, but that it lays open, with clarity, what the reader has been trying to formulate on his or her own”.
Bridgman also extended his operationalist thinking by considering its implications outside physics. This was important to him at least from the time of The Logic of Modern Physics, in which he ventured: “many of the questions asked about social and philosophical subjects will be found to be meaningless when examined from the point of operations. It would doubtless conduce greatly to clarity of thought if the operational mode of thinking were adopted in all fields of inquiry as well as in the physical” (30–32). To Bridgman it was clear that “to adopt the operational point of view … means a far-reaching change in all our habits of thought”. He knew that in practice this would be a very difficult thing to do: “Operational thinking will at first prove to be an unsocial virtue; one will find oneself perpetually unable to understand the simplest conversation of one's friends, and will make oneself universally unpopular by demanding the meaning of apparently the simplest terms of every argument”. Perhaps this throwaway remark was a sign of things to come, as Bridgman would in the end find himself rather isolated and plagued by misunderstanding, even among those who found his philosophical ideas worth discussing, as we will see in Section 2.
Bridgman did not develop in detail his operationalist ideas in relation to any other science than physics, apparently content to leave that job to the specialists in the respective fields. Some others did take up Bridgman's call for operationalist reformulations of their fields, with interesting consequences. It could be said that operationalism did not change the practice of physics itself much from what it already was, and the physicists followed him only as far as he asserted what was common sense to them. Matters were different in other sciences that paid attention to Bridgman, and that was most significant and pronounced in the case of psychology.
Behaviorist psychologists took up operationalism (or operationism, as it was more often called in psychology) as a weapon in their fight against more traditional psychologists, especially those who prized introspection as the most important source of psychological knowledge. The Harvard psychologist Edwin Boring (1886–1968) saw Bridgman's philosophy as a modern substitute for positivism, and he seems to have coined the term “operationism” (see Walter 1990, 178). It was Boring's student Stanley Smith Stevens (1906–1973) who was perhaps the most aggressive promoter of operationism in psychology (Hardcastle 1994; Feest 2005). Stevens saw operationism as a sure method of increasing rigor in psychological experiments and discourse, asserting that “to experience is, for the purpose of science, to react discriminately”, since these reactions are what science can measure and record publically (quoted in Feest 2005, 136). Echoing Bridgman's take on Einstein, Stevens declared in 1935: “The revolution that will put an end to the possibility of revolutions is the one that defines a straightforward procedure for the definition and validation of concepts. … Such a procedure is the one which tests the meaning of concepts by appealing to the concrete operations by which the concept is determined. We may call it operationism” (quoted in Walter 1990, 180).
In his concrete research in psychology Stevens focused on psychophysics, starting with his Ph.D. dissertation on the perceived attributes of tones, written under Boring's supervision. Another notable operationist in psychology was Edward Chace Tolman (1886–1959), also a Harvard Ph.D., who taught for most of his life at the University of California at Berkeley. Starting with his research on problem-solving behavior in rats, Tolman gave an operational treatment of desire, for example operationalizing hunger in terms of “time since last feeding”. Tolman did not deny that desire was a subjective feeling, but insisted that doing scientific research on it required experimentally tractable operations that would allow scientists to get a hold on something related to that subjective experience (see Feest 2005, 136–138). In Gustav Bergmann's assessment, operationism helped behaviorism to move from its initial metaphysical Watsonian variety to its modern version (Bergmann in Frank 1956, 53).
Despite the strong popularity of operationist behaviorism in certain quarters, it never commanded a complete consensus even in American psychology. Perhaps the most unexpected opposition came from Bridgman himself. Behaviorists were wanting to use operations to attain objectivity in psychology, which for them meant taking psychological discourse away from attempts to describe private experience. This was just the wrong move in Bridgman's view, as we will see in more detail in Section 2.4 below. Bridgman engaged in some discussion with Stevens but discovered that the latter's enthusiasm for “operational ideas” was really for something that he could not agree with. By 1936 he declared privately: “I have rather washed my hands of him” (quoted in Walter 1990, 184). Bridgman's disagreement with B. F. Skinner (1904–1990) was even more severe, and resulted in a prolonged dispute between the two (Holton 2005; Walter 1990, 188–192). Operationism became a subject of great controversy in psychology, epitomized in a 1945 special issue of the Psychological Review devoted to a symposium on operationism suggested by Boring, who remained supportive of the movement at some critical distance. To some limited extent this debate still continues in psychology (see Feest 2005).
Despite the initial popularity of Bridgman's ideas, by the middle of the 20th century the common reactions among philosophers and philosophically-minded scientists were strongly critical. Operationalism received many high-profile debates, among them a symposium held at the annual meeting of the American Association for the Advancement of Science (AAAS) in 1953 (published in Frank 1956), and the Psychological Review issue mentioned above. On such occasions Bridgman attempted to refine and defend his views, but also found that the debate was moving in directions that both surprised and disturbed him. In his contribution to the AAAS symposium he exclaimed:
There would seem to be no reason why I am better fitted than anyone else to open this discussion. As I listened to the papers I felt that I have only a historical connection with this thing called “operationalism.” In short, I feel that I have created a Frankenstein, which has certainly got away from me. I abhor the word operationalism or operationism, which seems to imply a dogma, or at least a thesis of some kind. The thing I have envisaged is too simple to be dignified by so pretentious a name. (Bridgman in Frank 1956, 75–76)
Still, arguably it was not Bridgman's own ideas about operational analysis but the Frankenstein of operationalism that had a more significant impact in philosophy and science, so this survey of operationalism must deal with how other people responded to operationalism as they saw it. In the course of the discussion I will try to point out some places where there were clear misunderstandings of Bridgman's ideas, and also other places where Bridgman himself was ambivalent or ambiguous, rather than simply misunderstood.
Given the timing and contexts of the publication of Bridgman's ideas, the philosophical debates surrounding them were to a large extent framed in relation to logical positivism, which was just making its big impact on the American philosophical scene. In fact, no less than Herbert Feigl (1902–1988) came to Harvard with the express purpose of learning from Bridgman, despite the latter's warning that he did not have much to teach (Walter 1990, 164–165). Bridgman's insistence on operational meaningfulness had at least a surface resemblance to the logical positivists' verification theory of meaning. Bergmann thought Bridgman had given a “scientist's version” of the latter (Bergmann in Frank 1956, 55), and Carl Hempel regarded operationalism and logical positivism as “closely akin” to each other (Hempel in Frank 1956, 56). And it is not difficult to see how a kindred philosophical doctrine coming from a world-class scientist would have captured the logical positivists' imagination.
When subjected to the scrutiny of professional philosophers, however, Bridgman's ideas were soon exposed as unsystematic and undeveloped, as he freely admitted himself. Moreover, it became evident that his ideas did not help logical positivists solve the key problems that they were struggling with. After the initial fascination, the standard positivist (and post-positivist) reaction to operationalism was disappointment, and operationalism was often seen as a failed philosophy that did not live up to its promises.
Nowhere was the positivist disappointment with Bridgman sharper than in considerations of operationalism as a theory of meaning. There was a set of objections which together amounted to a complaint that operational definitions did not give a sufficient account of the meaning of concepts, even where there were operations clearly relevant to the concepts in question.
The core of the problem here is an overly restrictive notion of meaning, which reduces it to measurement; I will call this Bridgman's reductive doctrine of meaning. Although Bridgman was not proposing a general philosophical theory of meaning, he did make remarks that revealed an impulse to do so. Consider the following statement, the last part of which I have already quoted:
We evidently know what we mean by length if we can tell what the length of any and every object is, and for the physicist nothing more is required. To find the length of an object, we have to perform certain physical operations. The concept of length is therefore fixed when the operations by which length is measured are fixed: that is, the concept of length involves as much as and nothing more than the set of operations by which length is determined. In general, we mean by any concept nothing more than a set of operations; the concept is synonymous with the corresponding set of operations. (Bridgman 1927, 5)
Similarly, he also displayed an impulse to use operations to make a criterion of meaningfulness: “If a specific question has meaning, it must be possible to find operations by which an answer may be given to it.” (Bridgman 1927, 28)
One lesson we can take from Bridgman's troubles is that meaning is unruly and promiscuous. The kind of absolute control on the meaning of scientific concepts that Bridgman wished for is not possible. The most control that can be achieved is for the scientific community to agree on an explicit definition and to respect it. But even firm definitions can only constrain the uses of a concept. The entire world can agree to define length by the standard meter in Paris (or by the wavelength of a certain atomic radiation), and that still comes nowhere near exhausting all that we mean by length. Bridgman himself later specifically admitted that his statement that meanings were synonymous with operations was “obviously going too far when taken out of context” (1938, 117). Especially compared with the notion of “meaning as use,” often traced back to the later phase of Ludwig Wittgenstein's work,[3] it is easy to recognize the narrowness of Bridgman's initial ideas. Bridgman's later gloss on his ideas was in fact rather late-Wittgensteinian: “To know the meaning of a term used by me it is evident, I think, that I must know the conditions under which I would use the term” (1938, 116). Since measurement operations provide only one specific context in which a concept is used, operational definitions can only cover one particular aspect of meaning.
Recognizing the restrictiveness of Bridgman's early remarks on meaning gives us a useful framework for understanding one common objection to operationalism. As Donald Gillies (1972, 6–7) emphasizes, if we accept the most extreme kind of operationalism, there is no point in asking whether a measurement method is valid; if the measurement method defines the concept and there is nothing more to the meaning of the concept, the measurement method is automatically valid, as a matter of convention or even tautology. Metrological validity becomes an interesting question only if the concept possesses a broader meaning than the specification of the method of its measurement. Then the measurement method can be said to be valid if it coheres with the other aspects of the concept's meaning. That way we may also make a judgment about whether an operational definition (or any other kind of definition) is an appropriate one, depending on how well it coheres with other elements of the concept's meaning and how beneficially it controls other elements of meaning.
So far I have noted that an operational definition is not sufficient to express a concept's meaning fully. Going further than that, many critics of operationalism have argued that not every good scientific concept needs to have an operational definition. If operationalism means demanding that every concept and every inferential step should have an immediate operational significance, it constitutes an overly restrictive empiricism. At times Bridgman did appear to be making such a demand, as illustrated in the poignant episode (discussed in Section 1.3 above) in which he criticized Einstein for betraying his own operationalist lesson in the general theory of relativity. Einstein's view was that there was no reason for physicists to shrink from using a non-operational concept if that delivered good results.
Einstein was self-consciously opportunistic in his methodological eclecticism, but philosophers wanted to find a more general rationale for freeing scientific theorizing from operationalist micro-management. The crux of the problem here for the operationalist is that theoretical concepts are much too useful in science. Bridgman actually acknowledged from early on that there were good theoretical concepts that were not amenable to direct operationalization, illustrating the point with the example of stress and strain inside a solid body (1927, 53–54), and the wavefunction in quantum mechanics (Bridgman in Frank 1956, 79). Bridgman saw clearly that these theoretical concepts only had indirect connections with physical operations, but he did not see any problems with that. He in fact went so far as to say: “there need be no qualms that the operational point of view will ever place the slightest restriction on the freedom of the theoretical physicist to explore the consequences of any free mental construction” (Bridgman in Frank 1956, 79). All that was required was that the theoretical system touched the operational ground somewhere, eventually. However, in that case Bridgman's message was the same as Einstein's, as the physicist R. B. Lindsay pointed out (Lindsay in Frank 1956, 71–72).
Bridgman's position on the matter of theoretical concepts was complicated, and perhaps not entirely self-consistent (I will return to this point in Section 3.3). One common objection to operationalism is based on a misunderstanding that reveals an essential difference between Bridgman and most of his critics. It is often said that operationalism cannot be right because each scientific concept can be measured in various ways. This criticism is based on the presumption that the concept in question has unity, which means that its definition must also be unified. If there are a variety of measurement methods all of which apply to one concept, then measurement methods cannot be what supplies the unified definition; instead, some theoretical account must be given as to how the variety of operations under consideration serve to measure the same thing. Bridgman, in contrast, had no such presumption of conceptual unity. For him, the initial position to take was that if there are different methods of measurement we have different concepts, as he said about “tactual” and “optical” length being two different concepts. Now, it could be that there is one aspect of reality that different measurement operations all get at, but that is something to be demonstrated, not to be assumed at the outset. The possibility of unity can be entertained if the minimum condition of numerical convergence is met—that is, if two measurement operations have an overlapping range and their results agree in the overlap. Still, Bridgman maintained some skepticism about whether it was safe for us to infer real conceptual unity from such numerical convergence.
Bridgman's ambivalence about conceptual unity elicited a serious worry about the systematic import of scientific concepts and theories, most astutely expressed by Hempel (1966, 91–97). Bridgman's skeptical caution would result in an intolerable fragmentation of science, Hempel argued. It would result in “a proliferation of concepts of length, of temperature, and of all other scientific concepts that would not only be practically unmanageable, but theoretically endless.” Hempel's worry was that Bridgman's quest for safety was blinding him to one of the ultimate aims of science, “namely the attainment of a simple, systematically unified account of empirical phenomena” (Hempel 1966, 94). Along similar lines, Lindsay (1937, 458) had earlier argued that “such an isolation of concepts would defeat the very aim of physical science, which is to provide a simple and economical description” of physical experience “in terms of a minimum number of concepts”. Bridgman had serious doubts about the plausibility of such a simple, unified account of nature, as I will explain further in Section 3.4. But Hempel and others could genuinely envisage it. Hempel noted that with the development of science there was a continually growing and thickening network of “nomic threads” linking various “knot-concepts” with each other as further empirical laws were discovered. Hempel argued that it was essential to keep this thickening conceptual network systematic and simple; to that end, “concept formation and theory formation must go hand in hand” (Hempel 1966, 97). That, in turn, often necessitated “a modification of the operational criteria originally adopted for some of the central concepts” (Hempel 1966, 95). Operationalism would stand in the way of such flexibility.
Apart from the questions of whether operational definitions are sufficient or necessary, it is actually unclear what types of things operations are, and how they should be specified. The surface-level intuition is simple: the operations that matter are measurement operations involving physical instruments. But from the start Bridgman (1927, 5) also stated that the operations which fixed meaning were mental if the concepts in question were mental (e.g., in mathematics). And he knew that measurement operations involved more than physical manipulations of instruments; at least there are recordings and computations involved in the processing and analysis of data, and there are mental acts linking various parts of that complex procedure, too. To take the simplest example, the operation of counting is a mental operation, but it is an integral part of many “physical” procedures. He called such crucial non-physical operations “paper-and-pencil” operations. Bridgman lamented that it was the “most wide spread misconception with regard to the operational technique” to think that it demanded that all concepts in physics must find their meaning only in terms of physical operations in the laboratory (Bridgman 1938, 122–124; also Bridgman 1959b, 522). Later he gave a rough classification of operations into the instrumental, mental/verbal, and paper-and-pencil varieties (Bridgman 1959a, 3).
This issue becomes sharper when we ask the question of purpose: what are the aims of operational analysis, and which operations are suitable for achieving those aims? Having distinguished various types of operations, Bridgman also had to deal with the question of whether different types had different epistemic values, going beyond his initial intuitive fondness for instrumental operations. For example, if the point of operationalizing a concept was to make its meaning clear and precise, which meant using “operations which can be unequivocally performed” (Bridgman 1938, 119), then why were paper-and-pencil operations such as the construction of Euclidean geometric figures not just as good as instrumental operations? In the end he was willing to dispense with any ultimate privileging of instrumental operations. But he still maintained a preference for them when possible, without giving a convincing reason for that preference (Bridgman 1938, 127).
Thus Bridgman's position on the nature and function of operations was uneasy from start to finish. Various critics rightly zeroed in on this point. The most important point of contention was whether and why physical or instrumental operations had any special epistemic advantage. The Yale physicist Henry Margenau put the point succinctly:
Operationalism is an attitude that emphasizes the need of recourse, wherever feasible, to instrumental procedures when meanings are to be established. Bridgman disavows its status as a philosophy, and wisely so, for as a general view …. it cannot define the meaning of “instrumental procedure” in a manner that saves the view from being either trivial (which would be true if “instrumental” were construed to include symbolic, mental and paper-and-pencil operations) or too restrictive (if all operations are to be laboratory procedures). (Margenau in Frank 1956, 45)
As one can imagine, this dilemma also hampered attempts to use operational analysis in psychology. Operations in psychological research inevitably involved verbal instructions, reports and reactions. It was difficult to argue that these mental or verbal operations were superior in reliability or meaningfulness to the introspective reporting of mental states, which operationists tried so hard to exclude from scientific psychology.
Bridgman himself was troubled by the question regarding the nature of operations and admitted late in his life that he had not really provided “an analysis of what it is that makes an operation suitable”, or “in what terms can operations be specified” (Bridgman in Frank 1956, 77). An even deeper pessimism was expressed in Bridgman's 30-year retrospective on The Logic of Modern Physics, commissioned for Daedalus by Holton: “To me now it seems incomprehensible that I should ever have thought it within my powers … to analyze so thoroughly the functioning of our thinking apparatus that I could confidently expect to exhaust the subject and eliminate the possibility of a bright new idea against which I would be defenseless” (Bridgman 1959b, 520).
One last issue needs to be mentioned, before I complete the discussion of the critique of operationalism: the privacy of operations. This is perhaps not widely remembered now, but it was the issue on which Bridgman's position elicited the most severe incomprehension and objection, even from many who called themselves operationalists.
The emblematic moment in this dispute came during the 5th International Congress for the Unity of Science in 1939, held at Harvard University—one of the highpoints of the activities of the “Vienna Circle in exile” in America (see Holton 1995b). Bridgman was invited to address this conference, and chose to give a talk entitled “Science: Public or Private?”.[4] At this point it became clear that his enterprise was fundamentally at odds with the logical positivist project, despite the surface kinship:
The process that I want to call scientific is a process that involves the continual apprehension of meaning, the constant appraisal of significance, accompanied by a running act of checking to be sure that I am doing what I want to do, and of judging correctness or incorrectness. This checking and judging and accepting that together constitute understanding are done by me, and can be done for me by no one else. They are as private as my toothache, and without them science is dead. (Bridgman 1955, 56)
Positivists and behaviorists had embraced operationalism for precisely the opposite reason: they thought operations were public, objective and verifiable, unlike private experience. But Bridgman was insistent that operations were a matter for private experience. He could see no warrant in simply taking someone else's testimony as true or reliable, or in regarding the report of an operation performed by someone else as the same kind of thing as an operation performed and experienced by himself. In a later paper called “New Vistas for Intelligence” he declared: “Science is not truly objective unless it recognizes its own subjective or individual aspects” (Bridgman 1955, 556). As Holton puts it (2005, 74), Bridgman's drive in operational analysis was “to throw the spotlight on performable action, above all an action performed by himself. Ultimately, he was a private man, so much so that he was accused of solipsism, to which he scarcely objected.” In his epistemic individualism Bridgman was perhaps only matched by Herbert Dingle among notable 20th-century philosophers of science (see Dingle 1950).
Bridgman's individualistic bent, both in epistemology and social life, was in stark contrast to the logical positivist vision of knowledge and society, particularly the strand of positivism driven by Otto Neurath (1882–1945). The latter's aversion to the private compelled him to express even first-hand observation reports as third-person happenings in space and time, of the following type: “Otto's protocol at 3:17 o'clock: [Otto's speech-thinking at 3:16 o'clock was: (at 3:15 o'clock there was a table in the room perceived by Otto)].” (Neurath [1932–33] 1983, p. 93). Bridgman was unyielding in his opposition to Neurath's type of objectification of personal experience. To him, operations provided the best possible refuge from the ocean of uncertainty always threatening to engulf science, and the relative certainty was only possible if he was learning from his own operations, not from second-hand reports from some other person. In this regard Bridgman was closer to the strand of logical positivism represented by Moritz Schlick (1882–1936), who insisted on maintaining the notion of direct experience as the final arbiter of knowledge. Schlick ([1930] 1979) admitted that experience was fleeting and only provided momentary points of verification rather than any lasting “foundation” one could build knowledge on. Bridgman's operations had more promise in this regard, as the operations were meant to be repeatable so the descriptions of operations and their results would be lasting. However, that was not to be such a straightforward matter, as we will see in Section 3.4.
As Holton (1995a; 2005) reports from his first-hand observations of Bridgman, the privacy of operations (and the consequent privacy of science) was not an idle philosophical doctrine. In the lab he carried out as much of the work as possible with his own hands, using few assistants and crafting most of his instruments himself. Holton (1995a, 222–223) quotes the following report as typical of the way Bridgman worked: “It is easy, if all precautions are observed, to drill a hole … 17 inches long, in from 7 to 8 hours”—that is, a hole as narrow as the lead in a pencil, in a block of very hard steel. In academic life Bridgman (1955, 44) openly lamented the “intellectual fashion … of emphasizing that all our activities are fundamentally social in nature”. As for his social and political writings, they were often agonizing attempts to clarify, for himself, the place of the “intelligent individual” in society. He was unabashedly elitist, both on behalf of the gifted individual and of scientists as a group, and argued that giving appropriate special treatment to scientists would in the end benefit society (that is, all the individuals in society). Maila Walter observes (1990, 192–193): “Within the community of scientists and scientific philosophers, Bridgman had become the lone spokesman for a radical existential subjectivism”, more akin to Rheinhold Niebuhr's existentialist theology than to any commonly recognized philosophy of science. Bridgman's uncompromising individualism continued right to the end, with a self-administered euthanasia in the late stage of a painful terminal illness (see Holton 1995a, 226–227).
Is operationalism only a historical curiosity? In this final section, I would like to give a synoptic view of the relevance of operationalism to some issues that are current in philosophy of science.
As already noted, Bridgman's ideas first gained recognition in the midst of the logical-positivist preoccupation with language and meaning; therefore, operationalism was taken primarily as a doctrine about meaning and, as such, shown to be inadequate. In that context, it was reasonable for most philosophers to abandon it. Bridgman made various attempts to get beyond the popular caricature of operationalism adopted by advocates and critics alike. These attempts never received sufficient attention, but they offer some valuable lessons and show many productive directions in which his views can be interpreted and extended.
It is useful to continue listening to his retrospective given in the 1953 AAAS conference, quoted earlier. Bridgman tells us that what he advocated was merely:
an attitude or point of view generated by continued practice of operational analysis. So far as any dogma is involved here at all, it is merely the conviction that it is better, because it takes us further, to analyze into doings or happenings rather than into objects or entities. (Bridgman in Frank 1956, 76)
Already in his paper on “Operational Analysis” Bridgman (1938, 115–116) had stated that in the attempt to understand how science works, “the subject matter … is activity of one sort or another”. He equated “activity” and “operations”, the term “operation” only accentuating the directedness of the activity in question. In his last general philosophical treatise, The Way Things Are, Bridgman came back to this theme and stated that an operational analysis was only “a particular case of an analysis in terms of activities—doings or happenings”, instead of analysis “in terms of objects or static abstractions”, or “in terms of things or static elements” (1959a, 3; also 1959b, 522).
If we take our inspiration from this later Bridgman, we can take him as a guide for a new practice-oriented philosophy of science. We can put aside his reductive doctrine of meaning, his puritanical search for certainty, and his ambivalent privileging of instrumental operations over other types of operations. What Bridgman started up but never quite accomplished in a systematic and complete way was a philosophical analysis of science in terms of activities. Operations provide the philosopher (and the historian) of science with a very useful unit of analysis: actions or events, as opposed to objects, statements, beliefs, theories, paradigms, research programs, etc. The concept of operation should provide an effective framework for incorporating certain highly valuable insights about the nature of scientific practice, including the ideas of Ludwig Wittgenstein (1953) on language-games, Michael Polanyi (1958) on tacit knowledge, Marjorie Grene (1974) on the knowing agent, and Ian Hacking (1983) on direct interventions in experimental investigations.
In order to develop Bridgman's operational analysis into a full-fledged philosophy of scientific practice, there are some aspects of his thought that we need to develop and articulate further. First of all, we need a clearer and more detailed taxonomy of operations, without trying to say which types are better or worse at the outset. From this point of view it should not be regarded as a problem or an annoyance that there are different types of operations. The categories offered by Bridgman are much too broad, so we need to identify and describe specific and concrete operations, and distinguish simple and elementary ones from more complex ones. For example, the operation of length measurement with a meter-stick would be analyzable into the instrumental operations of alignment and concatenation, the perceptual operation of judging spatial coincidence, and the mental operation of counting. The operation of hypothesis-testing (in the “received view”) would be analyzable into the simpler operations of deductive prediction, experimental observation, and the comparison between deduced and observed outcomes. In the understanding of these operations we would need a detailed account of the scientist as the agent who performs the operations; here we return to Bridgman's preoccupation with the free individual, but also in essential social interaction with other individuals. A full understanding of operations would require an understanding of the agent's purposes (partly based on the fundamental aims of science), assumptions (including metaphysical principles essential for the particular type of activity in question), and skills and capabilities (including the tacit dimension). If we can achieve such a thick description of the operations that constitute scientific practice, we would be able to make good on Bridgman's promise that “it is better, because it takes us further, to analyze into doings or happenings rather than into objects or entities.”
Above I have presented a new operationalism as a promising framework for the analysis of scientific practice. Do Bridgman's ideas and attitudes also hold any current relevance for scientific practice itself? He did intend to reform scientific practice itself, not just its second-order analysis, so we must ask whether his reformist agenda has anything left in it for current science. To the casual reader, much of Bridgman's writing will seem like a series of radical complaints about the meaninglessness of various concepts and statements. But he was not interested in skeptical critique as an idle philosophical exercise. He got most worried when a concept was being extended to new situations where the familiar operations defining the concept ceased to be applicable. His arguments often had an iconoclastic flavor because he was exceptionally good at recognizing where a concept had been extended to new domains unthinkingly and most people were not even aware that the extension had been made. From the methodological lesson he took from Einstein to the insights gained in his own high-pressure physics, an important focus of Bridgman's operationalism was on regulating the extension of concepts to uncharted domains.
Bridgman reminded us forcefully that measurement operations did not have unlimited domains of application, and that our conceptual structures consequently had “joints” at which operational meanings changed. But there can be no “joints” if there is no continuous tissue around the disjointed bones. Less metaphorically: if we reduce meaning entirely to measurement operations, there are no possible grounds for assuming or demanding any continuity of meaning where there is clear discontinuity in measurement operations. When we have two different operations which give convergent results in the overlapping domain, how do we tell whether what we have is an accidental convergence of the measured values of two unrelated quantities, or a unified concept measured by two different methods? Some critics have maintained that only a recourse to theories can give us the answer (e.g., Lindsay 1937, 458; Gillies 1972, 23). That does not seem to me always necessary, as theory is not the only source of semantic continuity. There are instrumental operations that are not metrological, and these operations can provide a continuity of meaning, against which metrological validity can be judged (see Section 2.1). We can take operationalism as a useful and practicable caution not to make conceptual extensions without operational grounds.
A homespun example from the 18th century illustrates this point nicely: the efforts of the English potter Josiah Wedgwood (1730–1795) to extend the temperature scale to cover the very high temperatures in his kilns, at which mercury vaporized and glass melted. All previously known thermometers failed in that pyrometric range, so Wedgwood felt obliged to invent a whole new measurement standard (reminiscent of Bridgman in his high-pressure laboratory). Wedgwood noticed that very high temperatures made pieces of clay shrink, and created a temperature scale by assuming that the amount of contraction was proportional to temperature beyond “red heat”. As the start of his scale (red heat, defined as 0) was already beyond the boiling point of mercury, Wedgwood's scale was wholly disconnected from the temperature scale defined by mercury thermometers. Later, in response to widespread demand to clarify the meaning of his scale in more usual terms, Wedgwood made a translation of his scale into Fahrenheit degrees, by means of an intermediate standard (thermal expansion of silver) which overlapped with the high end of the mercury scale and the low end of the clay scale. (This process produced some unlikely numbers, for example 21,877°F for the temperature of his air-furnace.) It seems that Wedgwood initially did exactly what operationalist conscience would dictate: as the new instrument did not operate at all in the range of any trustworthy previous thermometers, he made a fresh scale. Why was that not the honest thing to do, and quite sufficient, too? Why did everyone, including Wedgwood himself, feel compelled to interpret the Wedgwood clay scale in terms of the mercury-Fahrenheit scale? Why was a continuous extension desired so strongly, when a disjointed set of operations seemed to serve all necessary practical purposes?
The urge for conceptual extension, in the Wedgwood case, was rooted in a widespread feeling that there was a property in the pyrometric range that was continuous in its meaning with temperature in the everyday range. Where did that feeling come from, long before there was any well-defined and agreed-upon theoretical concept of temperature? If we look closely at the situation, numerous subtle and often-unspoken connections do emerge between pyrometric temperature and everyday temperature. In the first place, we do bring objects to pyrometric domains by prolonged heating—that is to say, by a prolonged application of ordinary processes that cause the rise of temperature within the everyday domain. Likewise, the same causes of cooling that operate in the everyday domain will bring objects from pyrometric temperatures down to everyday temperatures; that is precisely what happens in calorimetric pyrometry (or, when we simply leave very hot things out in cold air for a while). These concrete physical operations provide a continuity of operational meaning between two domains that are not connected by a common measurement standard. Here again we need to articulate something that Bridgman already did imply: not all instrumental operations are measurement operations as such (for instance, we may know how to make iron melt without thereby obtaining any precise idea of the temperature at which that happens). Operational meaning even in the narrow, instrumental sense is broader than meaning specified by methods of measurement.
The connections listed above rest on very basic qualitative causal assumptions about temperature: fire raises the temperature of any ordinary objects on which it acts directly; if two objects at different temperatures are put in contact with each other, their temperatures tend to approach each other. There are semi-quantitative links as well. It is taken for granted that the consumption of more fuel should result in the generation of more heat, and that is partly based on a primitive notion of energy conservation. It is assumed that the amount of heat communicated to an object is roughly proportional to the amount of change in its temperature (barring changes of state and interfering influences), and that assumption is based on the rough but robust understanding of temperature as the “degree of heat.” So, for example, when a crucible is put on a steady fire, one assumes that the temperature of its contents will rise steadily, up to a certain maximum. That is exactly the kind of reasoning used by the chemist John Frederic Daniell (1790–1845) to criticize some of Wedgwood's results:
Now, any body almost knows, how very soon silver melts after it has attained a bright red heat, and every practical chemist has observed it to his cost, when working with silver crucibles. Neither the consumption of fuel, nor the increase of the air-draught, necessary to produce this effect, can warrant us in supposing that the fusing point of silver is 4 1/2 times higher than a red heat, fully visible in day-light. Neither on the same grounds, is it possible to admit that a full red-heat being 1077°[F], and the welding heat of iron 12,777°[F], that the fusing point of cast iron can be more than 5000° higher. The welding of iron must surely be considered as incipient fusion. (quoted in Chang 2004, 149)
Similar types of rough assumptions were also used in the extension of temperature to very low temperatures (beyond the freezing point of mercury and alcohol).
These cases illustrate that concepts can and do get extended to fresh new domains in which theories are uncertain and experience scant, even if no definite measurement operations have been worked out. We start with a concept with a secure net of uses giving it stable meaning in a restricted domain of circumstances. The extension of such a concept consists in giving it a secure net of uses, also credibly linked to the earlier net, in an adjacent domain. Such extension can happen in all kinds of ways, including theoretical fiat and metaphysical presumption, but the operational method is the most assured one. Specific, well-defined operations, whether they be instrumental, mental or paper-and-pencil type, can start a secure skeleton of meaning in the new domain. With all the elements of new meaning operationally well-defined, it also becomes possible to attempt linking them up with each other at each step along the way and check the whole meaning for coherence. (Compare such a deliberate process with the vague presumption that terms in a theoretical equation must have the same meaning in the entire mathematical range given to the variables.) Operationalism in this guise can be used as a secure method of conceptual extension, not prone to the kind of fragmentation that Hempel feared. Such operationalism would not destroy systematic unity; on the contrary, it is an optimal strategy for achieving as much systematic unity as nature would allow, in a strongly empiricist system of knowledge.
Conceptual extension is important, especially since it served as one of the key initial motivations of Bridgman's thought, but it is only part of the operationalist story. In more general terms, operationalism can be seen as a strategy for increasing the empirical content of scientific theories. Empirical content is not something we hear about very much these days in philosophy of science after the receding of Popperian and Lakatosian doctrines, but for Bridgman it was one of the key issues. If we take operationalism as a commitment to increase empirical content, Bridgman was not so much a high-handed judge who pronounced upon the meaningfulness of concepts in a black-and-white manner. Rather, he offered operational analysis as a tool of self-diagnosis and self-improvement. He was interested in advancing science, not in carping against it; like Descartes, he used skepticism as a means of achieving a more positive end. The operationalist dictum could be phrased as follows: maintain and increase the empirical content of theories by the use of operationally well-defined concepts.
What is empirical content? Karl Popper saw the amount of the empirical content of a theory as the number of states of the world that are forbidden by it. Regarding laws of nature he said: “the more they prohibit they more they say” (Popper 1972, 41). Or, somewhat more formally: “I define the empirical content of a statement p as the class of its potential falsifiers” (120). Similarly but staying away from the strict falsificationist idiom, Imre Lakatos understood empirical content as the number of empirically testable predictions. It is difficult to craft an exact quantitative measure, but we can at least say that the amount of empirical content depends on the number of empirically testable relations that a theory specifies. That, in turn, depends on the number of independently measurable parameters. Increasing the latter, or at least maintaining it, was something that Bridgman sought to achieve with his operationalism. This, I submit, was one of the key reasons why he did not like conceptual extensions that were not backed up by measurement operations in the new domain. To follow Bridgman's thinking along these lines, consider this intriguing passage, which at first glance looks like another complaint about meaninglessness. But towards the end the main point emerges as a worry about diminishing empirical content:
What is the possible meaning of the statement that the diameter of an electron is 10-13 cm? Again the only answer is found by examining the operations by which the number 10-13 was obtained. This number came by solving certain equations derived from the field equations of electrodynamics, into which certain numerical data obtained by experiment had been substituted. The concept of length has therefore now been so modified as to include that theory of electricity embodied in the field equations, and, most important, assumes the correctness of extending these equations from the dimensions in which they may be verified experimentally into a region in which their correctness is one of the most important and problematical of present-day questions in physics. To find whether the field equations are correct on a small scale, we must verify the relations demanded by the equations between the electric and magnetic forces and the space coördinates, to determine which involves measurement of lengths. But if these space coördinates cannot be given an independent meaning apart from the equations, not only is the attempted verification of the equations impossible, but the question itself is meaningless. If we stick to the concept of length by itself, we are landed in a vicious circle. As a matter of fact, the concept of length disappears as an independent thing, and fuses in a complicated way with other concepts, all of which are themselves altered thereby, with the result that the total number of concepts used in describing nature at this level is reduced in number.[6] (Bridgman 1927, 21–22)
Such a reduction in the number of operationally meaningful concepts is almost bound to result in a corresponding reduction in the number of relations that can be tested empirically. A good scientist would fight against such a reduction of empirical content.
This concern with empirical content also explains why Bridgman was not content with the mainstream post-positivist philosophical discourse on concept-formation and empirical significance, exemplified by the works of Carl Hempel and Willard Van Orman Quine. As noted in Section 2.2, Bridgman did not object to theoretical science creating a system of concepts and laws that made contact with observations only at some points. However, Quinean holism, in which the unit of empirical significance was the entire system of knowledge, had no particular concern about increasing the number of those points-of-contact with experience. Bridgman's ideal was to operationalize each and every concept if possible, and each case of de-operationalization rang alarm bells in his head.
Recognizing the importance of empirical content helps us make sense of Bridgman's complex attitude toward theoretical concepts. In a little-known section of The Logic of Modern Physics, he discussed what he called “mental constructs” in science, particularly those created in order to “enable us to deal with physical situations which we cannot directly experience through our senses, but with which we have contact indirectly and by inference” (1927, 53–60). Not all constructs are the same:
The essential point is that our constructs fall into two classes: those to which no physical operations correspond other than those which enter the definition of the construct, and those which admit of other operations, or which could be defined in several alternative ways in terms of physically distinct operations. This difference in the character of constructs may be expected to correspond to essential physical differences, and these physical differences are much too likely to be overlooked in the thinking of physicists. (Bridgman 1927, 59–60)
They were also very easily overlooked in the thinking of philosophers who debated his ideas. What Bridgman says here is entirely contrary to the common image of his doctrines. When it came to constructs, “of which physics is full,” Bridgman not only admitted that one concept could correspond to many different operations, but even suggested that such multiplicity of operational meaning was “what we mean by the reality of things not given directly by experience.” In an illustration of these ideas, Bridgman argued that the concept of stress within a solid body had physical reality, but the concept of electric field did not, since the latter only ever manifested itself through force and electric charge, by which it was defined (Bridgman 1927, 57). This comes down to the stance that a theoretical concept without direct operational meaning is worthwhile only if it serves as a mediator connecting two or more operationally meaningful concepts, creating an empirically testable relation. This is in fact not so different from Hempel's view quoted in Section 2.2, albeit with a different emphasis.
In closing, I would like to pull out an insight from Bridgman that is usually not recognized in discussions of operationalism, but actually has emerged as a significant point of contention in more recent philosophy of science. This is the issue of complexity. In Section 1.1 I have already quoted Bridgman's puzzling statement that nature is ultimately “neither understandable nor subject to law.” As it turns out, that was not an isolated off-hand remark. An important aspect of Bridgman's operationalism was a search for certainty, and it was a search made all the more desperate by a deep-rooted pessimism about the possibility of attaining any certainty in science, at least if scientists were to seek a simple and unified system of knowledge. Bridgman professed his belief that “the external world of objects and happenings is … so complex that all aspects of it can never be reproduced by any verbal structure.” He lamented: “Even in physics this is not sufficiently appreciated, as is shown, for example, by the reification of energy. The totality of situations covered by various aspects of the energy concept is too complex to be reproduced by any simple verbal device.” (Bridgman in Frank 1956, 78)
Bridgman's view on the complexity of nature also had a direct implication for the limits of operational analysis itself in providing clarity and precision. Right from The Logic of Modern Physics, Bridgman stressed that “all results of measurement are only approximate”; this obvious fact, he said, “tacitly underlies all our discussion”. This he attributed ultimately to something fundamental about the nature of human experience: “all experience seems to be of this character; we never have perfectly clean-cut knowledge of anything, but all our experience is surrounded by a twilight zone, a penumbra of uncertainty, into which we have not yet penetrated. This penumbra is as truly an unexplored region as any other region beyond experiment” (1927, 33). This indicated a fundamental limitation to the certainty of operations: “Operations themselves are, of course, derived from experience, and would be expected also to have a nebulous edge of uncertainty” (1927, 36). Bridgman remained clearly aware of the complexities revealed by operational analysis, stating late in his life that “operational analysis can always be pushed to the point where sharpness disappears” (Bridgman in Frank 1956, 78), and that “there is nothing absolute or final about an operational analysis” (Bridgman 1959b, 522). Still, he would not give up on the pushing, which was necessary to reach as much clarity as possible.
Bridgman's battle against his own skeptical and pessimistic conscience was a heroic one. After decades of operationalist thinking he arrived at “a picture of man isolated … in an oasis of phenomena which he will never be able to transcend because beyond its bounds the operations are impossible which are necessary to give meaning to his thought” (Bridgman 1955, 540). What this picture forced on him was a deep sense of humility, as expressed in his memorable statement on the scientific method: “The scientific method, as far as it is a method, is nothing more than doing one's damnedest with one's mind, no holds barred” (1955, 535). Retaining that sense of humility will help us in developing Bridgman's unfinished thoughts to create a new operationalism that does full justice to the complexity and richness of both nature and human scientific practice.
1. History, Problems, and Issues
Traditional epistemology has its roots in Plato and the ancient skeptics. One strand emerges from Plato's interest in the problem of distinguishing between knowledge and true belief. His solution was to suggest that knowledge differs from true belief in being justified. Ancient skeptics complained that all attempts to provide any such justification were hopelessly flawed. Another strand emerges from the attempt to provide a reconstruction of human knowledge showing how the pieces of human knowledge fit together in a structure of mutual support. This project got its modern stamp from Descartes and comes in empiricist as well as rationalist versions which in turn can be given either a foundational or coherentist twist. The two strands are woven together by a common theme. The bonds that hold the reconstruction of human knowledge together are the justificational and evidential relations which enable us to distinguish knowledge from true belief.
The traditional approach is predicated on the assumption that epistemological questions have to be answered in ways which do not presuppose any particular knowledge. The argument is that any such appeal would obviously be question begging. Such approaches may be appropriately labeled “transcendental.”
The Darwinian revolution of the nineteenth century suggested an alternative approach first explored by Dewey and the pragmatists. Human beings, as the products of evolutionary development, are natural beings. Their capacities for knowledge and belief are also the products of a natural evolutionary development. As such, there is some reason to suspect that knowing, as a natural activity, could and should be treated and analyzed along lines compatible with its status, i. e., by the methods of natural science. On this view, there is no sharp division of labor between science and epistemology. In particular, the results of particular sciences such as evolutionary biology and psychology are not ruled a priori irrelevant to the solution of epistemological problems. Such approaches, in general, are called naturalistic epistemologies, whether they are directly motivated by evolutionary considerations or not. Those which are directly motivated by evolutionary considerations and which argue that the growth of knowledge follows the pattern of evolution in biology are called “evolutionary epistemologies.”
Evolutionary epistemology is the attempt to address questions in the theory of knowledge from an evolutionary point of view. Evolutionary epistemology involves, in part, deploying models and metaphors drawn from evolutionary biology in the attempt to characterize and resolve issues arising in epistemology and conceptual change. As disciplines co-evolve, models are traded back and forth. Thus, evolutionary epistemology also involves attempts to understand how biological evolution proceeds by interpreting it through models drawn from our understanding of conceptual change and the development of theories. The term “evolutionary epistemology” was coined by Donald Campbell (1974).
1.1 The Evolution of Epistemological Mechanisms (EEM ) versus The Evolutionary Epistemology of Theories (EET)
There are two interrelated but distinct programs which go by the name “evolutionary epistemology.” One focuses on the development of cognitive mechanisms in animals and humans. This involves a straightforward extension of the biological theory of evolution to those aspects or traits of animals which are the biological substrates of cognitive activity, e.g., their brains, sensory systems, motor systems, etc. The other program attempts to account for the evolution of ideas, scientific theories, epistemic norms and culture in general by using models and metaphors drawn from evolutionary biology. Both programs have their roots in 19th century biology and social philosophy, in the work of Darwin, Spencer, James and others. There have been a number of attempts in the intervening years to develop the programs in detail (see Campbell 1974, Bradie 1986, Cziko 1995). Much of the contemporary work in evolutionary epistemology derives from the work of Konrad Lorenz (1977), Donald Campbell (1974, et al.), Karl Popper (1972, 1984) and Stephen Toulmin (1967, 1972).
The two programs have been labeled EEM and EET (Bradie, 1986). EEM is the label for the program which attempts to provide an evolutionary account of the development of cognitive structures. EET is the label for the program which attempts to analyze the development of human knowledge and epistemological norms by appealing to relevant biological considerations. Some of these attempts involve analyzing the growth of human knowledge in terms of selectionist models and metaphors (e.g., Popper 1972, Toulmin 1972, Hull 1988; see Renzi and Napolitano 2011 for a critique of these efforts). Others argue for a biological grounding of epistemological norms and methodologies but eschew selectionist models of the growth of human knowledge as such (e.g., Ruse 1986, Rescher 1990).
The EEM and EET programs are interconnected but distinct. A successful EEM selectionist explanation of the development of cognitive brain structures provides no warrant, in itself, for extrapolating such models to understand the development of human knowledge systems. Similarly, endorsing an EET selectionist account of how human knowledge systems grow does not, in itself, warrant concluding that specific or general brain structures involved in cognition are the result of natural selection for enhanced cognitive capacities. The two programs, though similar in design and drawing upon the same models and metaphors, do not stand or fall together.
1.2 Ontogeny versus Phylogeny
Biological development involves both ontogenetic and phylogenetic considerations. Thus, the development of specific traits, such as the opposable thumb in humans, can be viewed both from the point of view of the development of that trait in individual organisms (ontogeny) and the development of that trait in the human lineage (phylogeny). The development of knowledge and knowing mechanisms exhibits a parallel distinction. We can consider the growth of an individual's corpus of knowledge and epistemological norms or his or her brain (ontogeny) or the growth of human knowledge and establishment of epistemological norms across generations or the development of brains in the human lineage (phylogeny). The EEM/EET distinction cuts across this distinction since we may be concerned either with the ontogenetic or phylogenetic development of, e.g., the brain or the ontogenetic or phylogenetic development of norms and knowledge corpora. One might expect that since current orthodoxy maintains that biological processes of ontogenesis proceed differently from the selectionist processes of phylogenesis, evolutionary epistemologies would reflect this difference. Curiously enough, however, for the most part they do not. For example, the theory of “neural Darwinism” as put forth by Edelman (1987) and Changeaux (1985) offers a selectionist account of the ontogenetic development of the neural structures of the brain. Karl Popper's conjectures and refutations model of the development of human knowledge is a well known example of a selectionist account which has been applied both to the ontogenetic growth of knowledge in individuals as well as the trans-generational (phylogenetic) evolution of scientific knowledge. B. F. Skinner's theory of operant conditioning, which deals with the ontogenesis of individual behavior, is explicitly based upon the Darwinian selectionist model (Skinner 1981).
A third distinction concerns descriptive versus prescriptive approaches to epistemology and the growth of human knowledge. Traditionally, epistemology has been construed as a normative project whose aim is to clarify and defend conceptions of knowledge, foundations, evidential warrant and justification. Many have argued that neither the EEM programs nor the EET programs have anything at all to do with epistemology properly (i. e., traditionally) understood. The basis for this contention is that epistemology, properly understood, is a normative discipline, whereas the EEM and EET programs are concerned with the construction of causal and genetic (i.e., descriptive) models of the evolution of cognitive capacities or knowledge systems. No such models, it is alleged, can have anything important to contribute to normative epistemology (e.g., Kim 1988). The force of this complaint depends upon how one construes the relationship between evolutionary epistemology and the tradition.
There are three possible configurations of the relationship between descriptive and traditional epistemologies. (1) Descriptive epistemologies can be construed as competitors to traditional normative epistemologies. On this view, both are trying to address the same concerns and offering competing solutions. Riedl (1984) defends this position. A standard objection to such approaches is that descriptive accounts are not adequate to do justice to the prescriptive elements of normative methodologies. The extent to which an evolutionary approach contributes to the resolution of traditional epistemological and philosophical problems is a function of which approach one adopts (cf. Dretske 1971, Bradie 1986, Ruse 1986, Radnitsky and Bartley 1987, Kim 1988). (2) Descriptive epistemology might be seen as a successor discipline to traditional epistemology. On this reading, descriptive epistemology does not address the questions of traditional epistemology because it deems them irrelevant or unanswerable or uninteresting. Many defenders of naturalized epistemologies fall into this camp (e.g., Munz 1993). (3) Descriptive epistemology might be seen as complementary to traditional epistemology. This appears to be Campbell's view. On this analysis, the function of the evolutionary approach is to provide a descriptive account of knowing mechanisms while leaving the prescriptive aspects of epistemology to more traditional approaches. At best, the evolutionary analyses serve to rule out normative approaches which are either implausible or inconsistent with an evolutionary origin of human understanding.
1.4 Future Prospects
EEM programs are saddled with the typical uncertainties of phylogenetic reconstructions. Is this or that organ or structure an adaptation and if so, for what? In addition, there are the uncertainties which result from the necessarily sparse fossil record of brain and sensory organ development. The EET programs are even more problematic. While it is plausible enough to think that the evolutionary imprint on our organs of thought influences what and how we do think, it is not at all clear that the influence is direct, significant or detectible. Selectionist epistemologies which endorse a “trial and error” methodology as an appropriate model for understanding scientific change are not analytic consequences of accepting that the brain and other ancillary organs are adaptations which have evolved primarily under the influence of natural selection. The viability of such selectionist models is an empirical question which rests on the development of adequate models. Hull's (1988) is, as he himself admits, but the first step in that direction. Cziko (1995) is a manifesto urging the development of such models (cf. also the evolutionary game theory modeling approach of Harms 1997). Much hard empirical work needs to be done to sustain this line of research. Non-selectionist evolutionary epistemologies, along the lines of Ruse (1986), face a different range of difficulties. It remains to be shown that any biological considerations are sufficiently restrictive to narrow down the range of potential methodologies in any meaningful way.
Nevertheless, the emergence in the latter quarter of the twentieth century of serious efforts to provide an evolutionary account of human understanding has potentially radical consequences. The application of selectionist models to the development of human knowledge, for example, creates an immediate tension. Standard traditional accounts of the emergence and growth of scientific knowledge see science as a progressive enterprise which, under the appropriate conditions of rational and free inquiry, generates a body of knowledge which progressively converges on the truth. Selectionist models of biological evolution, on the other hand, are generally construed to be non-progressive or, at most, locally so. Rather than generating convergence, biological evolution produces diversity. Popper's evolutionary epistemology attempts to embrace both but does so uneasily. Kuhn's “scientific revolutions” account draws tentatively upon a Darwinian model, but when criticized, Kuhn retreated (cf. Kuhn 1972, pp. 172f with Lakatos and Musgrave 1970, p. 264). Toulmin (1972) is a noteworthy exception. On his account, concepts of rationality are purely “local” and are themselves subject to evolution. This, in turn, seems to entail the need to abandon any sense of “goal directedness” in scientific inquiry. This is a radical consequence which few have embraced. Pursuing an evolutionary approach to epistemology raises fundamental questions about the concepts of knowledge, truth, realism, justification and rationality.
1.5 Expanding the Circle
Although Campbell and Popper both pointed to the continuity between the evolution of human knowledge and the evolution of knowledge in non-human organisms, much of the early work in evolutionary epistemology focused on the human condition. However, recent empirical investigations by psychologists, cognitive ethologists, cognitive neuroscientists and animal behaviorists have revealed that animals, both primates and non-primates, have much more sophisticated cognitive capacities than were previously suspected (Panksepp 1998, Heyes and Huber 2000, Rogers and Kaplan 2004). From an evolutionary perspective this is not surprising given the shared evolutionary heritage that all animals share. Taking Darwin seriously means reconsidering and reassessing the nature of human knowledge in the light of our increased awareness of the cogntive capabilities of the members of other species. In addition, once a firm empirical basis of the scope and limits of animal cognitive capacities has been established we will be in a position to reassess our philosophical evaluations of the mental lives of animals and their epistemic and moral status as well. Further field research promises to revolutionize our understanding of the sense in which human beings are one among the animals.
The KLI Theory Lab of the Konrad Lorenz Institute in Vienna offers an extensive bio- and bibliographical data base covering eighteen research areas related to evolution and cognition research. The entry for “evolutionary epistemology” contains links to authors and texts as well as a brief introduction and overview of the field. It is an interactive database and the Institute encourages authors to submit their own relevant bibliographies for inclusion in the database. The database can be accessed at <http://theorylab.org>. In addition, there is a journal devoted to issues in evolutionary epistemology in addition to other applications of biological theory, Biological Theory: Integrating Development, Evolution and Cognition. Information about the journal can be found at <http://www.kli.ac.at/journal>.
2. Formal Models
Every scientific enterprise requires formal and semi-formal models which allow the quantitative characterization of its objects of study. The attempt to transform the philosophical study of knowledge into a scientific discipline which approaches knowledge as a biological phenomenon is no different. Much of the evolutionary epistemology literature has been concerned with how to conceive of knowledge as a natural phenomenon, what difference this would make to our understanding of our place in the world, and with answering objections to the project. There are, as well, a number of more technical projects which attempt to provide the theoretical tools necessary for a naturalistic epistemology.
2.1 Static Optimization Models
In the simplest sort of model, an organism has to deal with an environment that has two states, S1 and S2, and has two possible responses R1 and R2. We suppose that what the organism does in each state makes a difference to its fitness. Fitnesses are usually written characterized by a matrix W.
The individual elements of the matrix Wij are the fitness consequences of response i in state j. So, for instance, W21 denotes the fitness consequences of R2 in S1. If we let W11 and W22 equal one and W12 and W21 equal zero, then there is a clear evolutionary advantage to performing R1 in S1 and R2 in S2.
However, the organism must first detect the state of the environment, and detectors are not in general perfectly reliable. If the organism responds automatically to the detector, we can use the probabilities of responses given states to characterize the reliability of the detector. We write the probability of R1 given S1 as Pr(R1|S1). This allows us to calculate that responding to the detector rather than always choosing R1 or R2 will be advantageous just in case the following inequality holds (cf. Godfrey-Smith 1996):
Pr(R2|S2)/(1−Pr(R1|S1)) > Pr(S1)(W11−W21)/(1−Pr(S1))(W22−W12)
This simple model demonstrates that whether or not flexible responses are adaptive depends on the particular characteristics of the fitness differences that the responses make, the probability of the various states of the environment, and the reliability of the detector. The particular result is calculated assuming that detecting the environmental state and the flexible response system is free in evolutionary terms. More complete analyses would include the costs of these factors.
Static optimization models like the one outlined above can be extended in several ways. Most obviously, the number of environmental states and organismic responses can be increased, but there are other modifications that are more interesting. Signal detection theory, for instance, models the detectors and cues in more detail. In one example, a species of “sea moss” detects the presence of predatory sea slugs via a chemical cue. They respond by growing spines, which is costly. The cue in this case, the water-borne chemical, comes in a variety of concentrations, which indicate various levels of danger. Signal detection theory allows us to calculate the best threshold value of the detector for the growing of spines.
Static models depict evolutionary processes in terms of fitness costs and benefits. They are static in the sense that they model no actual process, but merely calculate the direction of change for different situations. If fitness is high, a type will increase, if low it will decrease. When fitnesses are equal, population proportions remain at stable equilibrium. Dynamic models typically employ the kinds of calculations involved in static models to depict actual change over time in population proportions. Instead of calculating whether change will occur and in what direction, dynamic models follow change.
2.2 Population Dynamics
Population dynamics, sometimes referred to as “replicator dynamics”, offers a tractable way to model the evolution of populations over time under the kinds of selective pressures that can be characterized by static optimization models. This is often necessary, since the dynamics of such populations are often difficult to predict purely on the basis of static considerations of payoff differences. The so-called “replicator dynamics” were named by Taylor and Jonker (1978) and generalized by Schuster and Sigmund (1983) and Hofbauer and Sigmund (1988). They trace their source back to the seminal work of R.A. Fisher in the 1920's and 30's. The generalization covers evolutionary models used in population genetics, evolutionary game theory, ecology, and the study of prebiotic evolution. The models can be implemented either mathematically or computationally, and can model either stepwise (discrete) or continuous evolutionary change.
Population dynamics models the evolution of populations. A population is a collection of individuals, which are categorized according to type. The types in genetics are genes, in evolutionary game theory, strategies. The types of interest in epistemological models would be types of cognitive apparatuses, or cognitive strategies — ways of responding to environmental cues, ways of manipulating representations, and so forth. Roughly, EEM models focus on the inherited and EET models focus on the learned. The evolution of the population consists in changes of the relative frequency of the different types within the population. Selection, typified by differential reproductive success, is represented as follows. Each type has a growth rate or “fitness”, designated by w, and a frequency designated by p. The frequency of type i at the next generation pi′ is simply the old frequency multiplied by the fitness and divided by the mean fitness of the population “w”.
Division by w has the effect of “normalizing” the frequencies, so that they add up to one after each is multiplied by its fitness. It also makes evident that the frequency of a type will increase just in case its fitness is higher than the current population average.
Fitness
Fitnesses, which should be understood simply as the aggregation of probable-growth factors that drive the dynamics of large populations, may depend on a variety of factors. Fitness components differ from variation components in that they affect population frequencies proportionally to those frequencies, that is to say, multiplicatively. Fitness component in biological evolution include mortality and reproductive rate. In cultural evolution, they include transmission probability and rejection probability. Within either sort of model, what matters is how fitnesses change as a result of other changing factors within the model. In the simplest cases, fitnesses are fixed and the type with the highest fitness inevitably dominates the population. In more complex cases, fitnesses may depend on variable factors like who one plays against, or the state of a variable environment. Most commonly, variable fitnesses are calculated using a payoff matrix like the one above. In general, to calculate the expected fitness of a type, one multiplies the fitness a type would have in each situation times the likelihood that individuals in the population will confront that situation and adds the resulting products.
wi = SA Pr(A)·WiA
where WiA is type i's fitness in situation A. This sort of calculation assumes that the effects of the various situations are additive. More complex situations can be modeled, of course, but additive matrices are the standard. It should be noted, however, that matrix-driven evolution can exhibit quite complex behavior. For instance, chaotic behavior is possible with as few as four strategies (Skyrms 1992).
Some relationships may be represented without a matrix. Boyd and Richerson (1985), for instance, were interested in a special kind of frequency dependent transmission bias in culture, where being common conferred an advantage due to imitators “doing as the Romans do.” In such a case, the operative fitness of the type is just the fitness as calculated according to the usual factors, and then modified as a function of the frequency of the type.
Continuity and Computation
The conceptual bases of replicator dynamics are quite straightforward. Getting results typically requires one of two approaches. In order to prove more than rudimentary mathematical results, one typically needs to derive a continuous version of the dynamics. The basic form is
dpi/dt = p(wi − w)
with fitnesses calculated as usual. Mathematical approaches have been quite productive, though the bulk of theoretical results apply primarily to population genetics. See Hofbauer and Sigmund (1988) for a compendium of such results, as well as a reasonable graduate-level introduction to the mathematical study of evolutionary processes.
The second approach is computational. With the increase in power of personal computers, computational implementation of evolutionary models become increasingly attractive. They require only rudimentary programming skills, and are in general much more flexible in the assumptions they require. The general strategy is to create an array to hold population frequencies and fitnesses, and then a series of procedures (or methods or functions) which
calculate fitnesses,
update frequencies with the new fitnesses, and
manage interface details like outputting the new state of the population to a file or the screen.
A loop then runs the routines in sequence, over and over again. Most modelers are happy to put their source code on the internet, which is probably the best place to find it.
Modeling Cultural Evolution
Part of the difficulty in understanding cognitive behavior as the product of evolution is that there are at least three very different evolutionary processes involved. First, there is the biological evolution of cognitive and perceptual mechanisms via genetic inheritance. Second, there is the cultural evolution of languages and concepts. Third, there is the trial-and-error learning process that occurs during an individual's lifetime. Moreover, there is some reason to agree with Donald T. Campbell that understanding human knowledge fully will require understanding the interaction between these processes. This requires that we be able to model both processes of biological and cultural evolution. There are by now a number of well-established models of biological evolution. Cultural evolution presents more novelty.
Perhaps the most popular attempt to understand cultural evolution is Richard Dawkins' (1976) invention of the “meme.” Dawkins observed that what lies at the heart of biological evolution is differential reproduction. Evolution in general was then the competitive dynamics of lineages of self-replicating entities. If culture was to evolve, on this view, there had to be cultural “replicators”, or entities whose differential replication in culture constituted the cultural evolutionary process. Dawkins dubbed these entities “memes”, and they were characterized as informational entities which infect our brains, “leaping from head to head” via what we ordinarily call imitation. Common examples include infectious tunes, and religious ideologies. The main difficulty with this approach has been with providing specifications for the basic entities. The identity conditions of genes can be given, in theory, in terms of sequences of base pairs in chromosomes. There appears to be no such fundamental “alphabet” for the items of cultural transmission. Consequently, the project of “memetics” as contending basis for evolutionary epistemology is on hold pending an adequate understanding of its basic ontology. [See the online Journal of Memetics for more information.]
Population models have been used to good effect in modeling cultural transmission processes. Evolutionary game theory models are frequently claimed to cover both processes in which strategies are inherited and those in which they are imitated. This application is possible in the absence of any specification of the underlying nature of strategies, for instance, whether they are to be thought of as “things” which are replicated, or whether they are properties or states of the individuals whose strategies they are. This is sometimes referred to as the “epidemiological approach”, though again, the comparison to infection is due to the quantitative tools used in analysis rather than to any presupposition regarding the underlying ontology of cultural transmission.
2.3 Multi-Level Evolution
The kind of levels involved in evolutionary epistemology are quite different than the kind of levels of selection which are discussed much more often in the “levels of selection” debate in evolutionary biology. In evolutionary biology, the “levels” of selection under discussion are levels of scale. The debate concerns whether genes are always the “units” or “targets” of selection, or whether selection can occur on higher levels, like organisms, groups, and species. The levels involved in evolutionary epistemology, on the other hand, are levels of the regulatory hierarchy involved in the control of behavior. These include the genetic bases of cognitive and perceptual hardware, concepts, languages, techniques, beliefs, preferences, and so forth. Note that in the case of evolutionary epistemology, the terms “levels” and “hierarchy” may be impressionistic. There is often no clear arrangement of levels at all.
There are at least two different approaches that have been taken to modeling multi-level evolution.
Dual Transmission Models: Boyd and Richerson (1985) adapted models from genetics to model a case in which a trait (cooperation) was affected both by genetic and cultural evolution. It was first shown that a genetically determined bias on cultural transmission could be selected for in a migratory population. The bias made it easier to pick up local customs, increasing the likelihood of imitation beyond that determined by the frequency and perceived value of the behavior. Once this bias was in place, its effect was strong enough to overcome the perceived costs involved in cooperative behavior. The model yielded two important results. First, it provided a novel mechanism according to which cooperative behavior can stabilize in migratory populations. But more importantly, it demonstrated that cultural evolution cannot be predicted purely on the basis of genetic fitnesses.
Multiple Population Models: Harms (1997) constructed a multi-level dynamic population model of bumblebee learning. Mutual information between distributions of sensor types, overt foraging behaviors, and internal foraging preferences on the one hand and environmental states on the other was assessed and compared to average fitness of the population states. It was shown that information present in overt behaviors may be underutilized, and that exaptation of sensor mechanisms for preference formation can bring about the utilization of that information.
2.4 Meaning
Full descriptive accounts of truth and justification both demand a theory of meaning. Until a sign has meaning, it cannot be true or false. Moreover, determining the meaning of justificatory claims may provide a descriptive theory of justification. Presumably, what makes a claim of justification true is the basis of that justification. If meaning is conventional, then the evolution of meaning becomes an instance of the evolution of conventions.
Models of the evolution of conventions have in one case been extended to apply to meaning conventions. Skyrms (1996, chapter 5) gave an evolutionary interpretation of David Lewis' (1969) model of rational selection of meaning conventions. Skyrms was able to show that there is strong selection on the formation of “signaling systems” in mixed populations with a full set of coordinated, countercoordinated, and uncoordinated strategies. It is significant that the structure of the model and the selective process by which meaning conventions emerge and are stabilized largely parallels the account of the evolution of meaning given by Ruth Millikan (1984).
In the simplest version, the model is constructed as follows: We imagine that there are two states of affairs T, two acts A, and two signals M. Players have an equal chance of being in either the position of sender, or receiver. Receivers must decide what to do based purely on what the sender tells them. In this purely cooperative version, each player gets one point if the receiver does A1 if the state is T1 or A2 if the state is T2.
Since players will be both sender and receiver, they must have a strategy for each situation. There are sixteen such strategies, and we suppose them to be either inherited (or learned) from biological parents, or imitated on the basis of perceived success in terms of points earned. Strategies I1 and I2 are signaling systems, in that if both players play the same one of these two strategies they will always get their payoff. I3 and I4 are anti-signaling strategies, which result in consistent miscoordination, though they do well against each other. All of the other strategies involve S3, S4, R3, or R4, which results in the same act being performed no matter what the external state is.
Sender Strategies:
Receiver Strategies:
Complete Strategies:
Simulation results showed that virtually all initial population distributions become dominated by one or the other of the two signaling system strategies. The situation becomes more complex when more realistic payoffs are introduced, for instance, that the sender incurs a cost rather than automatically sharing the benefit that the receiver gets from correct behavior for the environment. Even in such situations, however, the most likely course of evolution is domination by a signaling system.
Evolving Artificial Moral Ecologies, Centre for Applied Ethics, University of British Columbia
The Journal of Memetics, sponsored by the Centre for Policy Modeling (Manchester Metropolitan University), the Principia Cybernetica Project, and Systems Engineering, Policy Analysis and Management (Delft University of Technology)
Evolutionary Epistemology, entry in the Internet Encyclopedia of Philosophy.Moral luck occurs when an agent can be correctly treated as an object of moral judgment despite the fact that a significant aspect of what she is assessed for depends on factors beyond her control. Bernard Williams writes, “when I first introduced the expression moral luck, I expected to suggest an oxymoron” (Williams 1993, 251). Indeed, immunity from luck has been thought by many to be part of the very essence of morality. And yet, as Williams (1981) and Thomas Nagel (1979) showed in their now classic pair of articles, it appears that our everyday judgments and practices commit us to the existence of moral luck. The problem of moral luck arises because we seem to be committed to the general principle that we are morally assessable only to the extent that what we are assessed for depends on factors under our control (call this the “Control Principle”). At the same time, when it comes to countless particular cases, we morally assess agents for things that depend on factors that are not in their control. And making the situation still more problematic is the fact that a very natural line of reasoning suggests that it is impossible to morally assess anyone for anything if we adhere to the Control Principle.
1. Generating the Problem of Moral Luck and Kinds of Luck
3. Kinds of Moral Assessment
4. Responding to the Problem: Three Approaches
5. Conclusion
Bibliography
Academic Tools
Other Internet Resources
Related Entries
1. Generating the Problem of Moral Luck and Kinds of Luck
The idea that morality is immune from luck finds inspiration in Kant:
A good will is not good because of what it effects or accomplishes, because of its fitness to attain some proposed end, but only because of its volition, that is, it is good in itself… Even if, by a special disfavor of fortune or by the niggardly provision of a step motherly nature, this will should wholly lack the capacity to carry out its purpose—if with its greatest efforts it should yet achieve nothing and only the good will were left (not, of course, as a mere wish but as the summoning of all means insofar as they are in our control)—then, like a jewel, it would still shine by itself, as something that has its full worth in itself. Usefulness or fruitlessness can neither add anything to this worth nor take anything away from it (Kant 1784/1998, 4:394).
Thomas Nagel approvingly cites this passage in the opening of his 1979 article, “Moral Luck.” Nagel's article began as a reply to Williams' paper of the same name, and the two articles together articulated in a new and powerful way a challenge for anyone wishing to defend the Kantian idea that morality is immune from luck.
To see exactly how the challenge arises, let us begin with the Control Principle:
(CP) We are morally assessable only to the extent that what we are assessed for depends on factors under our control.
It is intuitively compelling, as is the following corollary of it:
(CP-Corollary) Two people ought not to be morally assessed differently if the only other differences between them are due to factors beyond their control.
Not only are the Control Principle and its corollary plausible in themselves, they also seem to find support in our reactions to particular cases. For example, if we find out that a woman who has just stepped on your toes was simply pushed, then our temptation to blame her is likely to evaporate. It seems that the reason for this is our unwillingness to hold someone responsible for what is not in her control. Similarly, if two drivers have taken all precautions, and are abiding by all the rules of the road, and in one case, a dog runs in front of the car and is killed, and not in the other, then, given that the dog's running out was not something over which either driver had control, it seems that we are reluctant to blame one driver more than the other. Although we might expect different reactions from the two drivers, it does not seem that one is deserving of a worse moral assessment than the other.
At the same time, it seems that there are countless cases in which the objects of our moral assessments do depend on factors beyond agents' control. Even though “moral luck” seems to be an oxymoron, everyday judgments suggest that there is a phenomenon of moral luck after all. As Nagel defines it, “Where a significant aspect of what someone does depends on factors beyond his control, yet we continue to treat him in that respect as an object of moral judgment, it can be called moral luck” (Nagel 1979, 59). To bring out the conflict with the Control Principle even more starkly, we will understand moral luck as follows:
(ML) moral luck occurs when an agent can be correctly treated as an object of moral judgment, despite the fact that a significant aspect of what he is assessed for depends on factors beyond his control.
We certainly seem to be committed to the existence of moral luck. For example, we seem to blame those who have murdered more than we blame those who have merely attempted murder, even if the reason for the lack of success in the second case is that the intended victim unexpectedly tripped and fell to the floor just as the bullet arrived at head-height. Since whether the intended victim tripped or not is not something in control of either would-be murderer, we appear to violate the Control Principle and its corollary.
It might be tempting to respond at this point that what people are really responsible for are their intentions or their “willings,” and that we are thus wrong to offer different moral assessments in this pair of cases. Adam Smith (1790/1976), for example, advocates this position, writing that
To the intention or affection of the heart, therefore, to the propriety and impropriety, to the beneficence or hurtfulness of the design, all praise or blame, all approbation or disapprobation, of any kind, which can justly be bestowed upon any action, must ultimately belong. (II.iii.intro.3.)
But this tempting response faces difficulties of its own. First, as we will see, the would-be murderers offer only one of many cases in which our intuitive moral judgment appears to depend on “results” beyond one's intentions, as Smith himself noted (II.iii.intro.5). And even more importantly, luck can affect even our “willings” and other internal states (Feinberg 1970, 34–38). As Nagel develops the point, there are other types of luck that affect not only our actions but also every intention we form and every exertion of our wills. Further, once these kinds of luck are recognized, we will see that not one of the factors on which agents' actions depend is immune to luck.
Nagel identifies four kinds of luck in all: resultant, circumstantial, constitutive, and causal.
Resultant Luck. Resultant luck is luck in the way things turn out. Examples include the pair of would-be murderers just mentioned as well as the pair of innocent drivers described above. In both cases, each member of the pair has exactly the same intentions, has made the same plans, and so on, but things turn out very differently and so both are subject to resultant luck. If in either case, we can correctly offer different moral assessments for each member of the pair, then we have a case of resultant moral luck. Williams offers a case of “decision under uncertainty”: a somewhat fictionalized Gauguin, who chooses a life of painting in Tahiti over a life with his family, not knowing whether he will be a great painter. In one scenario, he goes on to become a great painter, and in another, he fails. According to Williams, we will judge Gauguin differently depending on the outcome. Cases of negligence provide another important kind of resultant luck. Imagine that two otherwise conscientious people have forgotten to have their brakes checked recently and experience brake failure, but only one of whom finds a child in the path of his car. If in any of these cases we correctly offer differential moral assessments, then again we have cases of resultant moral luck.
Circumstantial luck. Circumstantial luck is luck in the circumstances in which one finds oneself. For example, consider Nazi collaborators in 1930's Germany who are condemned for committing morally atrocious acts, even though their very presence in Nazi Germany was due to factors beyond their control (Nagel 1979). Had those very people been transferred by the companies for which they worked to Argentina in 1929, perhaps they would have led exemplary lives. If we correctly morally assess the Nazi collaborators differently from their imaginary counterparts in Argentina, then we have a case of circumstantial moral luck.
Constitutive luck. Constitutive luck is luck in who one is, or in the traits and dispositions that one has. Since our genes, care-givers, peers, and other environmental influences all contribute to making us who we are (and since we have no control over these) it seems that who we are is at least largely a matter of luck. Since how we act is partly a function of who we are, the existence of constitutive luck entails that what actions we perform depends on luck, too. For example, if we correctly blame someone for being cowardly or self-righteous or selfish, when his being so depends on factors beyond his control, then we have a case of constitutive moral luck. Further, if a person acts on one of these very character traits over which he lacks control by, say, running away instead of helping to save his child, and we correctly blame him for so acting, then we also have a case of constitutive moral luck. Thus, since both actions and agents are objects of moral assessment, constitutive moral luck undermines the Control Principle when it comes to the assessment of both actions and agents.
Causal luck. Finally, there is causal luck, or luck in “how one is determined by antecedent circumstances” (Nagel 1979, 60). Nagel points out that the appearance of causal moral luck is essentially the classic problem of free will. The problem of free will to which Nagel refers arises because it seems that our actions—and even the “stripped-down acts of the will”—are consequences of what is not in our control. If this is so, then neither our actions nor our willing are free. And since freedom is often thought to be necessary for moral responsibility, we cannot be morally responsible even for our willings. Sometimes the problem is thought to arise only if determinism is true, but this is not the case. Even if it turns out that determinism is false, but events are still caused by prior events according to probabilistic laws, the way that one is caused to act by antecedent circumstances would seem to be equally outside of one's control (e.g., Pereboom 2002, 41–54, Watson 1982, 9). Finally, it is worth noting that some have viewed the inclusion of the category of causal luck as redundant, since what it covers is completely captured by the combination of constitutive and circumstantial luck (Latus 2001).
Upon reflection, it seems that we morally assess people differently for what they do (or who they are) when their actions and personal qualities depend on luck of all kinds. And it is not only in unusual cases like that of would-be murderers that people are subject to the various types of luck. For example, whether any of our intentions are realized in action or not depends on some factors outside of our control. Thus, if resultant luck undermines our assessments of moral responsibility, as the Control Principle suggests, then many of our everyday judgments ought to be abandoned. Still, applying the Control Principle to resultant luck continues to leave open the possibility that we are correctly assessed for things like our intentions, just not for the results of our intentions. But consideration of the other sorts of luck leads to more and more global skepticism about moral assessment. For example, circumstantial luck affects even our intentions, so it seems that we cannot be assessed in virtue of our intentions. Once again, though, we might still be able to retain the idea that we are morally assessable for something, even if only for what we would have intended in various situations. But reflection on constitutive luck and causal luck can make it seem as though we cannot be properly assessed for anything we do. For if who we are and therefore what we would have done are themselves subject to luck, then according to the Control Principle, we cannot be properly assessed even for those things. What is left as an object of assessment? As Nagel puts it, “[t]he area of genuine agency, and therefore of legitimate moral judgment, seems to shrink under this scrutiny to an extensionless point” (1979, 66.) He goes on,
I believe that in a sense the problem has no solution, because something in the idea of agency is incompatible with actions being events, or people being things. But as the external determinants of what someone has done are gradually exposed, in their effect on consequences, character, and choice itself, it becomes gradually clear that actions are events and people things. Eventually nothing remains which can be ascribed to the responsible self, and we are left with nothing but a portion of the larger sequence of events, which can be deplored or celebrated, but not blamed or praised (1979, 68).
If this is right, then we could not simply revise our everyday moral judgments in accordance with a more diligent application of the Control Principle; at best, if we adhere to the Control Principle, we should refrain from making any moral judgments. Not everyone shares this skepticism, and there is naturally a wide variety of responses to the challenge of how to reconcile our adherence to the Control Principle with our everyday judgments that commit us to the existence of moral luck. At stake are not only our seemingly ubiquitous practices of moral praise and blame, but also the resolution of other central debates in ethics, philosophy of law, and political philosophy.
2. Implications for Other Debates
Before turning to proposed solutions to the problem, it will be helpful to see just what rests on resolving the problem of moral luck.
2.1 The Justification of Laws and Punishment
Whether or not we accept, reject, or qualify the Control Principle has implications for the law, and for punishment in particular. The question of how resultant luck should affect punishment has been debated at least since Plato (The Laws IX, 876–877). According to the Control Principle, if results are not in our control, then our attributions of moral responsibility and blameworthiness should not be affected by them. And if, in addition, justified punishment tracks moral blameworthiness, then the degree of punishment allotted for crimes should not be based even in part on results. H.L.A. Hart puts this conclusion in the form of a rhetorical question: “Why should the accidental fact that an intended harmful outcome has not occurred be a ground for punishing less a criminal who may be equally dangerous and equally wicked?” (1968, 129). It turns out, however, that the idea that results should not be taken into account in determining punishment is in direct tension with a variety of criminal laws, including, for example, the differential punishment accorded attempted murder and murder in the United States. It is also in direct tension with parts of the tort law in the United States such as the differential treatment accorded the merely negligent person and the negligent person whose negligence leads to harm. Interestingly, however, the Model Penal Code takes a different approach for at least some offenses, prescribing the same punishment for attempts and completed crimes. (Model Penal Code § 2.05 cmt. at 293–95 (Official Draft and Revised Comments 1985). And this approach is favored by a number of legal theorists.
Now the line of reasoning sketched above that rejects any tracking of results in punishment depends not only on the Control Principle (or a modified version of it), but also on a thesis that limits justified punishment to the proper objects of moral blameworthiness. Both of these premises can, and have been, questioned. But the debate in legal theory about whether results should make a difference to punishment very often centers on the premise about control, and thus, the status of the Control Principle has important implications for the legal debates concerning differential punishment for attempts and completed crimes. (On this debate, see, for example, Alexander, Ferzan, and Morse (2009) Davis (1986), Feinberg (1995), Herman (1995), Kadish (1994), Lewis (1989), Moore (1997 and 2009), Ripstein (1999), and Yaffe (2010). On luck and tort law, see Waldron (1995), and for a wide-ranging discussion of moral luck and the law, Enoch (2010).)
It is also important to note that the implications of the status of the Control Principle for the law are not limited to results. For example, if we accept the Control Principle in unqualified form, and accept the premise constraining justified punishment to that for which people are morally blameworthy, then it might turn out that no one is morally blameworthy and so no punishment is ever justified.
2.2 Egalitarianism
Whether or not the Control Principle is true either in its general or in some restricted form also has implications for the debate over what, if anything, justifies egalitarianism. Let us understand egalitarianism as the view that a distribution of relevant goods that is more equal over a relevant population is more just than one that is less equal. Inspired by the work of John Rawls, some egalitarians have invoked the idea that our constitution and circumstances are out of our control in the justification of their view. For example, Rawls writes that
The existing distribution of income and wealth, say, is the cumulative effect of prior distributions of natural assets—that is, natural talents and abilities—as these have been developed or left unrealized, and their use favored or disfavored over time by social circumstances and such chance contingencies as accident and good fortune. Intuitively, the most obvious injustice of the system of natural liberty is that it permits distributive shares to be improperly influenced by these factors so arbitrary from a moral point of view. (Rawls 1971, p. 72.)
Egalitarians who treat luck in this way are sometimes called “luck egalitarians.” (For examples of various versions of luck egalitarianism, see Arneson 1997, 2001, Cohen 1989, Dworkin 1981 and 2000, Roemer 1996; for criticisms see Nozick 1974, Anderson 1999, Hurley 2001, and Scheffler 2003.) It is often difficult to see exactly how the appeal to constitutive luck is meant to function in various arguments for egalitarianism. There are two very general ways the reasoning might go: a “positive” and a “negative” way (Nozick 1974). According to one positive line of reasoning, it is first observed that one's natural talents, circumstances of birth, and so on are things that are beyond one’s control, and at the same time, if a natural “free market” system operates, these circumstances give rise to many advantages and disadvantages relative to others. By the Control Principle, one is not responsible for these advantages and disadvantages. Further, it is wrong for people to have advantages and disadvantages for which they are not responsible. Therefore, justice requires a more egalitarian redistribution of goods to rectify this wrong. Although this line of reasoning has received much criticism, it is arguable that a weaker, and so less vulnerable “negative” line of reasoning is really behind much of luck egalitarianism (see, for example, Arneson 2001).
The “negative” luck argument for egalitarianism is really a rebuttal to the objection that people should not be deprived in the name of egalitarianism of what they have earned. The argument goes like this: Take as a starting point a presumption in favor of equality of condition. Next observe, as before, that one's natural talents, circumstances of birth, and so on are things beyond one’s control, and, again, that these factors often give rise to advantages and disadvantages relative to others. Therefore, by the Control Principle, one is not responsible for many advantages and disadvantages. If one is not responsible for these, then one is not deserving of them. And if one is not deserving of them, then it is not wrong to redistribute goods in a more egalitarian way that eliminates many advantages and disadvantages.
The explicit appeal to the Control Principle in both of these lines of reasoning shows ways in which the plausibility of Luck Egalitarianism depends on the resolution of the problem of moral luck. It is also notable that some luck egalitarians attempt to draw a line between certain sorts of luck; for example, it is sometimes argued that if one suffers a great financial setback due to one’s choice to engage in high-stakes gambling, then there might be circumstances in which it would be wrong to seek to treat one in the same way as another whose equal suffering was brought on by, say, a devastating earthquake. It might be that underlying this move is acceptance of a restricted version of the Control Principle; for example, one that allows that one can be responsible for one's choices and their expected consequences, but not for the results of one's choices that are in large part beyond one's control. Here, too, it is clear that how one resolves the problem of moral luck—whether one rejects the possibility of moral luck altogether, accepts it in all forms, or accepts certain kinds and not others—has implications for the ultimate success of Luck Egalitarianism. Thus, much is at stake in the resolution of the problem of moral luck. Before turning to suggested solutions, a brief bit of ground-clearing will be necessary.
3. Kinds of Moral Assessment
The Control Principle states that we are morally assessable only to the extent that what we are morally assessed for is under our control. But it is important to recognize that there are many different kinds of moral assessment. For example, there are judgments about a person's character, for example, as “good” or “bad” (sometimes called “aretaic” judgments). There are also judgments of states of affairs that concern people's actions as “ good ”or “ bad ”(sometimes called “axiological” judgments. Then there are judgments of actions as “right” or “wrong” (sometimes called “deontic” judgments). There are also judgments of responsibility, blame, and praise. As we will see, this category can be further divided in various ways.
Distinguishing between the various notions of moral assessment allows for the possibility that the Control Principle should be read as applying to some, but not to other forms of moral assessment. For example, some argue that there is a perfectly acceptable form of moral luck which does not conflict with the true spirit of the Control Principle, namely, luck in what you are responsible for (e.g., Richards 1986, Zimmerman 2002). For example, it will be readily admitted by many that the successful murderer can be responsible for a death, whereas the one who unsuccessfully attempts murder is not responsible for a death. At the same time, both could be equally responsible, or blameworthy, in degree (Zimmerman 2002, 560) or both could be equal in their moral worth (Richards 1986, 171, Greco 1995, 91). If the most important kind of moral assessment is, say, one's moral worth, then the Control Principle can be suitably restricted to apply to assessments of moral worth. As will become clear, a number of responses to the problem of moral luck appeal to the general strategy of distinguishing among different forms of moral assessment. Most focus on two families of moral assessment: (i) the family that includes responsibility, blame, and praise for actions and/or for one's own traits or dispositions, and (ii) the family that includes the notion of the moral worth of an agent and the moral quality of her character. (But see Zimmerman 2006 for a recent discussion of luck and deontic judgments.)
4. Responding to the Problem: Three Approaches
There are three general approaches to responding to the problem of moral luck: (i) to deny that there is moral luck despite appearances, (ii) to accept the existence of moral luck while rejecting or restricting the Control Principle, or (iii) to argue that it is simply incoherent to accept or deny the existence of some type(s) of moral luck, so that with respect to at least the relevant types of moral luck, the problem of moral luck does not arise.
Some who respond to the problem of moral luck take a single approach to all kinds of luck. But many take a mixed approach; that is, they embrace one approach for one kind of luck and another approach for another kind of luck, or address only a certain type(s) of luck, while remaining silent about the other types. Is taking a mixed approach legitimate? After all, it seems that if the Control Principle is true, then there is no moral luck, and if it is false, then there can be any type of moral luck. But, alas, matters are not necessarily so simple. It is possible at least in theory to offer a principled reason for qualifying the Control Principle so that it applies only to certain sorts of factors and not others. At the same time, as we will see, providing just such a principled way of distinguishing certain kinds of luck from others turns out to be a formidable task.
4.1 Denial
Most of those who deny that one or more types of moral luck exist are those who seek to preserve the centrality of morality in our lives. But it is also possible to adopt a position of denying the possibility of moral luck while at the same time showing that the Control Principle, while true, prevents morality from playing the central role we might have hoped for it. Something like this position seems to be the one Williams adopts in his (1993) “Postscript” to “Moral Luck,” for example.
4.1.1 Denying Moral Luck and Preserving the Centrality of Morality
Let us begin with the first and larger group of those who embrace the approach of denying the existence of moral luck. One of their main tasks is to explain away the appearance of moral luck. A second main task is to paint a plausible and coherent picture of morality that avoids luck.
An important tool for those who wish to explain away the existence of moral luck is what Latus (2000) calls the “epistemic argument” (see Richards, Rescher, Rosebury, and Thomson.) To see how it goes, let us begin by focusing on resultant luck. Why do we feel differently about the successful and unsuccessful murderers? Because, according to the epistemic argument, we rarely know exactly what a person's intentions are or the strength of her commitment to a course of action. One (admittedly fallible) indicator is whether she succeeds or not. In particular, if someone succeeds, that is some evidence that the person was seriously committed to carrying out a fully formed plan. The same evidence is not usually available when the plan is not carried out. Thus, rather than indicating our commitment to cases of resultant moral luck, our differential treatment of successful and unsuccessful murderers indicates our different epistemic situations with respect to each. If we were in the unrealistic situation of knowing that both agents had exactly the same intentions, the same strength of commitment to their plans, and so on, then we would no longer be inclined to treat them differently. Thomson represents a number of those who employ this strategy when she asks, “Well do we regard Bert [a negligent driver who causes a death] with an indignation that would be out of place in respect to Carol [an equally negligent driver who does not]? Even after we have been told about how bad luck figured in his history and good luck in hers?” And Thomson answers: “I do not find it in myself to do so” (1993, 205). Not everyone shares this intuition, however, as we will see in the next section.
The epistemic argument can be extended to circumstantial luck. Consider again the Nazi sympathizer, and a counterpart who moved in 1929 to Argentina on business. The counterpart has exactly the same dispositions as the Nazi sympathizer, but lives a quiet and harmless life in Argentina. According to this line of reasoning, while it is true that the counterpart is not responsible for the same deeds as the Nazi sympathizer, he should be judged precisely for what he would have done. Richards argues that we do judge people for what they would have done, but that what they do is often our strongest evidence for what they would have done. As a result, given our limited knowledge, we might not be entitled to treat the counterpart in the same way as the Nazi sympathizer, even though they are equally morally deserving of such treatment (Richards 1986, 174 ff.). Thus, circumstantial luck, like resultant luck, affects the basis available to us when we judge agents, but does not affect what those agents deserve.
It is hard to see how the argument can be extended further to cover constitutive or causal luck. But even if the epistemic argument is limited in this way, it can still be part of a good overall strategy of responding to the problem of moral luck insofar as it is possible to take a mix-and-match approach to different kinds of luck.
A second strategy for explaining away moral luck is most naturally applied to resultant luck. Those who adopt this strategy argue that it is understandable or even appropriate to feel differently about the driver who kills a child than about the one who does not. What is not appropriate is to offer different moral assessments of their behavior (e.g., Rosebury, Richards, Wolf, Thomson).
Williams elucidates a notion of “agent-regret,” a sentiment whose “constitutive thought” is a subject's first-person thought that it would have been much better had she done otherwise. Agent regret also requires a certain sort of expression that is different from that of what we might call “bystander regret.” For example, it might include the willingness to compensate a person who was harmed by one's actions. In the case of a lorry driver who, through no fault of his own, runs over a child, Williams writes, “we feel sorry for the driver, but that sentiment co-exists with, indeed presupposes, that there is something special about his relation to this happening, something which cannot merely be eliminated by the consideration that it was not his fault” (1981, 43).
It is possible to take this thought still further and argue that it is reasonable to expect and perhaps even demand that one who kills the child respond in a different way from the other. For example, Wolf argues that there is a “nameless virtue” which consists in “taking responsibility for one's actions and their consequences” (2001, 13). It is the virtue of taking responsibility in some sense for the consequences of one's actions, even if one is not responsible for them. In some ways it is akin to the virtue of generosity in that it “involves a willingness to give more…that justice requires” (14). To take another example, Richards suggests that we often have negative feelings about those who cause harm, even when we realize that they are not deserved, and that these can be feelings we ought to have. For example, it ought to be distressing for a parent to encounter a girl who accidentally dropped your baby, even if you know that no one could have held on (1986, 178–79). The feelings that both agents and observers naturally do or even ought to have can easily be confused with judgments that commit us to the existence of moral luck. Yet once we distinguish these legitimate feelings from moral judgments, we can and should eliminate the judgments that entail a commitment to moral luck. Again, this strategy is most naturally applied to resultant luck.
Recently, critics of this strategy have objected to it on a variety of grounds. For example, it has been argued against Wolf's view, in particular, that once we acknowledge the appropriateness of greater self-blame in cases of greater harm, no good reason for denying moral luck remains, and indeed we have good reason for accepting it. (See Moore (2009, 31 ff.) It has also been argued that Wolf's description of our phenomenology is at best incomplete: it is not merely that we wish people to blame themselves more when they cause greater harm, but that we judge them to be more blameworthy. Our judgments of greater responsibility also require explaining away. (See Domsky 2004.)
A variant of this strategy employs the idea that one can justify differential treatment of, say, the negligent driver who hits a child and one who does not, even if both are equally morally blameworthy. For example, Henning Jensen (1984) argues that while both are equally culpable, there are consequentialist reasons for not subjecting the first negligent driver to the same degree of blame behavior. Since we all take some risks, and some are less likely to lead to harm than others, to blame everyone for simply taking such risks would require such a high standard of care as to risk destroying our ability to function as moral agents. On the other hand, requiring punishment for or compensation from those who do cause harm is required to provide a “restorative value” for those agents and preserve their integrity.
A third strategy is to point out that we mistakenly infer moral luck from legal luck. While there might be good reasons for the law to treat people differently even if what they do depends on factors beyond their control, we (understandably) make the mistaken inference that the law directly reflects correct moral assessment in such cases. For example, there are a number of reasons why the law might justifiably punish successful crimes more severely than merely attempted ones, including the balancing of deterrence and privacy (Rosebury 521–24). If reasons like this provide the justification for the differential treatment of such cases in the law, then it would indeed be wrong to infer that the successful and unsuccessful murderers are deserving of different moral assessments. However, the fact that we do make such a mistaken inference explains why we often commit ourselves to the existence of moral luck, when reflection can show that doing so is a mistake.
In addition to explaining how there can be an appearance of moral luck, despite the fact that there is not any, some of those who wish to deny the existence of moral luck take on the task of offering a coherent and plausible picture of morality that avoids luck.
Some of those engaged in the free will debate have denied the existence of causal, and perhaps also of constitutive, moral luck by offering a distinctive metaphysical account of human agency. (See, for example, Chisholm, Taylor, Clarke, and O'Connor. See also Pereboom who argues that such an account is coherent, but not true.) The view is known as “Agent-Causal Libertarianism,” and the basic idea is that agents themselves cause actions or at least the formation of intentions, without their being caused to do so. Thus, the agent herself as a substance, exercising her causal powers is an undetermined cause of her intentions. On some agent causal views, only the agent, as opposed to events caused by other events is the cause of the intention (e.g., O'Connor), while on another view, the agent acts in tandem with events that probabilistically cause the action (Clarke 1993). Particularly on the first sort of view, we seem to avoid the conclusion that our actions must depend on causal factors that are beyond our control. At the same time, it is not clear exactly how the move to agent causation is supposed to restore the kind of control we seek. For we might ask why we should consider the agent cause in control of her actions, while we can imagine that other substance causes (e.g., tables or billiard balls) would not be in control of what they cause. It might be stipulated that the exercise of the particular causal power to cause intentions simply is an exercise of control, but we need further details to see that the challenge has not been stipulated away. (See Clarke 2005 and Mele 2006 for recent discussions of agent causation and luck.) It is also important to note that Agent-Causal views are consistent with actions and even intentions depending in part on factors beyond one's control, such as the reasons people have available at the time of decision or action.
In a very different way, as we have seen, it is possible to take on a part of the task of describing a coherent picture of luck-free morality by identifying an object of moral assessment in the case of circumstantial luck. For example, Richards suggests that people should be assessed for what they would have done in different circumstances. More fundamentally, people should be assessed for their characters, of which their actions in different circumstances are manifestations.
Zimmerman begins where Richards leaves off, proposing to pursue “the implications of the denial of the relevance of luck to moral responsibility” to their “logical conclusion” (2002, 559). With the possible exception of some kinds of constitutive luck, Zimmerman rejects the possibility of moral luck of all four kinds while proposing a coherent picture of moral assessment. He rejects the possibility of resultant luck by first acknowledging that a man who (by luck) succeeds in his plan to cause harm is responsible for more things than one who (by luck) fails to carry out an identical plan. But, according to Zimmerman, we must distinguish between scope and degree of responsibility. Both men are responsible to the same degree, and it is this kind of moral assessment to which the Control Principle ought to apply. When it comes to circumstantial luck, things are more difficult. For when it comes to cases like those of resultant luck in which we want to hold people responsible, we can find something to hold them responsible for, namely, their plans or intentions or attempts. However, when it comes to cases of circumstantial luck, such as the Nazi collaborator and his counterpart, there are no counterpart plans or intentions or attempts that have simply failed to come to fruition. Zimmerman suggests that there is nothing that we hold the counterpart responsible for; in this case, the scope of the agent's responsibility is 0. But we can and should still hold him responsible to the same degree as the Nazi sympathizer. He is responsible tout court even if he is not responsible for anything (2002, 565). He is responsible in the sense that his moral record is affected for better or worse in virtue of something about him. For there is something in virtue of which he is responsible, namely, his being such that he would have freely performed the very same wrong actions had he been in the same circumstances as the Nazi sympathizer.
This reasoning can be extended still further to cover the case of constitutive and even one kind of causal luck. Suppose that Georg does not kill Henrik, and George does kill Henry. Further suppose that “the reason for Georg's not killing Henrik was that he was too timid, or that he had a thick skin and Henrik's insults did not upset him in the way that Henry's insults upset George, or that he was deaf and simply did not hear the insults that Henrik hurled his way. If it is nonetheless true that Georg would have freely shot and killed Henrik but for some such feature of the case over which he had no control, then, I contend, he is just as responsible, in virtue of this fact, as George is” (2002, 565). Zimmerman acknowledges that there are features of one's constitution that are essential to who one is, although he denies that timidity, thick-skinnedness, and so on count among them. However, if such features are essential, then it will not be true to say that had Georg lacked them, he would have freely killed Henrik. Since Georg is responsible, on Zimmerman's view, precisely in virtue of such counterfactuals being true, he would be absolved of responsibility if such features were essential to him. For this reason, Zimmerman concedes that “the role that luck plays in the determination of moral responsibility may not be entirely eliminable…” (2002, 575).
Finally, Zimmerman goes on to claim that his reasoning applies even to cases in which a person's actions are causally determined. If it is true that, say, Georg would have killed Henrik if his deterministic causal history, over which he has no control, had been different, then Georg is as responsible as he would have been had he killed Henrik in a world that was not determined. The upshot of the application of Zimmerman's reasoning is that we are all responsible, blameworthy, and even praiseworthy in ways we have never imagined. If Zimmerman is right, there are countless counterfactuals that apply to each and every one of us, in virtue of which we are responsible to one degree or another. The view thus takes the Control Principle extremely seriously, and applies it in the broadest possible way. The price we pay for “taking luck seriously” is that our everyday moral judgments are, if not always mistaken, at the very least radically incomplete.
A number of objections can be raised to Zimmerman's view, including (i) that at least large classes of the counterfactuals in virtue of which he thinks people are responsible lack truth value (e.g., Adams 1977, Nelkin 2004, and Zimmerman 2002, 572), and (ii) that he is simply mistaken that one can be responsible without being responsible for anything. Even if one or both of these objections are on target, Zimmerman's article is very helpful in showing what an attempt to follow out the denial of moral luck to its logical conclusion looks like.
Unlike Zimmerman, most of those who adopt the denial strategy do so only for certain sorts of moral luck. By treating all sorts of luck in the same way (with the exception of constitutive luck with respect to one's essential properties), Zimmerman challenges those who adopt this strategy to defend the drawing of the line between resultant and other sorts of luck. As we will see, this very same challenge is also issued by those who take a diametrically opposed position and accept all forms of moral luck.
4.1.2 Denying Moral Luck and Setting Aside Morality in Favor of Ethics
Before turning to the approach of accepting the existence of moral luck, it remains to consider the view ascribed earlier to Williams' “Postscript” (1993). Extracting Williams' position on “Moral Luck” is a notoriously difficult task, made easier only by Williams' own acknowledgment in the “Postscript” that his original article “may have encouraged” some misunderstandings (251). Many commentators have read Williams as advocating the position that moral luck exists and is deeply threatening to morality. There is certainly a line of reasoning in Williams' original article that suggests this (see 37–42, 51–53). But in the Postscript, Williams makes a distinction between morality and ethics that allows him to deny the existence of moral luck, thus preserving a certain integrity for morality.
Williams understands morality to embody the Kantian conception of it described above, accepting that the essence of the Control Principle is “built into” morality so understood (1993, 252). At the same time, examples like the Gauguin case described earlier show that one can be rationally justified in one's decision in virtue of its outcome. Further, such a case shows that our overall value judgment of someone's decision can depend on factors beyond the control of the agent. We must conclude, then, that there is a kind of value that competes with, if not trumps, moral value. And if that is right, then we must give up “the point of morality” so understood, namely, to “provide a shelter against luck, one realm of value (indeed, of supreme value) that is defended against contingency” (1993, 251, emphasis mine). It seems that morality can only insulate itself from luck at the expense of foregoing supreme value. Once we acknowledge this cost, we can keep morality intact (although skeptical doubts about its ability to resist luck can still be raised), but we have lost our reason to care about it. Instead, Williams suggests, we should care about ethics, where ethics is understood to address the most general question of how we ought to live.
Questions can be raised about this line of reasoning. For example, we can ask whether there is any sense in which Williams' Gauguin ought to have left his family, despite the fact that the result was so welcome. If there is not, then Williams has not shown that morality competes with, or is trumped by, some other value. From the other direction, we can ask whether Williams is right that morality loses its point if it is not the supreme source of value. Of course, even if Williams' reasoning is unsound, the conclusion could still be correct, and others have offered different routes to it.
The idea that we ought to care about ethics, understood as Williams does, finds inspiration in the work of Aristotle. Aristotle is concerned with the nature of the good life in the broadest sense—in what he calls “eudaimonia,” often translated as “happiness”. Aristotle defends the idea that happiness consists in being a virtuous person over a complete life, and, in turn, the idea that being a virtuous person requires not only that one have virtuous qualities and dispositions, but also that one act on them. Luck enters into the account in at least two ways. First, on Aristotle's account, one becomes a virtuous person by undergoing the right kind of upbringing and training. Since whether one receives this training is at least to some extent beyond one's control, one's ability to live a virtuous life is deeply dependent on luck. Second, the fact that being a virtuous person requires the performance of certain kinds of activities means that the world must cooperate in various ways in order for one to be truly virtuous, and so be truly happy. Aristotle writes that happiness “needs the external goods as well; for it is impossible, or not easy, to do noble acts without proper equipment” (1984 NE 1099a 31–33). For example, in order to engage in acts of generosity, one must have resources at one's disposal to share. And since having the right equipment is at least to some extent a matter of circumstantial luck, the value of one's life itself will depend in part on what is not in one's control. On one interpretation of Aristotle, luck enters into the account in yet a third way. Acting in accordance with virtue does not suffice for happiness, on this interpretation, although it is the “dominant component” of Aristotle's account of happiness (Irwin 1988, 445). According to this view, one must also have a minimum provision of external goods (e.g., health, security, access to resources) whose contribution to happiness is independent of their making virtuous activity possible. If this is right, then the value of one's life will depend at least in part on factors beyond one's control. In sum, while there is some dispute about whether Aristotle thought more than a life of virtuous activity is required for happiness, it is clear that luck plays a significant role in determining both whether people are truly virtuous and whether people's lives are good in the broadest sense. Hence, “the fragility of goodness” (Nussbaum).
4.2 Acceptance
All of those who accept the existence of some type of moral luck reject the Control Principle and the Kantian conception of morality that embraces it. As a result, they must either explain how we can revise our moral judgments and practices in a coherent way or show that we are not committed to the Control Principle in the first place.
4.2.1 Accepting Moral Luck and Revising our Practices
Some who accept luck argue that doing so requires a significant change in our moral practices. Browne (1992), for example, suggests that if the Control Principle is false, we ought not to respond to an agent's wrongdoing with anger and blame that is “against” him, but rather with anger that does not include hostility or the desire to punish. Nevertheless, we can still respond to the successful murderer with more of the “right” kind of anger than we feel toward the unsuccessful one. One question that might be raised here is whether we are left with enough of our ordinary conception of morality to include genuine notions of blame and responsibility.
4.2.2 Accepting Moral Luck without Revision
Others suggest that the Control Principle does not have nearly the hold on us that Nagel and Williams assume, and that rejecting it would not change our practices in a significant way. Among these are some who focus on the free will debate and others who take on the broader problem of moral luck directly.
4.2.2.1 Accepting Moral Luck and the Free Will Debate
A large group who accept moral luck do not explicitly address the problem of moral luck as so formulated because they focus on what Nagel identifies as a narrower issue, namely, that of free will. One traditional problem of free will is posed by the following line of reasoning: if determinism is true, then no one can act freely, and, assuming that freedom is necessary for responsibility, no one can be responsible for their actions. Compatibilists have argued that we can act freely and responsibly even if determinism is true. Since most do not adopt Zimmerman's radical account of moral assessment in which one can be responsible despite not being responsible for anything, they admit the existence of causal moral luck. If, as some have argued, causal luck is exhausted by constitutive and circumstantial luck, then they also accept that there can be these sorts of moral luck, as well.
A basic compatibilist strategy is to argue that agents can have control over their actions in the sense required for freedom and/or responsibility even if they do not control the causal determinants of those actions. For example, if one acts with the ability to act in accordance with good reasons (Wolf 1990) or if one acts with “guidance control” which consists in part of acting on a reasons-responsive mechanism for which one has taken responsibility, (Fischer and Ravizza 1998), one can be responsible for one's actions. The key move here is to distinguish between different kinds of factors over which one has no control. If one's actions are caused by factors that one does not control and that prevent one from having or exercising certain capacities, then one is not responsible. However, if one's actions are caused by factors that one does not control, but that do allow one to have and exercise the relevant capacities, then one can be “in control” of one's actions in the relevant sense, and so responsible for one's actions.
Interestingly, compatibilists are often silent on the question of resultant and circumstantial moral luck, although these forms of luck might represent an underutilized resource for them. For if it turns out that the luck—or lack of control—delivered by determinism is but one source of luck among others, then determinism does not embody a unique obstacle to free will and responsibility, at least when it comes to control. This is to expand the application of a widely used compatibilist strategy to show that when it comes to causal luck, compatibilists are not alone.
For within the free will debate, compatibilists are not alone in accepting the existence of certain types of luck. Many libertarians assume that our actions are caused by prior events (not themselves in our control) in accordance with probabilistic laws of nature (see, for example, Kane 1996, 1999, Nozick 1981). Given this view, it is natural to conclude that if determinism is false, there is at least one kind of luck in what sort of person one decides to be and so in what actions one performs. That is, there is luck in the sense that there is no explanation as to why a person chose to be one way rather than another. At the same time, Kane, for example, denies that there must be luck in the sense that one's choices are flukes or accidents if determinism is false. In Kane's view, what is important is to be free from luck of the second kind. For even if one's action is not determined, it can still be the case that the causes of one's action are one's own efforts and intention. And if one's action is caused by one's own efforts and intentions, then one's action is not lucky in the sense of being a fluke or accident. But while this shows that one's actions can be free of luck of an important kind, it still leaves unaddressed luck of a third kind, namely the kind at issue in the moral luck debate: the dependence of agents' choices on factors beyond their control. And it appears that on the libertarian view in question, our choices are indeed subject to luck of this sort. (See Pereboom (2002) for a discussion of the similar burdens shared by compatibilists and this sort of libertarian.) Only the agent-causal libertarians discussed above offer an account that aims specifically at eliminating a type of moral luck.
4.2.2.2 Accepting Moral Luck and Distinctive Conceptions of Morality
It is also possible to argue that we are not committed to the Control Principle by taking on the problem of moral luck directly.
One strategy is to argue that moral luck is only a problem for an overly idealized conception of human agency. But once we adopt a realistic conception of human agency, the problem evaporates. Margaret Urban Walker (1991) argues in this vein that moral luck is only problematic for a conception of moral agents as “noumenal” or pure (238). In contrast, adopting a conception of morality that applies to human beings in all of their impurity will not be threatened by moral luck. According to Walker, the Control Principle is far from obvious, and we would not want to live in a world in which it held sway. The argument appears to rest on the idea that without moral luck, we would lack several virtues that allow us to help each other in most essential ways. Our very reactions to moral luck can be virtuous. For example, by accepting that our “responsibilities outrun control,” we are able to display the virtue of dependability by accepting that we will be there for our friends, even if their needs are not in our control. In contrast, pure agents who are only responsible for what they control “may not be depended on, much less morally required, to assume a share of the ongoing and massive human work of caring, healing, restoring, and cleaning-up on which each separate life and the collective one depend.” (247). Thus, if we focus on our actual moral commitments, we will see that the Control Principle is neither attractive nor necessary for morality.
It is not obvious that a world in which people denied the existence of moral luck would be as bleak as the one Walker envisions. Moral luck skeptics have material with which to question Walker's claim. For example, those who deny resultant moral luck can still agree that agents have an obligation to minimize their risks of doing harm, and those who deny circumstantial moral luck can still agree that agents have an obligation to cultivate qualities that prepare them to act well in whatever circumstances arise.
A second strategy for rejecting the Control Principle turns Nagel's argument on its head by taking as a starting point ordinary judgments and reactions that reveal our implicit rejection of the Control Principle. Adams (1985) adopts this strategy, drawing our attention to common practices, such as blaming people for their racist attitudes even if we do not think that such people are in control of their attitudes. Since Adams focuses primarily on agents' states of mind that have intentional objects such as anger and self-righteousness, it is possible to see him as accepting the existence of constitutive moral luck in particular. But it is also possible to adopt the same sort of strategy for other sorts of luck, including resultant luck. Moore (1997 and 2009), for example, points to the fact that we resent those who succeed in causing harm more than those who do not, we feel greater guilt when we ourselves cause harm, and when we face decisions, we feel that the consequences of matter to the moral quality of our choices. According to Moore, the best explanation of these reactive attitudes, such as guilt and resentment, is that their objects are genuinely more blameworthy.
Now opponents who deny the existence of moral luck have ways of explaining away these phenomena. When it comes to cases of constitutive luck, like the case of the racist, they can say that we confuse agents' blameworthiness for their character and attitudes with blameworthiness for their actions that manifest these offending attitudes and for their failure to take steps to eliminate them. On reflection, we can see that we ought to blame the racists only for their actions or omissions, not for the attitudes themselves over which they have no control. Similarly, as we saw earlier, when it comes to resultant luck, moral luck skeptics have a variety of strong alternative explanations of our judgments and emotional responses. It is possible that there is a disagreement here at the level of intuitions: some find it easier on reflection to reject moral judgments that depend on results than others. Further, those accepting resultant moral luck face a challenge of articulating a positive theory of how exactly results affect one's moral status while at the same time accounting for our intuitions. Sverdlik (1988) argues that it is not obvious how such a challenge can be met.
At this point in the debate, those who accept moral luck offer ordinary judgments and responses in their defense, while moral luck skeptics offer alternative explanations of those practices and hold up the Control Principle itself, together with other reflective intuitive judgments, as reason to reject moral luck. We seem to have something of a stalemate. So it is no surprise that those who accept moral luck tend not to rely exclusively on ordinary judgments to make their case, but rather go on to try to undermine the Control Principle in other ways.
Another way of trying to undermine the appeal of the Control Principle itself is to show how it might be mistaken for something else that is more plausible. For example, Adams (1985) recognizes that there are limits to what we can be responsible for, and writes that the states of mind “for which we are directly responsible are those in which we are responding, consciously or unconsciously, to data that are rich enough to permit a fairly adequate ethical appreciation of the state's intentional object and of the object's place in the fabric of personal relationships” (26). Thus, according to Adams' conception of morality, adherents of the Control Principle are correct in an important respect, namely, in their understanding that what one is responsible for springs in the right way from oneself. But this requirement is more general than a strict requirement of control, and although easily confused with the Control Principle, is superior to it, on this view.
Adopting the same general strategy, Moore (1997) identifies still other principles with which the Control Principle might be confused. He points out that when we use the word “luck” in the context of moral assessment, we tend not to mean that the person lacked control over what he did, but rather that what happened was far off of “some moral baseline of the normal” (213). For example, consider two would-be murderers, one of whom fires his gun and hits his target, and the other of whom fires in the same way, from the same distance, and so on, but whose bullet is deflected by an unexpected and unusually strong wind. Moore suggests that the first gunman is not “lucky” in the ordinary sense, even though it is true that whether a hurricane-force wind arose or not was not in his control. According to Moore, there is something intuitively right about morality being immune to luck, but only if we understand “luck” in the sense of “freakishness.” Further, the successful murderer is “in control” of his action in the normal sense of the word “control,” even though he doesn't control the wind. Thus, while we do care about luck and control in making both moral and legal assessments, they aren't Nagel's concepts, on this view. Thus, according to Moore, there is no contradiction in our everyday commitments.
Now those who think we are naturally drawn to the Control Principle can respond by pointing out both the intuitive plausibility of the principle in the abstract and the cases described earlier that seem to support it. They might also accept that Adams and Moore have pointed out further necessary conditions for responsibility, while still maintaining that the Control Principle is true. Again, differing intuitions about cases and about the Control Principle have the potential to make a big difference to one's view at this point.
Michael Otsuka offers yet another principle in place of the Control Principle: One is only blameworthy in cases in which one had the kind of control that would have allowed one to be entirely blameless. Consistent with this is a kind of moral luck, however: one's blameworthiness can vary in degree as a function of harm done, where harm done may be affected by what is not in one's control. Although one cannot be blameworthy if one lacked the control necessary to avoid blameworthiness, one's degree of blameworthiness can increase if the risk one takes comes out badly due to circumstances one could do nothing to avoid. For example, in the case of the two assassins, both are blameworthy, but, Otsuka argues, the one who hits and kills his target is more blameworthy. In sketching the view, Otsuka draws a parallel with Dworkin's (1981) treatment of option luck in the debate over egalitarianism. In that debate, a distinction is drawn between option luck (“a matter of ...whether someone gains or loses through accepting an isolated risk he or she should have anticipated and might have declined”) and brute luck (“a matter of how risks fall out that are not that sense...gambles).” If one's luck is just brute-one did not assume a risk, as when one has done everything a careful driver would do, and due to sheer luck, a dog runs into the street and one drives over it-one is not blameworthy. But if one assumes a risk by knowingly and freely driving recklessly, and, as a result, one kills a dog, then one is blameworthy. And, further, one might be more blameworthy in the case in which one kills the dog than in the case in which one takes the same risk but luckily reaches home without hitting anything. It would be reasonable, on Otsuka's view, for the dog owner whose dog is killed to be more resentful than the one whose dog escapes, and this supports the conclusion that the driver who kills the dog is more blameworthy than the one who does not.
The parallel to option an brute luck is suggestive, but a defender of the unqualified Control principle has resources here. Appealing to the distinction between scope and degree, one might grant that the reckless driver is, importantly, responsible for more things (including a death), but not more blameworthy. In fact, the parallel to the treatment of option luck in the debate about distributive justice may fit best if we are interested in what we are responsible for, rather than how responsible we are. Further, we have seen reason to think that on reflection we should not blame one reckless driver more than another. One might question Otsuka's premise that degree of blameworthiness is to be understood in terms of appropriate degree of attitudes such as resentment (or even the weaker premise that the degree of blameworthiness tracks the appropriate degree of such attitudes). But even if we accept this premise, we might conclude that while it is understandable that one dog owner would be more resentful than the first, more resentment is not actually justified. This observation takes us back to the subtle nature of the dialectic.
In adjudicating this debate between those defending the Control Principle and those defending alternative principles, we can ask just how much weight should be given to our natural reactions to cases, and, in particular, to our reactive attitudes, such as resentment and guilt. At least in some cases, these can be tempered when we reflect explicitly on key features of cases, and our initial responses can be revised in light of these reflections, together with reflection on general principles.
Notably, there has recently been an attempt by philosophers to appeal to results from empirical psychology to explain away some set of intuitions or other, and this strategy has been applied in the area of moral luck in particular. For just two examples, see Domsky (2004) and Royzman and Kumar (2004) whose explanations in different ways support the preservation of our adherence to the Control Principle, and see Enoch and Guttel (2010) for a reply to both. Psychologists and experimental philosophers have also simply tried to offer explanations of our intuitions, particularly of ones that appear to conflict as we find in the debate about moral luck. For example, see Cushman and Green (2012), who offer an explanation of apparently conflicting intuitions about moral results luck in terms of two dissociable processes, and Björnsson and Persson (2012), who offer an explanation in terms of shifting explanatory perspectives. It is worth noting, however, as several of these authors do themselves, that even if we were confident in our possession of psychological explanations of our intuitions, there would still be philosophical work to do to sort out what the normative facts are. And given the variety of proposals and results, it seems we are not in a position to have that kind of confidence.
There is a final argument in favor of the acceptance of moral luck of a very different kind that might ultimately help decide the issue in one direction or the other. It explicitly encompasses every kind of luck and thus poses a deep and difficult challenge to moral luck skeptics, particularly the large group who focus exclusively on resultant luck. The main idea is that rejecting resultant luck, but not other sorts of luck, is an unstable position (Moore 1997). In a nutshell, one cannot find a principled place to draw the line at refusing to accept moral luck. In effect, this argument is Nagel's argument in reverse. Begin by observing that we lack control over everything: the results of our actions, our circumstances, our constitution, and our causal history. If we are to avoid moral skepticism, then we must accept moral luck in some areas, and if we do that, then we ought to accept it in the area of results. Particularly if we accept that we are not predisposed to accept the Control Principle in the first place, then we ought to accept luck in all areas, thereby avoiding moral skepticism.
Now even if no one has adequately defended a way of drawing a line between different sorts of luck, it is not obvious that the door has been closed on all future attempts. Thus, one way of seeing this argument is as a shift-the-burden one. Those who wish to draw a line between different sorts of moral luck must offer a deeper rationale for doing so than has yet been offered.
4.3 Incoherence
According to this approach, it is simply incoherent to accept or deny the existence of some type(s) of moral luck. This approach has been used for constitutive luck in particular.
Among those who wish to preserve the centrality of morality in our lives, many have appealed to an idea formulated by Nicholas Rescher (1993), according to which “[o]ne cannot meaningfully said to be lucky in regard to who one is, but only with respect to what happens to one. Identity must precede luck” (155). It is easy to take Rescher's point out of context without realizing that he is working with a notion of luck that differs from the notion of “lack of control.” According to Rescher, something is lucky if (i) it came about “by accident” where this seems to mean something like “unplanned” or “unexpected” or “out of the ordinary” and (ii) the outcome “has a significantly evaluative status in representing a good or bad result, a benefit or loss”(145). Taken this way, it does seem at least very odd to say that one's identity is (or is not) a matter of luck. But it is less clear that there is anything odd—let alone incoherent—about saying that one's identity is not a matter within one's control.
Could there nevertheless be some truth to Rescher's claim even if we understand “luck” as “out of one's control?” Perhaps it does not make sense, for example, to say that a person is in control of who she is. For one could argue that this would amount to saying that a person is a self-creator. And in fact the Control Principle, taken to its logical extreme, seems to lead to just such a requirement (see, e.g., Browne 1992, Nagel 1986, 118). If it turns out that self-creation is conceptually impossible as many argue (e.g., Galen Strawson 1986), then perhaps there is a sense in which it is right to say that being in control of one's constitution makes no sense. But it does not follow from this that it is meaningless to deny that one can control one's constitution.
Perhaps the best way of deploying the insight that there is something special about luck and constitution is not to say that it is meaningless to discuss it, but to say that constitutive moral luck is simply unproblematic for morality in the way that resultant moral luck is. This would be to take up the “line-drawing” challenge as described in the last section. On this line of reasoning, for purposes of moral assessment, it does not matter how you came to be; what matters is what you do with what you are. Of course, as we saw, this requires defense and explanation, but it is a way of capturing the insight that constitutive luck is relevantly different from the resultant luck that has captivated a number of commentators.
5. Conclusion
The problem of moral luck is deeply unsettling. Naturally, there is a wide variety of responses to it. On the one extreme are those who deny that there is any sort of moral luck, and on the other are those who accept every sort of moral luck. Most writers who have responded to the problem fall somewhere in between; either they explicitly take a mixed approach or they confine their arguments to a carefully delineated subset of types of moral luck while remaining uncommitted with respect to the others. The extreme positions are vulnerable to the objection that they have left some consideration or other completely unaccounted for. But those who occupy the middle also face a formidable challenge: where can one draw a principled line between acceptable and unacceptable forms of luck? As we have seen, one apparently natural place to draw a line is between resultant luck and all of the other sorts. On this view, there is no resultant moral luck, despite initial appearances, although there is moral luck of all the other kinds. Thus, occupiers of this position face the challenge of setting out a plausible rationale for drawing the line where they do. But they also face the challenge of where precisely to draw another line, namely, the line around what counts as “results.” For we can ask on which side of this line do intentions, willings, bodily movements, and so on, fall. Do results include everything that happens after the formation of an intention or the exertion of the will, for example? Or everything that follows the beginning of the formation of an intention or the beginning of the exertion of the will? Or everything that follows the “affection of the heart” of which Adam Smith wrote so eloquently? These are difficult questions for those who would draw a line at resultant luck. But difficult questions await every other proposal, too. Fortunately, there is a rich and growing literature providing a full spectrum of responses to explore.
I am very grateful to David Brink, Nina Davis, Derk Pereboom, and Sam Rickless for their very helpful input and constructive suggestions. I also benefited greatly from participation in the University of San Diego Institute for Law and Philosophy Roundtable on Moral Luck in April 2003.Constructive mathematics is distinguished from its traditional counterpart, classical mathematics, by the strict interpretation of the phrase “there exists” as “we can construct”. In order to work constructively, we need to re-interpret not only the existential quantifier but all the logical connectives and quantifiers as instructions on how to construct a proof of the statement involving these logical expressions.
In this article we introduce modern constructive mathematics based on the BHK-interpretation of the logical connectives and quantifiers. We discuss four major varieties of constructive mathematics, with particular emphasis on the two varieties associated with Errett Bishop and Per Martin-Löf, which can be regarded as minimal constructive systems. We then outline progress in (informal) constructive reverse mathematics, a research programme seeking to identify principles, such as Brouwer's fan theorem, that, added to the minimal constructive varieties, facilitate proofs of important analytic theorems. Finally, we describe two relatively recent constructive approaches to topology: the theory of apartness spaces, and formal topology.
1. Introduction
2. The Constructive Interpretation of Logic
6. Concluding Remarks
Academic Tools
Other Internet Resources
Related Entries
1. Introduction
Before mathematicians assert something (other than an axiom) they are supposed to have proved it true. What, then, do mathematicians mean when they assert a disjunction P ∨ Q, where P and Q are syntactically correct statements in some (formal or informal) mathematical language? A natural — although, as we shall see, not the unique — interpretation of this disjunction is that not only does (at least) one of the statements P, Q hold, but also we can decide which one holds. Thus just as mathematicians will assert P only when they have decided that P holds by proving it, they may assert P ∨ Q only when they either can produce a proof of P or else produce one of Q.
With this interpretation, however, we run into a serious problem in the special case where Q is the negation, ¬P, of P. To assert ¬P is to show that P implies a contradiction (such as 0 = 1). But it will often be that mathematicians have neither a proof of P nor one of ¬P. To see this, we need only reflect on the following Goldbach conjecture (GC):
Every even integer > 2 can be written as a sum of two primes,
which remains neither proved nor disproved despite the best efforts of many of the leading mathematicians since it was first raised in a letter from Goldbach to Euler in 1742. We are forced to conclude that, under the very natural decidability interpretation of P ∨ Q, only a stubborn optimist can retain a belief in the law of excluded middle (LEM):
For every statement P, either P or ¬P holds.
Classical logic gets round this by widening the interpretation of disjunction: it interprets P ∨ Q as ¬(¬P∧¬Q), or in other words, “it is contradictory that both P and Q be false”. In turn, this leads to the idealistic interpretation of existence, in which ∃xP(x) means ¬∀x¬P(x) (“it is contradictory that P(x) be false for every x”). It is on these interpretations of disjunction and existence that mathematicians have built the grand, and apparently impregnable, edifice of classical mathematics which serves a foundation for the physical, the social, and (increasingly) the biological sciences. However, the wider interpretations come at a cost: for example, when we pass from our initial, natural interpretation of P ∨ Q to the unrestricted use of the idealistic one, ¬(¬P∧¬Q), the resulting mathematics cannot generally be interpreted within computational models such as recursive function theory.
This point is illustrated by a well-worn example, the proposition:
There exist irrational numbers a, b such that ab is rational.
A slick classical proof goes as follows. Either √2√2 is rational, in which case we take a = b = √2; or else √2√2 is irrational, in which case we take a = √2√2 and b = √2 (see Dummett 1977 [2000], 6). But as it stands, this proof does not enable us to pinpoint which of the two choices of the pair (a, b) has the required property. In order to determine the correct choice of (a, b), we would need to decide whether √2√2 is rational or irrational, which is precisely to employ our initial interpretation of disjunction with P the statement “√2√2 is rational”.
Here is another illustration of the difference between interpretations. Consider the following simple statement about the set R of real numbers:
(*) ∀x ∈ R (x = 0 ∨ x ≠ 0),
where, for reasons that we divulge shortly, x ≠ 0 means that we can find a rational number r with 0 < r < |x|. A natural computational interpretation of (*) is that we have a procedure which, applied to any real number x, either tells us that x = 0 or else tells us that x ≠ 0. (For example, such a procedure might output 0 if x = 0, and 1 if x ≠ 0.) However, because the computer can handle real numbers only by means of finite rational approximations, we have the problem of underflow, in which a sufficiently small positive number can be misread as 0 by the computer; so there cannot be a decision procedure that justifies the statement (*). In other words, we cannot expect (*) to hold under our natural computational interpretation of the quantifier ∀ and the connective ∨.
Let's examine this from another angle. Let G(n) act as shorthand for the statement “2n + 2 is a sum of two primes”, where n ranges over the positive integers, and define an infinite binary sequence a = (a1, a2, …) as follows:
an =
{
0 if G(n) holds for all k ≤ n
1 if ¬G(n) holds for some k ≤ n.
There is no question that a is a computationally well-defined sequence, in the sense that we have an algorithm for computing an for each n: check the even numbers 4, 6, 8, …, 2n+2 to determine whether each of them is a sum of two primes; in that case, set an = 0, and in the contrary case, set an = 1. Now consider the real number whose nth binary digit is an :
If (*) holds under our computational interpretation, then we can decide between the following two alternatives:
2−1a1 + 2−2a2 + ··· = 0, which implies that an = 0 for every n;
we can find a positive integer N such that 2−1a1 + 2−2a2 + ··· > 2−N.
In the latter case, by testing a1, …, aN, we can find n ≤ N such that an = 1. Thus the computational interpretation of (*) enables us to decide whether there exists n such that an = 1; in other words, it enables us to decide the status of the Goldbach Conjecture. An example of this type, showing that a constructive proof of some classical result P would enable us to solve the Goldbach conjecture (and, by similar arguments, many other hitherto open problems, such as the Riemann hypothesis), is called a Brouwerian example for, or even a Brouwerian counterexample to, the statement P (though it is not a counterexample in the normal sense of that word).
The use of the Goldbach Conjecture here is purely dramatic. To avoid it, we define a function ƒ classically on the set of binary sequences as follows:
ƒ(a) =
{
0 if an = 0 for all n
1 if an = 1 for some n.
The argument of the preceding paragraph can then be modified to show that, under our computational interpretation, (*) provides us with a procedure for calculating ƒ(a) for any computationally well-defined binary sequence a. Now, the computability of the function ƒ can be expressed informally by the limited principle of omniscience (LPO):
For each binary sequence (a1, a2, …) either an = 0 for all n or else there exists n such that an = 1,
which is generally regarded as an essentially nonconstructive principle for several reasons. First, its recursive interpretation,
There is a recursive algorithm which, applied to any recursively defined binary sequence (a1, a2, …), outputs 0 if an = 0 for all n, and outputs 1 if an = 1 for some n,
is provably false within recursive function theory, even with classical logic (see Bridges and Richman 1987, Chapter 3); so if we want to allow a recursive interpretation of all our mathematics, then we cannot use LPO. Secondly, there is a model theory (Kripke models) in which it can be shown that LPO is not constructively derivable (Bridges and Richman 1987, Chapter 7).
Why, incidentally, do we have the word “classically” in the second sentence of the preceding paragraph? It is because, from a constructive standpoint, ƒ(a) is defined only for those binary sequences a for which we can decide either that an = 0 for all n or else that there exists (we can compute) a positive integer n with an = 1; in other words, f is a constructively well-defined function on the set of all binary sequences if and only if LPO is constructively derivable!
2. The Constructive Interpretation of Logic
It should, by now, be clear that a full-blooded computational development of mathematics disallows the idealistic interpretations of disjunction and existence upon which most classical mathematics depends. In order to work constructively, we need to return from the classical interpretations to the natural constructive ones:
∨ (or):
to prove P ∨ Q we must either have a proof of P or have a proof of Q.
∧ (and):
to prove P ∧ Q we must have both a proof of P and a proof of Q.
⇒ (implies):
a proof of P → Q is an algorithm that converts any proof of P into a proof of Q.
¬ (not):
to prove ¬P we must show that P implies 0 = 1.
∃ (there exists):
to prove ∃xP(x) we must construct an object x and prove that P(x) holds.
∀ (for each/all):
a proof of ∀x∈S P(x) is an algorithm that, applied to any object x and to the data proving that x∈S, proves that P(x) holds.
These BHK-interpretations (the name reflects their origin in the work of Brouwer, Heyting, and Kolmogorov) can be made more precise using Kleene's notion of realizability; see (Dummett 1977 [2000], 222–234; Beeson 1985, Chapter VII).
What sort of things are we looking for if we are serious about developing mathematics in such a way that when a theorem asserts the existence of an object x with a property P, then the proof of the theorem embodies algorithms for constructing x and for demonstrating, by whatever calculations are necessary, that x has the property P. Here are some examples of theorems, each followed by an informal description of the requirements for its constructive proof.
For each real number x, either x = 0 or x ≠ 0.
Proof requirement: An algorithm which, applied to a given real number x, decides whether x = 0 or x ≠ 0. Note that, in order to make this decision, the algorithm might use not only the data describing x but also the data showing that x is actually a real number.
Each nonempty subset S of R that is bounded above has a least upper bound.
Proof requirement: An algorithm which, applied to a set S of real numbers, a member s of S, and an upper bound for S,
computes an object b and shows that b is a real number;
shows that x ≤ b for each x ∈ S; and
given a real number b′ < b, computes an element x of S such that x > b′.
If ƒ is a continuous real-valued mapping on the closed interval [0, 1] such that ƒ(0)·ƒ(1) < 0, then there exists x such that 0 < x < 1 and ƒ(x) = 0.
Proof requirement: An algorithm which, applied to the function ƒ, a modulus of continuity for ƒ, and the values ƒ(0) and ƒ(1),
computes an object x and shows that x is a real number between 0 and 1; and
shows that ƒ(x) = 0.
If ƒ is a continuous real-valued mapping on the closed interval [0, 1] such that ƒ(0)·ƒ(1) < 0, then for each ε > 0 there exists x such that 0 < x < 1 and |ƒ(x)| < ε.
Proof requirement: An algorithm which, applied to the function ƒ, a modulus of continuity for ƒ, the values ƒ(0) and ƒ(1), and a positive number ε,
computes an object x and shows that x is a real number between 0 and 1; and
shows that |ƒ(x)| < ε.
We already have reasons for doubting that (A) has a constructive proof. If the proof requirements for (B) can be fulfilled, then, given any mathematical statement P, we can apply our proof of (B) to compute a rational approximation z to the supremum σ of the set
S = {0} ∪ {x ∈ R: P ∧ x = 1}
with error < ¼. We can then determine whether z > ¼, in which case σ > 0, or z < ¾, when σ < 1. In the first case, there exists x ∈ S with x > 0, so we must have x = 1 and therefore P. In the case σ < 1, we have ¬P. Thus (B) implies the law of excluded middle.
However, in Bishop's constructive theory of the real numbers, based on Cauchy sequences with a preassigned convergence rate, we can prove the following constructive least-upper-bound principle:
Let S be a nonempty subset of R that is bounded above. Then S has a least upper bound if and only if it is upper-order located, in the sense that for all real numbers α, β with α < β, either β is an upper bound for S or else there exists x ∈ S with x > α (Bishop and Bridges 1985, p. 37, Proposition (4.3)).
In passing, we mention an alternative development of the constructive theory of R based on interval arithmetic; see Chapter 2 of Bridges and Vîță 2006.
Each of statements (C) and (D), which are classically equivalent, is a version of the Intermediate Value Theorem. In these statements, a modulus of continuity for ƒ is a set Ω of ordered pairs (ε, δ) of positive real numbers with the following two properties:
for each ε > 0 there exists δ > 0 such that (ε, δ) ∈Ω
for each (ε, δ) ∈ Ω, and for all x, y ∈ [0, 1] with |x − y| < δ, we have |ƒ(x) − ƒ(y)| < ε.
Statement (C) is essentially nonconstructive, since it entails the essentially nonconstructive lesser limited principle of omniscience (LLPO):
For each binary sequence (a1,a2,…) with at most one term equal to 1, either an = 0 for all even n or else an = 0 for all odd n.
Statement (D), a weak form of (C), can be proved constructively, using an interval-halving argument of a standard type. The following stronger constructive intermediate value theorem, which suffices for most practical purposes, is proved using an approximate interval-halving argument:
Let ƒ be a continuous real-valued mapping on the closed interval [0, 1] such that ƒ(0) and ƒ(1) have opposite signs. Suppose also that ƒ is locally nonzero, in the sense that for each x ∈ [0, 1] and each r > 0, there exists y such that |x − y| < r and ƒ(y) ≠ 0. Then there exists x such that 0 < x < 1 and ƒ(x) = 0.
The situation of the intermediate value theorem is typical of many in constructive analysis, where we find one classical theorem with several constructive versions, some or all of which may be equivalent under classical logic. (See also, for example, Bridges et al. 1982.)
There is one omniscience principle whose constructive status is less clear than that of LPO and LLPO—namely, Markov's principle (MP):
For each binary sequence (an), if it is contradictory that all the terms an equal 0, then there exists a term equal to 1.
This principle is equivalent to a number of simple classical propositions, including the following:
For each real number x, if it is contradictory that x equal 0, then x ≠ 0 (in the sense we mentioned earlier).
For each real number x, if it is contradictory that x equal 0, then there exists y ∈ R such that xy = 1.
For each one-one continuous mapping ƒ : [0, 1] → R, if x ≠ y, then ƒ(x) ≠ ƒ(y).
Markov's principle represents an unbounded search: if you have a proof that all terms an being 0 leads to a contradiction, then, by testing the terms a1,a2,a3,… in turn, you are “guaranteed” to come across a term equal to 1; but this guarantee does not extend to an assurance that you will find the desired term before the end of the universe. Most practitioners of constructive mathematics view Markov's principle with at least suspicion, if not downright disbelief. Such views are reinforced by the observation that there is a Kripke Model showing that MP is not constructively derivable (Bridges and Richman 1987, 137–138.)
3. Varieties of Constructive Mathematics
The desire to retain the possibility of a computational interpretation is one motivation for using the constructive reinterpretations of the logical connectives and quantifiers that we gave above; but it is not exactly the motivation of the pioneers of constructivism in mathematics. In this section we look at some of the different approaches to constructivism in mathematics over the past 130 years.
3.1 Intuitionistic Mathematics
In the late nineteenth century, certain individuals — most notably Kronecker and Poincaré — had expressed doubts, or even disapproval, of the idealistic, nonconstructive methods used by some of their contemporaries; but it is in the polemical writings of L.E.J. Brouwer (1881–1966), beginning with his Amsterdam doctoral thesis (1907) and continuing over the next forty-seven years, that the foundations of a precise, systematic approach to constructive mathematics were laid. In Brouwer's philosophy, known as intuitionism, mathematics is a free creation of the human mind, and an object exists if and only if it can be (mentally) constructed. If one takes that philosophical stance, then one is inexorably drawn to the foregoing constructive interpretation of the logical connectives and quantifiers: for how could a proof of the impossibility of the non-existence of a certain object x describe a mental construction of x?
Brouwer was not the clearest expositor of his ideas, as is shown by the following quotation:
Mathematics arises when the subject of two-ness, which results from the passage of time, is abstracted from all special occurrences. The remaining empty form [the relation of n to n+1] of the common content of all these two-nesses becomes the original intuition of mathematics and repeated unlimitedly creates new mathematical subjects. (quoted in Kline 1972, 1199–2000)
A modern version of Brouwer's view was given by Errett Bishop (Bishop 1967, p. 2):
The primary concern of mathematics is number, and this means the positive integers. We feel about number the way Kant felt about space. The positive integers and their arithmetic are presupposed by the very nature of our intelligence and, we are tempted to believe, by the very nature of intelligence in general. The development of the positive integers from the primitive concept of the unit, the concept of adjoining a unit, and the process of mathematical induction carries complete conviction. In the words of Kronecker, the positive integers were created by God.
However obscure Brouwer's writings could be, one thing was always clear: for him, mathematics took precedence over logic. One might say, as Hermann Weyl does in the following passage, that Brouwer saw classical mathematics as flawed precisely in its use of classical logic without reference to the underlying mathematics:
According to [Brouwer's] view and reading of history, classical logic was abstracted from the mathematics of finite sets and their subsets. … Forgetful of this limited origin, one afterwards mistook that logic for something above and prior to all mathematics, and finally applied it, without justification, to the mathematics of infinite sets. This is the Fall and original sin of set theory, for which it is justly punished by the antinomies. It is not that such contradictions showed up that is surprising, but that they showed up at such a late stage of the game. (Weyl 1946)
In particular, this misuse of logic led to nonconstructive existence proofs which, in Hermann Weyl's words, “inform the world that a treasure exists without disclosing its location”.
In order to describe the logic used by the intuitionist mathematician, it was necessary first to analyse the mathematical processes of the mind, from which analysis the logic could be extracted. In 1930, Brouwer's most famous pupil, Arend Heyting, published a set of formal axioms which so clearly characterise the logic used by the intuitionist that they have become universally known as the axioms for intuitionistic logic (Heyting 1930). These axioms captured the informal BHK-interpretation of the connectives and quantifiers that we gave earlier.
Intuitionistic mathematics diverges from other types of constructive mathematics in its interpretation of the term “sequence”. Normally, a sequence in constructive mathematics is given by a rule which determines, in advance, how to construct each of its terms; such a sequence may be said to be lawlike or predeterminate. Brouwer generalised this notion of a sequence to include the possibility of constructing the terms one-by-one, the choice of each term being made freely, or subject only to certain restrictions stipulated in advance. Most manipulations of sequences do not require that they be predeterminate, and can be performed on these more general free choice sequences.
Thus, for the intuitionist, a real number x = (x1, x2, …) — essentially, a Cauchy sequence of rational numbers — need not be given by a rule: its terms x1, x2, … , are simply rational numbers, successively constructed, subject only to some kind of Cauchy restriction such as the following one used by Bishop (1967):
∀m∀n[|xm − xn| ≤ (1/m + 1/n)]
Once free choice sequences are admitted into one's mathematics, so, perhaps to one's initial surprise, are certain strong choice principles. Let P be a subset of NN × N (where N denotes the set of natural numbers and, for sets A and B, BA denotes the set of mappings from A into B), and suppose that for each a ∈ NN there exists n ∈ N such that (a,n) ∈ P. From a constructive point of view, this means that we have a procedure, applicable to sequences, that computes n for any given a. According to Brouwer, the construction of an element of NN is forever incomplete: a generic sequence a is purely extensional, in the sense that at any given moment we can know nothing about a other than a finite set of its terms. It follows that our procedure must be able to calculate, from some finite initial sequence (a0, …, aN) of terms of a, a natural number n such that P(a,n). If b ∈ NN is any sequence such that bk = ak for 0 ≤ k ≤ N, then our procedure must return the same n for b as it does for a. This means that n is a continuous function of a with respect to the topology on NN given by the metric
ρ : (a, b) inf{2−n : ak = bk for 0 ≤ k ≤ n}.
We are therefore led to the following principle of continuous choice, which we divide into a continuity part and a choice part.
CC1 : Any function from NN to N is continuous.
CC2 : If P ⊆ NN × N, and for each a ∈ NN there exists n ∈ N such that (a, n) ∈ P, then there is a function ƒ : NN → N such that (a, ƒ(a)) ∈ P for all a ∈ NN.
If P and ƒ are as in CC2, then we say that ƒ is a choice function for P.
The omniscience principles LPO and LLPO are demonstrably false under the hypotheses CC1–2; but MP is consistent with it. Among the remarkable consequences of CC1–2 are the following.
Any function from NN or 2N to a metric space is pointwise continuous.
Every mapping from a nonempty complete separable metric space to a metric space is pointwise continuous.
Every map from the real line R to itself is pointwise continuous.
Let X be a complete separable normed space, Y a normed space, and (un) a sequence of linear mappings from X to Y such that for each unit vector x of X,
φ(x) = sup{ ||un(x)|| : n ∈ N }
exists. Then there exists c > 0 such that ||un(x)|| ≤ c for all n∈N and all unit vectors x of X (Uniform boundedness principle).
Each of these statements appears to contradict known classical theorems. However, the comparison with classical mathematics should not be made superficially: in order to understand that there is no real contradiction here, we must appreciate that the meaning of such terms as “function” and even “real number” in intuitionistic mathematics is quite different from that in the classical setting. (In practice, intuitionistic mathematics cannot be compared, readily and directly, with classical mathematics.)
Brouwer's introspection over the nature of functions and the continuum led him to a second principle, which, unlike that of continuous choice, is classically valid. This principle requires a little more background for its explanation.
For any set S we denote by S* the set of all finite sequences of elements of S, including the empty sequence ( ). If α = (a1, …, an) is in S*, then n is called the length of α and is denoted by |α|. If m ∈ N, and α is a finite or infinite sequence in S of length at least m, then we denote by α(m) the finite sequence consisting of the first m terms of α. Note that α(0) = ( ). If α ∈ S* and β = α(m) for some m, we say that α is an extension of β, and that β is a restriction of α.
A subset σ of S is said to be detachable (from S) if
∀x∈S (x ∈ σ ∨ x ∉ σ).
A detachable subset σ of N* is called a fan if
it is closed under restriction: for each α ∈ N* and each n, if α(n) ∈ S, then α(k) ∈ S whenever 0 ≤ k ≤ n; and
for each α ∈ σ, the set
{ α*n ∈ S: n ∈ N }
is finite or empty, where α*n denotes the finite sequence obtained by adjoining the natural number n to the terms of α.
A path in a fan σ is a sequence α, finite or infinite, such that α(n) ∈ σ for each applicable n. We say that a path α is blocked by a subset B if some restriction of α is in B; if no restriction of α is in B, we say that α misses B. A subset B of a fan σ is called a bar for σ if each infinite path of σ is blocked by B; a bar B for σ is uniform if there exists n ∈ N such that each path of length n is blocked by B.
At last we can state Brouwer's next principle of intuitionism, the fan theorem for detachable bars (FTD):
Every detachable bar of a fan is uniform.
In its classical contrapositive form, FTD is known as König's Lemma: if for every n there exists a path of length n that misses B, then there exists an infinite path that misses B (see Dummett 1977 [2000], 49–53). (Of course, classically the condition of detachability is superfluous.) It is simple to construct a Brouwerian counterexample to König's Lemma.
Brouwer actually posited the fan theorem without the restriction of detachability of the bar. Attempts to prove that more general fan theorem constructively rely on an analysis of how we could know that a subset is a bar, and led Brouwer to a notion of bar induction; this is discussed in Section 3.6 of the entry on intuitionism in the philosophy of mathematics; another good reference for bar induction is van Atten (2004). We shall return to fan theorems in Section 4.
Of the many applications of Brouwer's principles, the most famous is his uniform continuity theorem (which follows from the pointwise continuity consequences of CC1-2 together a form of fan theorem more general than FTD):
Every mapping from a compact (that is, complete, totally bounded) metric space into a metric space is uniformly continuous.
The reader is warned once again to interpret this carefully within Brouwer's intuitionistic framework, and not to jump to the erroneous conclusion that intuitionism contradicts classical mathematics. It is more sensible to regard the two types of mathematics as incomparable. For further discussion, see the entry on intuitionistic logic.
Unfortunately — and perhaps inevitably, in the face of opposition from mathematicians of such stature as Hilbert — Brouwer's intuitionist school of mathematics and philosophy became more and more involved in what, at least to classical mathematicians, appeared to be quasi-mystical speculation about the nature of constructive thought, to the detriment of the practice of constructive mathematics itself. This unfortunate polarisation between the Brouwerians and the Hilbertians culminated in the notorious Grundlagenstreit of the 1920s, details of which can be found in the Brouwer biographies by van Dalen (1999, 2005) and van Stigt (1990).
3.2 Recursive Constructive Mathematics
In the late 1940s, the Russian mathematician A.A. Markov began the development of an alternative form of constructive mathematics (RUSS), which is, essentially, recursive function theory with intuitionistic logic (Markov 1954, Kushner 1985). In this variety the objects are defined by means of Gödel-numberings, and the procedures are all recursive; the main distinction between RUSS and the classical recursive analysis developed after, in 1936, the work of Turing, Church, and others clarified the nature of computable processes, is that the logic used in RUSS is intuitionistic.
One obstacle faced by the mathematician attempting to come to grips with RUSS is that, being expressed in the language of recursion theory, it is not easily readable; indeed, on opening a page of Kushner's excellent lectures (1985), one might be forgiven for wondering whether this is analysis or logic. (This remark should be tempered with reference to the two, relatively readable books on classical recursive analysis by Aberth (1980, 2001).) Fortunately, one can get to the heart of RUSS by an axiomatic approach due to Richman (1983) (see also Chapter 3 of Bridges and Richman 1987).
First, we define a set S to be countable if there is a mapping from a detachable subset of N onto S. With intuitionistic logic we cannot prove that every subset of N is detachable (the reader is invited to provide a Brouwerian example to demonstrate this). Countable subsets of N in Richman's axiomatic approach are the counterparts of recursively enumerable sets in the normal development of RUSS.
By a partial function on N we mean a mapping whose domain is a subset of N; if the domain is N itself, then we call the function a total partial function on N. Richman's approach to RUSS is based on intuitionistic logic and a single axiom of computable partial functions (CPF):
There is an enumeration φ0, φ1, … of the set of all partial functions from N to N with countable domains.
It is remarkable what can be deduced cleanly and quickly using this principle. For example, we can prove the following result, which almost immediately shows that LLPO, and hence LPO, are false in the recursive setting.
There is a total partial function ƒ : N × N → {0, 1} such that
for each m there exists at most one n such that ƒ(m, n) = 1; and
for each total partial function ƒ : N → {0, 1}, there exist m,k in N such that ƒ(m, 2k+ƒ(m)) = 1.
Of more interest, however, are results such as the following within RUSS.
Specker's Theorem: There exists a strictly increasing sequence (r1, r2, …) of rational numbers in the closed interval [0, 1] such that for each x ∈ R there exist N ∈ N and δ > 0 such that |x − rn| ≥ δ for all n ≥ N.
For each ε > 0, there exists a sequence (I1, I2, …) of bounded open intervals in R such that
(Such a sequence of intervals is called an ε-singular cover of R.)
There exists a pointwise continuous function ƒ : [0, 1] → R that is not uniformly continuous.
There exists a positive-valued uniformly continuous function ƒ : [0, 1] → R whose infimum is 0.
From a classical viewpoint, these results fit into place when one realises that words such as “function” and “real number” should be interpreted as “recursive function” and “recursive real number” respectively. Note that the second of the above four recursive theorems is a strong recursive counterexample to the open-cover compactness property of the (recursive) real line; and the fourth is a recursive counterexample to the classical theorem that every uniformly continuous mapping of a compact set into R attains its infimum.
3.3 Bishop's Constructive Mathematics
Progress in all varieties of constructive mathematics was relatively slow throughout the next decade and a half. What was needed to raise the profile of constructivism in mathematics was a top-ranking classical mathematician to show that a thoroughgoing constructive development of deep analysis was possible without a commitment to Brouwer's non-classical principles or to the machinery of recursive function theory. This need was fulfilled in 1967, with the appearance of Errett Bishop's monograph Foundations of Constructive Analysis (1967), the product of an astonishing couple of years in which, working in the informal but rigorous style used by normal analysts, Bishop provided a constructive development of a large part of twentieth-century analysis, including the Stone-Weierstrass Theorem, the Hahn-Banach and separation theorems, the spectral theorem for self-adjoint operators on a Hilbert space, the Lebesgue convergence theorems for abstract integrals, Haar measure and the abstract Fourier transform, ergodic theorems, and the elements of Banach algebra theory. (See also Bishop and Bridges 1985.) Thus, at a stroke, he gave the lie to the commonly-held view expressed so forcefully by Hilbert:
Taking the principle of excluded middle from the mathematician would be the same, say, as proscribing the telescope to the astronomer or to the boxer the use of his fists. (Hilbert 1928)
Not only did Bishop's mathematics (BISH) have the advantage of readability — if you open Bishop's book at any page, what you see is clearly recognisable as analysis, even if, from time to time, his moves in the course of a proof may appear strange to one schooled in the use of the law of excluded middle — but, unlike intuitionistic or recursive mathematics, it admits many different interpretations. Intuitionistic mathematics, recursive constructive mathematics, and even classical mathematics all provide models of BISH. In fact, the results and proofs in BISH can be interpreted, with at most minor amendments, in any reasonable model of computable mathematics, such as, for example, Weihrauch's Type Two Effectivity Theory (Weihrauch 2000; Bauer 2005).
How is this multiple interpretability achieved? At least in part by Bishop's refusal to pin down his primitive notion of “algorithm” or, in his words, “finite routine”. This refusal has led to the criticism that his approach lacks the precision that a logician would normally expect of a foundational system. However, this criticism can be overcome by looking more closely at what practitioners of BISH actually do, as distinct from what Bishop may have thought he was doing, when they prove theorems: in practice, they are doing mathematics with intuitionistic logic. Experience shows that the restriction to intuitionistic logic always forces mathematicians to work in a manner that, at least informally, can be described as algorithmic; so algorithmic mathematics appears to be equivalent to mathematics that uses only intuitionistic logic. If that is the case, then we can practice constructive mathematics using intuitionistic logic on any reasonably defined mathematical objects, not just some class of “constructive objects”.
This view, more or less, appears to have first been put forward by Richman (1990, 1996). Taking the logic as the primary characteristic of constructive mathematics, it does not reflect the primacy of mathematics over logic that was part of the belief of Brouwer, Heyting, Markov, Bishop, and other pioneers of constructivism. On the other hand, it does capture the essence of constructive mathematics in practice.
Thus one might distinguish between the ontological constructivism of Brouwer and others who are led to constructive mathematics through a belief that mathematical objects are mental creations, and the epistemological constructivism of Richman and those who see constructive mathematics as characterised by its methodology, based on the use of intuitionistic logic. Of course, the former approach to constructivism inevitably leads to the latter; and the latter is certainly not inconsistent with a Brouwerian ontology.
This view, more or less, appears to have first been put forward by Richman (1990, 1996). Taking the logic as the primary characteristic of constructive mathematics, it does not reflect the primacy of mathematics over logic that was part of the belief of Brouwer, Heyting, Markov, Bishop, and other pioneers of constructivism. On the other hand, it does capture the essence of constructive mathematics in practice.
Of course, to do actual mathematics we need more than just intutionistic logic. For Bishop, the building blocks of mathematics were the positive integers (see the quote from Bishop 1967 in Section 3.1 above). Myhill (1975) gave an axiomatic foundation for BISH based on primitive notions of number, set, and function. Friedman (1977) dealt with intuitionistic ZF set theory, and Bridges (1987) worked with a highly formalised constructive Morse set theory. The two main formal underpinnings of BISH a this stage are the CZF set theory of Aczel and Rathjen (2000 and forthcoming), and the type theory of Martin-Löf (1975, 1984). For an overview, see Crosilla 2009.
3.4 Martin-Löf's Constructive Type Theory
Before ending our tour of varieties of modern constructive mathematics, we visit a fourth variety, based on Per Martin-Löf's type-theory (ML). In 1968, Martin-Löf published his Notes on Constructive Mathematics, based on lectures he had given in Europe in 1966–68; so his involvement with constructivism in mathematics goes back at least to the period of Bishop's writing of Foundations of Constructive Analysis. Martin-Löf's book is in the spirit of RUSS, rather than BISH; indeed, its author did not have access to Bishop's book until his own manuscript was finished. Martin-Löf later turned his attention to his theory of types as a foundation for Bishop-style constructive mathematics.
Here, in his own words, is an informal explanation of the ideas underlying ML:
We shall think of mathematical objects or constructions. Every mathematical object is of a certain kind or type [… and] is always given together with its type. … A type is defined by describing what we have to do in order to construct an object of that type. … Put differently, a type is well-defined if we understand … what it means to be an object of that type. Thus, for instance N → N [functions from N to N] is a type, not because we know particular number theoretic functions like the primitive recursive ones, but because we think we understand the notion of number theoretic function in general. (Martin-Löf 1975)
In particular, in this system every proposition can be represented as a type: namely, the type of proofs of the proposition. Conversely, each type determines a proposition: namely, the proposition that the type in question is inhabited. So when we think of a certain type T as a proposition, we interpret the formula
x ∈ T
as “x is a proof of the proposition T”.
Martin-Löf goes on to construct new types, such as Cartesian products and disjoint unions, from old. For example, the Cartesian product
(Πx ∈ A) B(x)
is the type of functions that take an arbitrary object x of type A into an object of type B(x). In the propositions-as-proofs interpretation, where B(x) represents a proposition, the above Cartesian product corresponds to the universal proposition
(∀x ∈ A) B(x).
Martin-Löf distinguishes carefully between proofs and derivations: a proof object is a witness to the fact that some proposition has been proved; whereas a derivation is the record of the construction of a proof object. Also, he exercises two basic forms (one dare not say “types” here!) of judgement. The first is a relation between proof objects and propositions, the second a property of some propositions. In the first case, the judgement is either one that a proof object a is a witness to a proposition A, or else one that two proof objects a and b are equal and both witness that A has been proved. The first case of the second form of judgement states that a proposition A is well-formed, and the second records that two propositions A and B are equal.
There is a careful, and highly detailed, set of rules for formalising ML. We will not go into those here, but refer the reader to other sources such as Bridges & Reeves 1999. However, there is one further technical matter we would like to mention: the axiom of choice is derivable in ML.
The full axiom of choice can be stated as follows:
If A, B are inhabited sets, and S a subset of A × B such that
∀x∈A∃y∈B ((x, y) ∈ S),
(1)
then there exists a choice function ƒ : A → B such that
∀x∈A ((x, ƒ(x)) ∈ S).
Now, if this is to hold under a constructive interpretation, then for a given x ∈ A, the value ƒ(x) of the choice function will depend not only on x but also on the data proving that x belongs to A. In general, we cannot expect to produce a choice function of this sort. On the other hand, the BHK interpretation of (1) is that there is an algorithm A which, applied to any given x ∈ A, produces an element y ∈ B such that (x, y) ∈ S. If A is a completely presented (or, in Bishop's words, basic) set, one for which no work beyond the construction of each element in the set is required to prove that the element does indeed belong to A, then we might reasonably expect the algorithm A to be a choice function. In Bishop-style mathematics, basic sets are rare (N is one). In Martin-Löf's type theory, every set is completely presented and, in keeping with what we wrote above about the BHK interpretation of (1), the axiom of choice is derivable therein. In this connection, we should point the reader to the Diaconescu (1975), and Goodman and Myhill (1978), proofs that the axiom of choice entails the law of excluded middle (see also Problem 2 on page 58 of Bishop 1967). Clearly, the Diaconescu-Goodman-Myhill theorem applies under the assumption that not every set is completely presented. For an analysis of the axiom of choice in set theory and type theory see Martin-Löf 2006.
When actually doing constructive mathematics in type theory, one often needs to equip completely presented sets (types) with an equivalence relation, the combination being known as a setoid. Mappings are then functions that respect those equivalence relations.This is in close agreement with the way Bishop presented his informal theory of sets. The dependent types of Martin-Löf are useful for constructing subsets. For instance, the real numbers can be constructed using the Σ-type (see Martin-Löf 1984):
(Σx ∈ N+→Q}(Πm ∈ N+)(Πn ∈ N+)[|xm − xn| ≤ (1/m + 1/n)],
(1)
An element of this type B is thereby a pair consisting of a convergent sequence x of rationals and a proof p that it is convergent. A suitable equivalence relation ~ on R is defined by taking (x,p) ~ (y,q) to mean
∀m ∈ N+ ( |xm − ym| ≤ 2/m).
The resulting setoid of real numbers is R = (R,~). We can readily prove that
∀x ∈ R ∃n ∈ Z (n < z < n+2)
and then, using the type-theoretic axiom of choice, find a function f : R→Z such that f(x) < x < f(x)+2. However, there is no reason to believe that the function f respects the equivalence relations—that is, that f(x) = f(y) holds if x ~ y.
Every constructive proof embodies an algorithm that, in principle, can be extracted and recast as a computer program; moreover, the constructive proof is itself a verification that the algorithm is correct — that is, meets its specification. One major advantage of Martin-Löf's formal approach to constructive mathematics is that it greatly facilitates the extraction of programs from proofs. This has led to serious work on the implementation of constructive mathematics in various locations (see Martin-Löf 1980, Constable 1986, and Hayashi and Nakano 1988). Some recent implementations of type theory for proof extraction are Coq and Agda.
4. Constructive Reverse Mathematics
In the 1970s, Harvey Friedman initiated a research programme of reverse mathematics, aiming to classify mathematical theorems according to their equivalence to one of a small number of set-theoretic principles (Friedman 1975). This classification reveals interesting, sometimes remarkable, differences in proof-theoretic complexity. For example, although the Ascoli-Arzelà theorem is used in the standard proof of Peano's existence theorem for solutions of ordinary differential equations (Hurewicz 1958, page 10), a reverse-mathematical analysis shows that the former is equivalent to a strictly stronger set-theoretic principle than the one equivalent to Peano's theorem (Simpson 1984, Theorems 3.9 and 4.2). The standard treatise on classical reverse mathematics is (Simpson 1999, 2009).
Around the turn of this century, Veldman (Veldman 2005), in the Netherlands, and Ishihara (Ishihara 2005, 2006), in Japan, independently initiated a programme of constructive reverse mathematics (CRM), based on intuitionistic, rather than classical, logic. (Note, though, that the first published work in the modern era of CRM is probably that of Julian and Richman (1984), which was twenty years ahead of its time.) In this section of the article, we describe a less formal approach to CRM, in the style and framework of BISH. The aim of that CRM program is to classify the theorems in the three standard models—CLASS, INT, and RUSS—according to which principles we must, and need only, add to BISH in order to prove them.
We stress that we restrict ourselves here to informal CRM, in which we take for granted the principles of function- and set-construction described in the first chapters of (Bishop 1967, Bishop and Bridges 1985, Bridges and Richman 1987, Bridges and Vîță 2006), and we work in the informal, though rigorous, style of the practising analyst, algebraist, topologist, … .
In practice, CRM splits naturally into two parts. In the first of these, we consider a theorem T of INT or RUSS, and try to find some principle, valid in that model and other than T itself, whose addition to BISH is necessary and sufficient for a constructive proof of T. In the second part of CRM we deal with a theorem T of CLASS that we suspect is nonconstructive, and we try to prove that T is equivalent, over BISH, to one of a number of known essentially-nonconstructive principles, such as MP, LLPO, LPO, or LEM. For an example of this part of CRM, we mention our earlier proof that the classical least-upper-bound principle implies, and hence is equivalent to, LEM.
Incidentally, there is a strong argument for Brouwer being the first to deal with reverse-mathematical ideas: his Brouwerian counterexamples (see the one using the Goldbach conjecture, in Section 1 above), dating back to (Brouwer 1921), lie squarely in the second part of CRM. Even if Brouwer did not state those examples as logical equivalences, but as implications of the type
P ⇒ some nonconstructive principle,
it is hard to believe that he was unaware that the right-hand-side implied the left in such cases.
To illustrate the first part of CRM, we now concentrate on theorems of the type
BISH ⊢ FT? ⇔ T,
where FT? is some form of Brouwer's fan theorem, and T is a theorem of INT. To do so, we need to distinguish between certain types of bar for the complete binary fan 2* (the set of all finite sequences in {0, 1}).
Let α ≡ (α1, α2, …) be a finite or infinite binary sequence. The concatenation of α with another string β is
α¯β ≡ (α1, α2, …, αn, β1, …, βm).
For b in {0, 1} we write α¯b rather than α¯(b). By a c-subset of 2* we mean a subset B of 2* such that
B = {u ∈ 2* : ∀v ∈ 2*(u¯v ∈ D}
(2)
for some detachable subset D of 2*. Every detachable subset of 2* is a c-subset. On the other hand, by a Π0-subset of 2* we mean a subset B of 2* with the following property: there exists a detachable subset S of 2* × N such that
∀u∈2*∀n∈N ((u, n) ∈ S ⇒ (u¯0, n) ∈ S ∧ (u¯1, n) ∈ S)
and
B = {u ∈ 2* : ∀n∈N((u, n) ∈ S)}.
Every c-subset B of 2* is a Π0-subset: simply take S = D × N, where D is a detachable subset of 2* such that (1) holds.
If ? denotes a property of subsets of 2*, then Brouwer's fan theorem for ?-bars tells us that every bar with the property ? is a uniform bar. We are particularly interested in the fan theorem for detachable bars (already discussed in Section 3.1):
FTD: Every detachable bar of the complete binary fan is uniform;
the fan theorem for c-bars (that is, bars that are c-subsets):
FTc: Every c-bar of the complete binary fan is uniform;
the fan theorem for Π0-bars (that is, bars that are Π0-subsets):
FTΠ0: Every Π0-bar of the complete binary fan is uniform;
and the full fan theorem (already discussed in Section 3.1):
FT: Every bar of the complete binary fan is uniform.
Note that, relative to BISH,
It is not known whether any of these implications can be reversed; but Diener (2008) has shown that the negations of these four principles are equivalent over BISH.
Typically, we want to prove that FT? is equivalent, over BISH, to the proposition that, for every set S of an appropriate sort, some pointwise property of the form
∀x ∈S ∃t ∈ T P(s,t)
actually holds uniformly in the form
∃t ∈ T ∀s ∈ S P(s,t).
Our strategy for attacking this problem is two-fold. First, given a set S of the appropriate sort, we construct a ?-subset N of 2* such that
if (2) holds, then B is a bar, and
if B is a uniform bar, then (3) holds.
This, though, is only half of the solution. To prove that the implication from (3) to (2) implies FT?, we consider a ?-subset B of 2* and construct a corresponding set S such that
if B is a bar, then (2) holds, and
if (3) holds, then B is a uniform bar.
The canonical example of such results is that of Julian and Richman (1984), in which S is the set of values of a given uniformly continuous mapping f : [0, 1] → R, T is the set of positive real numbers, and
P(s,t) ≡ (s ≥ t)
The pointwise property we consider is
∀x ∈ [0, 1] ∃t > 0 (f(x) ≥ t),
its uniform version being
∃t >0 ∀x ∈ [0, 1] (f(x) ≥ t).
The Julian-Richman results are as follows.
Theorem 1:
Let f : [0, 1] → R be uniformly continuous. Then there exists a detachable subset B of 2* such that
if f(x) > 0 for each x ∈ [0, 1], then B is a bar, and
if B is a uniform bar, then inf f > 0.
Theorem 2: Let B be a detachable subset of 2*. Then there exists a uniformly continuous f : [0, 1] → R such that
if B is a bar, then f(x) > 0 for each x ∈ [0, 1], and
if inf f > 0, then B is a uniform bar.
The proofs of these two theorems are subtle and tricky; see Julian and Richman 1984, or Bridges and Richman 1987 (Chapter 6), for details. A new, but equally complex, proof can be found in Berger and Bridges 2009.
The two Julian-Richman theorems together reveal that, relative to BISH, the fan theorem FTD is equivalent to the positivity principle, POS:
Each positive-valued, uniformly continuous function on [0, 1] has a positive infimum.
It follows that POS is derivable in INT, in which the full fan theorem, not just FTD, is a standard principle. The situation is quiet the opposite in RUSS, where there exist a detachable bar of 2* that is not uniform and a positive-valued, uniformly continuous function on [0, 1] that has infimum equal to 0; see Chapters 5 and 6 of Bridges and Richman 1987.
Berger and Ishihara (2005) have taken a different, indirect route to establishing the equivalence of POS and FTc. They establish a chain of equivalences between POS, FTc, and four principles of the type “if there is at most one object with property P, then there is one such object”. The four unique-existence principles are:
CIN!: Each descending sequence of inhabited closed located subsets of a compact metric space with at most one common point has inhabited intersection (Cantor's intersection theorem with uniqueness).
MIN!: Each uniformly continuous, real-valued function on a compact metric space with at most one minimum point has a minimum point.
WKL! Each infinite tree with at most one infinite branch has an infinite branch (the weak König lemma with uniqueness).
FIX!: Each uniformly continuous function from a compact metric space into itself with at most one fixed point and with approximate fixed points has a fixed point.
In, for example, the last of these, we say that a map f of a metric space (X,ρ) into itself
has at most one fixed point if ρ(f(x),x) + ρ(f(y),y) > 0 whenever ρ(x,y) > 0;
has approximate fixed points if for each ε > 0 there exists x ∈ X such that ρ(f(x),x) < ε.
A major open problem in CRM is that of finding a form of the fan theorem that is equivalent, over BISH, to the uniform continuity theorem for [0, 1] (UCT[0,1]):
Every pointwise continuous mapping of [0, 1] into R is uniformly continuous.
the proposition for which Brouwer originally developed his proof of the fan theorem. (Note that UCT[0,1] is equivalent, relative to BISH, to the general uniform continuity theorem for metric spaces: Every pointwise continuous mapping of a complete, totally bounded metric space into a metric space is uniformly continuous. See Loeb 2005, and Bridges and Diener 2007.)
It follows from results of Berger (2006) that
BISH + UCT[0,1] ⊢ FTc.
Also, Diener and Loeb (2008) have proved that
However, we do not know if either of these implications can be replaced by a bi-implication. Perhaps UCT[0,1], and hence the full uniform continuity theorem for compact metric spaces, is equivalent, relative to BISH, to some natural, but as yet unidentified, version of the fan theorem.
For additional material on the fan theorem in constructive reverse mathematics, see, for example, Berger and Bridges 2006, 2007; Bridges 2008; Diener 2008, 2012; Diener and Loeb 2009; and Diener and Lubarsky 2013.
Interested readers may pursue the topic of constructive reverse mathematics in greater detail in the following supplementary document:
Ishihara's principle BD-N and the anti-Specker Property
5. Constructive Topology
Constructive mathematicians have tended to concentrate their efforts on the field of analysis, with considerable success—witness the wealth of functional analysis developed in Bishop 1967. This does not mean that, for example, algebra has been sidelined from the constructive enterprise: the material in the monograph by Mines et al (1986) can be regarded as a substantial algebraic counterpart to the constructive analysis carried out by Bishop. Much more recently, Lombardi and Quitté (2011) have published the first large volume of a proposed two-volume work on constructive algebra. However, not being expert in algebra, and being aware of the danger of making this article too long to hold the reader's attention, we choose not to discuss constructive analysis or algebra in any detail; rather, in the following supplementary document, we turn to constructive topology, describing two rather different approaches to that subject:
6. Concluding Remarks
The traditional route taken by mathematicians wanting to analyse the constructive content of mathematics is the one that follows classical logic; in order to avoid decisions, such as whether or not a real number equals 0, that cannot be made by a real computer, the mathematician then has to keep within strict algorithmic boundaries such as those formed by recursive function theory. In contrast, the route taken by the constructive mathematician follows intuitionistic logic, which automatically takes care of computationally inadmissible decisions. This logic (together with an appropriate set- or type-theoretic framework) suffices to keep the mathematics within constructive boundaries. Thus the mathematician is free to work in the natural style of an analyst, algebraist (e.g., Mines et al. 1988), geometer, topologist (e.g., Bridges and Vîță 2011, Sambin forthcoming), or other normal mathematician, the only constraints being those imposed by intuitionistic logic. As Bishop and others have shown, the traditional belief promulgated by Hilbert and still widely held today, that intuitionistic logic imposes such restrictions as to render the development of serious mathematics impossible, is patently false: large parts of deep modern mathematics can be, and have been, produced by purely constructive methods. Moreover, the link between constructive mathematics and programming holds great promise for the future implementation and development of abstract mathematics on the computer.
1. The Basics
1.1 Definition of Free Logic
Free logic is formal logic whose quantifiers are interpreted in the usual way—that is, objectually over a specified domain D—but whose singular terms may denote objects outside of D, or fail to denote at all. Singular terms include proper names (individual constants), definite descriptions, and such functional expressions as ‘2 + 2’. Since classical (i.e., Fregean) predicate logic requires that singular terms denote members of D, free logic is a “nonclassical” logic. Where D is, as usual, taken to be the class of existing things, free logic may be characterized as logic the referents of whose singular terms need not exist.
Karel Lambert (1960) coined the term ‘free logic’ as an abbreviation for ‘logic free of existence assumptions with respect to its terms, singular and general’. General terms are predicates. Lambert was suggesting that just as classical predicate logic generalized Aristotelian logic by, inter alia, admitting predicates that are satisfied by no existing thing (‘is a Martian’, ‘is non-self-identical’, ‘travels faster than light’), so free logic generalizes classical predicate logic by admitting singular terms that denote no existing thing (‘Aphrodite’, ‘the greatest integer’, ‘the present king of France’).
Because classical logic's singular terms must denote existing things (when, as usual, ‘∃’ is read as “there exists”), classical logic is unreliable in application to statements containing singular terms whose referents either do not exist or are not known to. Consider, for example, the true statement:
(S) We detect no motion of the earth relative to the ether,
using ‘the ether’ as a singular term for the light-bearing medium posited by nineteenth century physicists. The reason why (S) is true is that, as we now know, the ether does not exist. According to classical logic, however, (S) is false, because it implies the existence of the ether. Free logic allows such statements to be true despite the non-referring singular term. Indeed, it allows even statements of the form ~∃x x=t (e.g., “the ether does not exist”) to be true, though in classical logic, which presumes that t refers to an object in the quantificational domain, they are self-contradictory.
Free logic accommodates empty singular terms (those that denote no member of the quantificational domain D) by rejecting inferences whose validity depends on the classical presumption that they must denote members of D. Consider, for example, the rule of universal instantiation (specification): from the premise “Every x (in D) satisfies A” we may infer “t satisfies A.” This rule, whose formal expression is:
∀xA ⊢ A(t/x),
is invalid in free logic; for even if every object in D satisfies A, if t does not denote a member of D, A(t/x) may be false. (Here and elsewhere A(t/x) is the result of replacing all occurrences of x in A by individual constant t; if there are no such occurrences, then A(t/x) is just A.) Existential generalization (the principle that from “t satisfies A” we may infer “there exists (in D) a thing x that satisfies A”):
A(t/x) ⊢ ∃xA
is likewise invalid; for if t does not denote an object in D then the truth of A(t/x) does not guarantee that there exists in D an object that satisfies A. Though free logic rejects such classical inferences, it accepts no classically invalid inferences; hence it is strictly weaker than classical logic for a language with the same vocabulary.
To distinguish terms that denote members of D from those that do not, free logic often employs the one-place “existence” predicate, ‘E!’ (sometimes written simply as ‘E’). For any singular term t, E!t is true if t denotes a member of D, false otherwise. ‘E!’ may be either taken as primitive or (in bivalent free logic with identity) defined as follows:
E!t =df ∃x(x=t).
Using ‘E!’ we can express classical logic's blanket presumption that singular terms denote members of D as an explicit premise, E!t, for selected terms t. Thus we can formulate the following weaker analogs of universal instantiation:
∀xA, E!t ⊢ A(t/x)
and existential generalization:
A(t/x), E!t ⊢ ∃xA,
which are valid in free logic.
Classical predicate logic presumes not only that all singular terms refer to members of the quantificational domain D, but also that D is nonempty. Free logic rejects the first of these presumptions. Inclusive logic (sometimes also called empty or universally free logic) rejects them both. Thus while inclusive logic for a language containing singular terms must be free, free logics need not be inclusive.
Many existential assertions—e.g., ∃x(x=x), ∃x(Px → Px), ∃x(Px → ∀yPy)—are true in all nonempty domains and hence are valid in both classical logic and non-inclusive free logic. But since all existentially quantified formulas are false in the empty domain, none are valid in inclusive logic. Correlatively, since all universally quantified formulas are true in the empty domain, none are self-contradictory in inclusive logic. Even vacuously universally quantified formulas (formulas of the form ∀xA, where x is not free in A) are true in the empty domain. Hence the schema:
∀xA → A, where x is not free in A,
which is valid in both classical logic and non-inclusive free logic, is invalid in inclusive logic. Inclusive logic also invalidates some of the laws of confinement—e.g.,
∀x(P & A) ↔ (P & ∀xA), where x is not free in P,
that are used for prenexing formulas (giving quantifiers the widest possible scope) or purifying them (giving quantifiers the narrowest possible scope). And in inclusive logic the formula:
∀x(A ↔ x=t),
widely used in the theory of definite descriptions, is not equivalent, as it otherwise is, to:
∀x(A → x=t) & A(t/x),
since with D empty and A(t/x) false, the first but not the second is true. Where there is need for such regularities, a non-inclusive free logic may be preferable to an inclusive one. Yet because inclusivity frees logic from one more existential presumption, many free logicians favor it.
2. Formal Systems
Logics may be represented in various ways. Axiom systems, natural deduction systems and trees (or, equivalently, tableaux) are among the most common. This section presents all three for the bivalent inclusive form of free logic known as Positive Free Logic (PFL) and mentions some variants. (For the meaning of the term “positive” in this context see Section 3.2). PFL is formulated in a first-order language L without sentence letters or function symbols, whose primitive logical operators are negation (not) ‘~’, the conditional (if-then) ‘→’, the universal quantifier (for all) ‘∀’, identity ‘=’ and ‘E!’, the others being defined as usual. We assume for the sake of definiteness that the formulas of L are closed (contain no unquantified variables) and that they may be vacuously quantified (have the form ∀xA or ∃xA, where x does not occur free in A). An occurrence of a variable is quantified if it lies within the scope of an operator such as ‘∀’ or ‘∃’ that binds that variable; otherwise it is free.
2.1 Axiom Systems
PFL may be axiomatized, with modus ponens as the sole inference rule, by adding the following schemas to the tautologies of classical propositional logic:
(A1) A → ∀xA
(A2) ∀x(A → B) → (∀xA → ∀xB)
(A3) ∀xA, if A(t/x) is an axiom
(A4) ∀xA → (E!t → A(t/x))
(A5) ∀xE!x.
A note on conventions involving variables: once again, A(t/x) is the result of replacing all occurrences of x in A by individual constant t. If there are no such occurrences, then A(t/x) is just A. In (A1) the variable x is not free in A (since otherwise A would be an open formula and formulas of L are closed). However, x may be free in A or B in (A2) and in A in (A3) and (A4).
(A4) and (A5) are special axioms for free logic. The others are classical. (A4) modifies the classical principle:
(A4c) ∀xA → A(t/x)
by using ‘E!’ to restrict specification. (A4) stipulates in effect that the quantifiers range over all objects that satisfy ‘E!’, (A5) that they range only over objects that satisfy ‘E!’. Omitting (A5) and replacing (A4) with (A4c) yields classical logic. To obtain a non-inclusive free logic, we may add to (A1)-(A5) the axiom ∃xE!x—or any axiom of the form ∃xT such that for any term t, T(t/x) is a tautology.
For languages containing the identity predicate, we also need:
(A6) s=t → (A → A(t//s))
(where A(t//s) is the result of replacing one or more occurrences of s in A by t), and either
(A7) t=t
if all self-identity statements, including those whose singular term is empty, are to be true or
(A7−) ∀x(x=x)
if not (see Sections 3.1 and 3.2 below). If ‘E!’ is defined in terms of the identity predicate as indicated in Section 1.2, then (A4) takes the form:
∀xA → (∃y(y=t) → A(t/x))
and (A5) is redundant and may be omitted. ‘E!’ cannot be defined without the identity predicate (Meyer, Bencivenga and Lambert, 1982).
Free logic can be formalized without either ‘=’ or ‘E!’. (A1)–(A3) remain unchanged, but (A4) and (A5) are replaced respectively by:
(A4′) ∀y(∀xA → A(y/x))
(A5′) ∀x∀yA → ∀y∀xA.
(A4′), like (A4), restricts specification to objects within D, but it uses a quantifier instead of ‘E!’ to do so. The quantifier permutation axiom (A5′) is redundant in the presence of the identity axioms but, as Fine proved in (1983), is independent of the other axioms.
The formulas used in the axiom systems discussed so far are closed, but some free logics allow open formulas—i.e., formulas that contain free variables. These logics follow one of two conventions for variable assignments. Those that assign to each free variable a member of D are called E+-logics; those that do not are called E-logics. The following specification rule is valid in E+-logics but not in E-logics:
∀xA ⊢ A(v/x).
(Here A(v/x) is the result of replacing every occurrence of the variable x in A by a variable v that is free for x in A.) Conversely, the following substitution rule is valid in E-logics but not in E+-logics:
A ⊢ A(t/x).
But since this article employs closed formulas, the distinction between E- and E+-logics may here be ignored. (See Williamson (1999) for an illuminating discussion of problems engendered by permitting open formulas in inclusive logics.)
2.2 Natural Deduction Rules
PFL can also equivalently be formulated in a natural deduction system. The introduction and elimination rules for the operators of propositional logic and identity are as usual. The quantifier introduction and elimination rules are restricted by use of the predicate ‘E!’, as follows:
∀I:
Given a derivation of A(t/x) from E!t, where t is new and does not occur in A, discharge E!t and infer ∀xA.
∀E:
From ∀xA and E!t infer A(t/x).
∃I:
From A(t/x) and E!t infer ∃xA.
∃E:
Given ∃xA and a derivation of a formula B from A(t/x) & E!t, where t is new and does not occur in either A or B, discharge A(t/x) & E!t and infer B from ∃xA.
The variable x need not be free in A, in which case A(t/x) is just A. ‘E!’ may either be taken as primitive (in which case it requires no additional rules) or defined in terms of the identity predicate as in Section 1.2. For non-inclusive logic, we may add a rule that introduces ∃xE!x.
2.3 Tree Rules
Jeffrey-style tree rules (Jeffrey 1991) for PFL can be obtained by replacing the classical rules for existentially and universally quantified formulas with the following:
Existential Rule: If ∃xA appears unchecked on an open path, check it, and
if x is free in A, choose a new individual constant t and list both E!t and A(t/x) at the bottom of every open path beneath ∃xA, and
if x is not free in A, write A at the bottom of every open path beneath ∃xA.
Universal Rule: If ∀xA appears on an open path, then
if x is free in A, then where t is an individual constant that occurs in a formula on that path, or a new individual constant if there are none on the path, split the bottom of every open path beneath ∀xA into two branches, writing ~E!t at the bottom of the first branch and A(t/x) at the bottom of the second, and
if x is not free in A, write A at the bottom of every open path beneath ∀xA.
For languages that do not allow vacuous quantification, clause (ii) can in each case be omitted. Non-inclusive free logic needs an additional rule that introduces E!t for some new individual constant t if a path does not already contain a formula of this form.
3. Semantics
Semantics for free logics differ in how they assign truth-values to atomic formulas that are empty-termed—i.e., contain at least one empty singular term. There are three general approaches:
Negative semantics require all empty-termed atomic formulas to be false,
Positive semantics allow some empty-termed atomic formulas not of the form E!t to be true, and
Neutral (or nonvalent) semantics require all empty-termed atomic formulas not of the form E!t to be truth-valueless.
3.1 Negative Semantics
A negative semantics is a bivalent semantics on which all empty-termed atomic formulas (including identity statements) are false. The inclusive version presented here makes only minimal adjustments to classical semantics to allow for non-denoting terms.
Let the language L be defined as in Section 2. Then a negative inclusive model for L is a pair ⟨D,I⟩, where D is a possibly empty set (the domain) and I is an interpretation function that assigns referents to individual constants and extensions to predicates such that:
for each individual constant t of L, either I(t) ∈ D or I(t) is undefined, and
for each n-place predicate P of L, I(P) ⊆ Dn.
(Dn is the set of n-tuples of members of D, a 1-tuple of an object d being just d itself.) Given a model ⟨D,I⟩, we recursively define a valuation function V that assigns truth values to formulas as follows:
V(Pt1…tn)
=
T ⇔ I(t1),…, I(tn) are all defined and ⟨I(t1),…, I(tn)⟩ ∈ I(P);
F otherwise.
V(s=t)
=
T ⇔ I(s) and I(t) are both defined and I(s) = I(t);
F otherwise.
V(E!t)
=
T ⇔ I(t) is defined;
F otherwise.
V(~A)
=
T ⇔ V(A) = F;
F otherwise.
V(A → B)
=
T ⇔ V(A) = F or V(B) = T;
F otherwise.
V(∀xA)
=
T ⇔ for all d∈D, V(t,d)(A(t/x)) = T (where t is any individual constant not in A and V(t,d) is the valuation function on the model ⟨D,I*⟩ such that I* is just like I except that I*(t) = d);
F otherwise.
(The metalinguistic symbol ‘⇔’ means “if and only if.”) A logic adequate to this semantics may be axiomatized by making three changes to the axioms of PFL. The first is to add the axiom:
(A−) Pt1…tn → E!ti, where 1≤i≤n and P is any primitive n-place predicate, including ‘=’.
This expresses the convention that an atomic formula cannot be true unless its terms refer. Second, because all empty-termed identity statements are false on a negative semantics, (A7) is invalid and must be replaced by (A7−). Third, since (A2), (A3), (A−) and (A7−) together imply (A5), (A5) may be omitted. The resulting logic is known as NFL (Negative Free Logic). For languages with function symbols, negative free logic requires in addition this axiom of strictness:
E!f(t1,…,tn) → E!ti, where 1≤i≤n,
which assures that a function has a value only if each of its arguments does. Because of its unusual treatment of identity, negative free logic validates the equivalence:
t=t ↔ E!t.
(This equivalence is sometimes taken as a definition of E!t.) Identity statements in negative free logic thus have existential implications. This may be problematic in certain contexts. According to Shapiro and Weir (2000), for example, use of such an “existential” notion of identity sullies the “epistemic innocence” of some recent efforts to base neo-logicist philosophies of mathematics on free logic.
Negative free logic is also peculiar in that it validates the principle of indiscernibility of nonexistents:
(~E!s & ~E!t) → (A → A(t//s)),
where A(t//s) is the result of replacing one or more occurrences of s in A by t.
3.2 Positive Semantics
Positive semantics allow some empty-termed atomic formulas not of the form E!t to be true. They are typically bivalent, though there are variants that allow truth-value gaps or extra truth values. Only bivalent semantics are considered in this section.
Positive semantics treat formulas of the form t=t as true, whether or not t is empty. Hence they validate (A7), which affirms all self-identity statements, not merely the weaker (A7−), which affirms only self-identities between nonempty terms.
Like negative semantics, some positive semantics require each singular term to denote either a member of D or nothing at all. But then when a term fails to denote, the truth value of an atomic formula containing it cannot as usual be a function of its denotation, and the formula must be evaluated in some nonstandard way. To avoid such irregularity and yet permit empty-termed formulas to be true, other positive semantics allow singular terms to denote, and predicates to be satisfied by, nonmembers of D. These nonmembers are collected into a second or outer domain Do, in contrast to which D is described as the inner domain. The result is a dual-domain semantics.
Positive semantics with dual domains are generally the simplest. The members of the outer domain Do typically represent “non-existing” things. Depending on the application, these may be theoretical or ideal entities, error objects (in computer science), fictional objects, merely possible (or even impossible) objects, and so on. Some authors make D a subset of Do, which is the convention throughout this article; others make the two disjoint. In a bivalent dual-domain semantics each singular term denotes an object in Do though possibly not in D. Thus D, though not Do, may empty. Predicates are assigned extensions from Do, and the truth-values of atomic formulas (whether empty-termed or not) are computed in the usual Tarskian fashion: an atomic formula is true if and only if the n-tuple of objects denoted by its singular terms, taken in order, is a member of the predicate's extension. Identity statements are no exception. Statements of the form s=t are true if and only if s and t denote the same object. Hence, even if empty-termed, they may be true.
More formally, a dual-domain model for a language L of the sort defined in Section 2 is a triple ⟨D,Do,I⟩, where D is a possibly empty inner domain, Do is a nonempty outer domain such that D ⊆ Do, and I is an interpretation function such that for every individual constant t of L, I(t) ∈ Do, and for every n-place predicate P of L, I(P) ⊆ Don. Given a model ⟨D,Do,I⟩, the valuation function V assigns truth values to atomic and quantified formulas as follows:
V(Pt1…tn)
=
T ⇔ ⟨I(t1),…,I(tn)⟩ ∈ I(P);
F otherwise
V(s=t)
=
T ⇔ I(s) = I(t);
F otherwise
V(E!t)
=
T ⇔ I(t) ∈ D;
F otherwise
V(∀xA)
=
T ⇔ for all d∈D, V(t,d)(A(t/x)) = T (where t is not in A and V(t,d) is the valuation function on the model ⟨D,Do,I*⟩ such that I* is just like I except that I*(t) = d);
F otherwise
The clauses for ‘~’ and ‘ → ’ are the same as in negative free logic. PFL with classical identity — that is, the logic axiomatized by (A1)–(A7) — is sound and complete with respect to this semantics (Leblanc and Thomason 1968).
Dual-domain semantics have been criticized as ontologically extravagant. In response, some authors have advocated single-domain positive semantics, which assign no denotation to empty singular terms. In such semantics empty-termed atomic formulas require unconventional treatment. Typically such semantics determine the truth-values of atomic formulas in two different ways: a Tarksi-style calculation for formulas whose terms all refer, and a separate truth-value assignment for empty-termed atomic formulas. The details, however, tend to get complicated. Antonelli (2000), for example, advocated such a single-domain free logic, which he called proto-semantics, but more recently (2007, p. 72) he has characterized all semantics for positive free logic as “somewhat artificial” and has questioned the logical character of free quantification in general.
3.3 Neutral Semantics
Neutral semantics make all empty-termed atomic formulas not of the form E!t truth-valueless. Truth-valueless formulas are often said to have “truth-value gaps.” Neutral semantics are of two types: ordinary neutral semantics, which provide conventions for calculating the truth values of complex formulas directly from their components, even when there are empty terms, and supervaluational semantics, which calculate the truth values of complex formulas by considering all the values that their components could have if their empty terms had referents. Ordinary neutral semantics will be considered in this section, supervaluations in Section 3.4.
The uniform policy of making all empty-termed atomic formulas truth-valueless has the advantages of plausibility and simplicity at the atomic level, but it complicates the evaluation of complex formulas. How are the logical operators to function when some of the values on which they usually operate are absent? Some cases are fairly clear. The negation of a truth-valueless formula, for example, is generally taken to be truth-valueless. But:
If A is true and B truth-valueless, is A → B false or truth-valueless?
If A is false and B truth-valueless, is A → B true or truth-valueless?
Let A = (B & C), where x is free in B, B be true of some but not all members of D, and C be closed and truth-valueless. Clearly this open formula is either truth-valueless of every object in D or truth-valueless of some and false of others. In either case, is ∃xA truth-valueless or false?
At one extreme, we might want the operators to generate as many plausible truth values as possible in order to validate as many classically valid formulas as we can. At the other, one might arrange things so that all empty-termed formulas are truth-valueless, which would produce a very weak logic (Lehman 2001). But however we choose, many formulas that are valid in both classical predicate logic and the usual forms of free logic—indeed, even in propositional logic—will become invalid. The law of noncontradiction, for example:
~(A & ~A)
is truth-valueless whenever A is (unless we make negations of truth-valueless statements true) and hence becomes invalid. Of course this law and many other standard logical principles remain weakly valid—i.e., not false on any model—and it is possible to construct a logic based on weak validity rather than ordinary validity. But because any such logic will still be weaker than classical logic and because its theorems need not even be true, most logicians reject this strategy. For more on neutral free logic, see Lehman 1994, 2001, and 2002, pp. 233–237.
3.4 Supervaluations
Neutral semantics can be made to validate all the theorems of standard free logics by augmenting them with supervaluations. Supervaluations were first formalized by van Fraassen (1966). The version presented here is a variant of Bencivenga's approach (1981 and 1986).
The fundamental idea is this: when empty terms deprive a formula of truth-value, supervaluational semantics nevertheless accounts it true (or false) if all possible ways of assigning referents to those terms agree in making it true (or false). This strategy restores validity to many principles that would lose it in an ordinary neutral semantics. The following instance of the law of noncontradiction:
~(Pt & ~Pt),
for example, is truth-valueless when t is nondenoting (assuming an ordinary neutral semantics that makes the negation of a truth-valueless formula truth-valueless). Hence in such a semantics the law itself is invalid. Yet were we to assign a referent to t, that referent would either be in the extension of P or not. If it were, then Pt would be true. If it were not, then Pt would be false. In either case ~(Pt & ~Pt) would be true. Thus, since all possible ways of assigning referents to t agree in making ~(Pt & ~Pt) true, we should count ~(Pt & ~Pt) itself as true. In this way the law of noncontradiction can be preserved.
More explicitly, a supervaluation begins with a neutral model M with a single, possibly empty domain. We then construct the set of completions of M. These may be regarded as bivalent dual-domain positive models whose inner domain is the domain of M, but which also have an outer domain Do to provide referents for the empty terms. In each completion, singular terms that are nonempty in M retain their referents, and those that are empty in M denote a member of Do — D. For each n-place predicate P, the extension of P is a subset of Don and a superset of P's extension in M.
From these completions we now construct a supervaluation. A supervaluation of M is a partial assignment of truth-values to formulas that makes a formula true if all completions of M make it true, false if they all make it false, and truth-valueless if they disagree. A formula is valid on a supervaluational semantics if and only if it is true on all supervaluations. This semantics validates all and only the theorems of PFL (Bencivenga 1981, Morscher & Simons 2001, pp. 14–18).
Supervaluations employ what Bencivenga (1986) calls a “counterfactual theory” of truth: an empty-termed statement is true if it would be true on any assignment of referents to its empty terms. This has struck many critics as simply false. Moreover, the logic itself leaves much to be desired. For one thing, supervaluational consequence is too strong. Thus, for example, although the formula Pt → E!t is (quite properly) not valid on a supervaluational semantics, nevertheless since E!t is true on every supervaluation on which Pt is true, the sequent (derivability statement) Pt ⊢ E!t is improperly semantically valid. Therefore, although PFL is sound on supervaluational semantics and every semantically valid formula is a theorem of PFL, not all semantically valid sequents are provable in PFL. In fact, supervaluational consequence is not axiomatizable by any extension of free logic. This follows from a result of Woodruff (1984), who has shown that supervaluational semantics has many of the undesirable properties of second-order semantics. Jerry A. Fodor and Ernest Lapore (1996) argue, furthermore, that the completions needed to construct supervaluations are not meaning-preserving. Hence, they conclude, two alleged advantages of supervaluations—that they explain the meaningfulness of sentences with truth value gaps and that they allow us to preserve classical logic—are illusory. Finally, since supervaluations are built from completions that are in effect positive dual-domain models, we may wonder whether the detour through supervaluations is worth the trouble, since positive dual-domain models alone are simpler and more adequate to PFL.
4. Generic Anomalies
While problems noted above are specific to particular forms of free logic, there are anomalies that infect all, or nearly all, forms. This section considers three: (1) a cluster of problems related to the application of primitive predicates to empty terms, (2) the failure of substitutivity salva veritate of co-referential expressions, and (3) the inability of free logic to express sufficient conditions for existence.
4.1 Problems with Primitive Predicates
In classical logic and in positive free logic any substitution instance of a valid formula (or form of inference) is itself a valid formula (or form of inference). But in negative or neutral free logic this is not the case. A substitution instance is the result of replacing primitive non-logical symbols by possibly more complex ones of the same semantic type—n-place predicates with open formulas in n variables, and individual constants with singular terms—each occurrence of the same primitive symbol being replaced by the same possibly complex symbol. The replacement of an occurrence of a primitive n-place predicate P in some formula B by an open formula A with free variables x1,…,xn is performed as follows: where t1,…,tn are the individual constants or variables immediately following P in that occurrence, replace Pt1…tn in B by A(ti/xi)—the result of replacing xi by ti in A, for each i, 1≤i≤n.
Let P, for example, be a primitive one-place predicate. Then if the semantics is negative, Pt → E!t is valid. But now consider the substitution instance ~Pt → E!t, in which the open formula ~Px is substituted for P. This substitution instance is false when t is empty. Hence valid formulas may have invalid substitution instances. The same holds for ordinary neutral semantics that make conditionals true whenever their consequents are true.
In a negative semantics, moreover, the truth value of an empty-termed statement depends arbitrarily on our choice of primitive predicates. Consider, for example, a negative free logic interpreted over a domain of people that takes as primitive the one-place predicate ‘A’, meaning “is an adult,” and defines “is a minor” by this schema:
Mt =df ~At.
For any non-denoting name t, At is false in this theory; hence Mt is true. If we take ‘is a minor’ as primitive instead, the truth-values of At and Mt are reversed. But why should truth-values depend on primitiveness in this way?
Positive semantics avoid these anomalies. But, if bivalent, in application they force us to assign truth values to empty-termed formulas in some other way, often without sufficient reason. Consider, for example, these three formulas, all of which contain the empty singular term ‘1/0’ (where ‘/’ is the division sign):
1/0 = 1/0
1/0 > 1/0
1/0 ≤ 1/0
Assuming a bivalent positive semantics, which ones should we make true and which false? Since the semantics is positive, ‘1/0 = 1/0’ is automatically true. One might argue further that since ‘≤’ expresses a relationship weaker than ‘=’ and since ‘1/0 = 1/0’ is true, ‘1/0 ≤ 1/0’ should be true as well. But that is merely to mimic with empty terms an inference pattern that holds for denoting terms. To what extent is such mimicry justified? Suppose we do decide to make ‘1/0 ≤ 1/0’ true; should we therefore make ‘1/0 > 1/0’ false? There are no non-arbitrary criteria for answering such questions. To a large extent, of course, the answers don't matter. There are no facts here; any consistent convention will do. But that's just the problem. Some convention is needed, and establishing one can be a lot of bother for nothing.
4.2 Substitutivity Failures
Classical predicate logic has the desirable feature that co-extensive open formulas may be substituted for one another in any formula salva veritate—i.e., without changing that formula's truth value. (Open formulas A and B in n free variables x1,…,xn are coextensive if and only if ∀x1…∀xn(A ↔ B) is true.) But, as Lambert noted in 1974, this principle fails for nearly all free logics with identity. Consider, for example, the formula t=t, where t is empty, which is an instance of the open formula x=x. Now x=x is coextensive with both (x=x & E!x) and (E!x → x=x), since all three formulas are satisfied by all members of D. Hence if co-extensive open formulas could be exchanged salva veritate, (t=t & E!t) and (E!t → t=t) would have the same truth value as t=t. But on nearly all free logics this is not the case. Positive free logic and the supervaluations described in Section 3.4 make t=t true and (t=t & E!t) false; negative free logic makes t=t false and (E!t → t=t) true; and any ordinary neutral free logic whose conditionals are true whenever their antecedents are false makes t=t truth-valueless and (E!t → t=t) true. Many find this troubling because, since Frege, it has been widely held that (1) extensions of complex linguistic expressions should be functions of the extensions of their components (so that co-extensive components should be exchangeable without affecting the extension of the whole) and (2) the extension of a formula (or statement) is a truth value.
One possible response is to reject (2). Leeb (2006) develops for a version of PFL a dual-domain semantics in which the extensions of formulas are abstract states of affairs. In this semantics, co-referential open sentences are exchangeable not salve veritate, but (as he puts it) salve extensione; that is, the exchange does not alter the state of affairs designated by the statement in which it occurs. But Leeb's state-of-affairs semantics is so complex that it may discourage application.
Those who wish to retain (2) may be consoled by the following observation: though substitutivity salve veritate of co-extensive open formulas fails for nearly all free logics, a related but weaker principle, the substitutivity salve veritate of co-comprehensive open formulas, is valid for positive free logics. Open formulas A and B in n free variables x1,…,xn are co-comprehensive if every assignment of denotations in the outer domain Do to x1,…,xn satisfies A if and only if it satisfies B. Among the open formulas mentioned in the previous paragraph, for example, x=x and (E!x → x=x) are co-comprehensive in a dual-domain positive free logic, being satisfied by all members of Do, but (x=x & E!x) is not co-comprehensive with them, since it is satisfied only by the members of D. Unlike co-extensiveness, however, co-comprehensiveness is not expressible in the language of PFL. But it becomes expressible with the introduction of quantifiers over the outer domain—a strategy considered in Section 5.5.
4.3 Inexpressibility of Existence Conditions
‘Whatever thinks exists,’ ‘Any necessary being exists’, ‘That which is immediately known exists’: such statements of sufficient conditions for existence are prominent in metaphysical debates. But, somewhat surprisingly, they are not expressible in free logic. Their apparent form is ∀x(A → E!x). But because the universal quantifier ranges just over D, which is also the extension of E!, this form is valid in free logic—as it is in classical logic with E!x expressed as ∃y y=x. No statement of this form—not even ‘all impossible things exist’—can be false. Hence on free logic all such statements are equally devoid of content. Argument evaluation suffers as a result. Consider, for example, the obviously valid inference:
Its natural formalization in free logic is Ti, ∀x(Tx → E!x) ⊢ E!i. But this form is invalid. To obtain the conclusion, we must first deduce Ti → E!i by specification from the second premise and then use modus ponens with the first. But since the logic is free, specification requires the question-begging premise E!i. A remedy is not to be found in free logic alone, but once again quantification over the outer domain of a dual-domain semantics may help (see Section 5.5).
5. Some Applications
This section considers applications of free logic in theories of definite descriptions, languages that allow partial or non-strict functions, logics with Kripke semantics, logics of fiction and logics that are in a certain sense “Meinongian.” Free logic has also found application elsewhere—most prominently in theories of predication, programming languages, set theory, logics of presupposition (with neutral semantics), and definedness logics. For more on these and other applications, see Lambert 1991 and 2001b; Lehman 2002, pp. 250–253; and Nolt 2006, pp. 1039–1053.
5.1 Theories of Definite Descriptions
The earliest and most extensive applications of free logic have been to the theory of definite descriptions. A definite description is a phrase that may be expressed in the form “the x such that A,” where A is an open formula with only x free. Formally, this is written using a special logical operator, the definite description operator ‘ι’, as ιxA. Contra Russell, free logic treats definite descriptions not as merely apparent singular terms in formulas whose logical form is obtainable only by elaborate contextual definitions, but as genuine singular terms. Thus, like an individual constant, ιxA may be attached to predicates and (under appropriate conditions) substituted for variables. For any object d in the domain D, ιxA denotes d if and only if among all objects in D, d and only d satisfies A. If in D there is more than one object satisfying A, or none, ιxA is empty. The description operator therefore obeys Lambert's Law:
(LL) ∀y(y=ιxA ↔ ∀x(A ↔ x=y)), x free in A.
Adding (LL) to the free logic defined by (A1)–(A6) and (A7−) gives the minimal free definite description theory MFD. MFD is the core of virtually all free description theories, which therefore differ only in the additional principles they endorse.
There is plenty of room for variation, for MFD fails to specify truth conditions for atomic formulas (including identities) when they contain empty descriptions, and there are many ways to do it. Making all atomic formulas containing empty descriptions false yields a negative free description theory axiomatizable by adding (LL) to NFL (Burge 1974, Lambert 2001h). The result is essentially Bertrand Russell's theory of definite descriptions, but with the description operator taken as primitive rather than contextually defined.
The simplest positive free description theory makes all identities between empty terms true. Known as FD2, it may be axiomatized by adding (LL) and:
(~E!s & ~E!t) → s=t
to PFL. FD2 is akin to Gottlob Frege's theory of definite descriptions; but whereas Frege chose a single arbitrary existing object to serve as the conventional referent for empty singular terms, FD2 makes this object non-existent. FD2 is readily modeled in a dual-domain positive semantics with just one object in the outer domain.
On FD2 all empty descriptions are intersubstitutable salve veritate. But this result is subject to counterexamples in ordinary language. This statement:
The golden mountain is a possible object,
for instance, is true, while this one:
The set of all non-self-membered sets is a possible object,
is false—though each applies the same predicate phrase ‘is a possible object’ to an empty description. Thus we may prefer a more flexible positive free description theory on which identities between empty terms may be false. The literature presents a surprising diversity of these (Lambert 2001a, 2003c, 2003d, 2003h; Bencivenga 2002, pp. 188–193; Lehman 2002, pp. 237–250).
5.2 Logics with Partial or Non-Strict Functions
Some logics employ primitive n-place function symbols—symbols that combine with n singular terms to form a complex singular term. Thus, for example, the plus sign ‘+’ is a two-place function symbol that, when placed between, say, ‘2’ and ‘3’, forms a complex singular term, ‘2 + 3’ that denotes the number five. Similarly, ‘2’ is a one-place function symbol that, when placed after term denoting a number, forms a complex singular term that denotes that number's square. Semantically, the extension of a function symbol is a function whose arguments are members of the quantificational domain D, and the resulting complex term denotes the result of applying that function to the referents of the n component singular terms, taken in the order listed. Since classical logic requires every singular term (including those formed by function symbols) to refer to to an object in D, for each such function symbol f, it requires that:
∀x1…∀xn∃y(y = f(x1, …, xn)).
Hence classical logic prohibits primitive function symbols whose extensions are partial functions—functions whose value is for some arguments undefined. Such, for example, is the binary division sign ‘/’, since when placed between two numerals the second of which is ‘0’, it forms an empty singular term. Similarly, the limit function symbol ‘lim’ yields an empty singular term when applied to the name of a non-coverging sequence. Classical logic can accomodate function symbols for partial functions via elaborate contextual definitions. But then (as with Russellian definite descriptions) the form in which these function symbols are usually written is not their logical form. Free logic provides a more elegant solution. Because it allows empty singular terms, symbols for partial functions may simply be taken as primitive.
In applications of free logic involving partial functions, the existence predicate ‘E!’ is often replaced by the postfix definedness predicate ‘↓’. For any singular term t, t↓ is true if and only if t has some definite value in D. Thus, for example, the formula ‘(1/0)↓’ is false. While some writers (e.g., Feferman (1995)) distinguish ‘↓’ from ‘E!’, the literature as a whole does not, and ‘↓’ is often merely a syntactic variant of ‘E!’.
In addition to partial functions, positive free logics can also readily handle non-strict functions. A non-strict function is a function that may yield a value even if not all of its arguments are defined. The binary function f such that f(x,y) = x, for instance, can yield a value even if the y-term is empty. So, for example, the formula f(1, 1/0) = 1 can be regarded as true. Logics for non-strict functions must be positive because in a negative or neutral logic empty-termed atomic formulas, such as f(1, 1/0) = 1, cannot be true. Free logics involving non-strict functions find application in some programming languages (Gumb 2001, Gumb and Lambert 1991). Such logics may employ a dual-domain semantics in which the referents of empty functional expressions such as ‘1/0’ are regarded as error objects—objects that correspond in the running of a program to error messages. Thus, for example, an instruction to calculate f(1, 1/0) might return the value 1, but an instruction to calculate f(1/0, 1) would return an error message.
5.3 Logics with Kripke Semantics
Kripke semantics for quantified modal logics, tense logics, deontic logics, intuitionistic logics, and so on, are often free. This is because they index truth to certain objects that we shall call “worlds,” and usually some things that we have names for do not exist in some of these worlds. Worlds may be conceived in various ways: they may, for example, be understood as possible universes in alethic modal logic, times or moments in tense logic, permissible conditions in deontic logic, or epistemically possible states of knowledge in intuitionistic logic. Associated with each world w is a domain Dw, of objects (intuitively, the set of objects that exist at w). An object may exist in (or “at”) more than one world but need not exist in all. Thus, for example, Kripke semantics for tense logic represents the fact that Bertrand Russell existed at one time but exists no longer by Russell's being a member of the domains of certain “worlds”—that is, times (specifically, portions of the last two centuries)—but not others (the present, for example, or all future times). Two natural assumptions are made here: that the same object may exist in more than one world (this is the assumption of transworld identity), and that some singular terms—proper names, in particular—refer to not only to an object at a given world, but to that same object at every world. Such terms are called rigid designators. Any logic that combines rigid designators with quantifiers over the domains of worlds in which their referents do not exist must be free.
Kripke semantics gives predicates different extensions in different worlds. Thus, for example, the extension of the predicate ‘is a philosopher’ was empty in all worlds (times) before the dawn of civilization and more recently has varied. For rigidly designating terms, this raises the question of how to evaluate atomic formulas at worlds in which their referents do not exist. Is the predicate ‘is a philosopher’ satisfied, for example, by Russell in worlds (times) in which he does not exist—times such as the present? The general answers given to such questions determine whether a Kripke semantics is positive, negative or neutral.
For negative or neutral semantics, the extension at w of an n-place predicate P is a subset of Dwn. An atomic formula can be true at w only if all its singular terms have referents in Dw; if not, it is false (in negative semantics) or truth-valueless (in neutral semantics). In a positive semantics, atomic formulas that are empty-termed at w may nevertheless be true at w. Predicates are usually interpreted over the union U of domains of all the worlds, which functions as a kind of outer domain for each world, so that the extension of an n-place predicate P at a world w is a subset of Un. Some applications, however, require predicates to be true of—and singular terms to be capable of denoting—objects that exist in no world. If so, we may collect these objects into an outer domain that is a superset of U. (They might be fictional objects, timeless Platonic objects, impossible objects, or the like.)
Quantified formulas, like all formulas, are true or false only relative to a world. Thus ∃xA, for example, is true at a world w if and only if some object in Dw satisfies A. Except in intuitionistic logic, where it has a specialized interpretation, the universal quantifier is interpreted similarly: ∀xA is true at w if and only if all objects in Dw satisfy A. Kripke semantics often specify that for each w, Dw is nonempty, so that the resulting free logic is non-inclusive—but we shall not do so.
Any of various free modal or tense logics can be formalized by adding to a language L of the sort defined in Section 2 the sentential operator ‘□’. If A is a formula, so is □A. In alethic modal logic, this operator is read “it is necessarily the case that.” More generally, it means “it is true in all accessible worlds that,” where accessibililty from a given world is a different relation for different modalities: possibility for alethic logics, permissibility for deontic logics, various temporal relations for tense logics, and so on. A typical bivalent Kripke model M for such a language consists of a set of worlds, a binary accessibility relation R defined on that set; an assignment to each world w of a domain Dw; an “outer” domain Do of objects (which typically is either U or a superset thereof); and a two-place interpretation function I that assigns denotations at worlds to individual constants and extensions at worlds to predicates. For each individual constant t and world w, I(t,w)∈ Do. In such a model, a singular term is a rigid designator if and only if for all worlds w1 and w2, I(t,w1) = I(t,w2). For every n-place predicate P, I(P,w) ⊆ Dwn if the semantics is negative or neutral; if it is positive, I(P,w) ⊆ Don. Truth values at the worlds of a model M are assigned by a two-place valuation function V (where V(A,w) is read “the truth value V assigns to formula A at world w”) as follows:
V(Pt1…tn,w)
=
T ⇔ ⟨I(t1,w),…,I(tn,w)⟩ ∈ I(P,w);
F otherwise.
V(s=t,w)
=
T ⇔ I(s,w) = I(t,w);
F otherwise.
V(E!t,w)
=
T ⇔ I(t,w) ∈ Dw;
F otherwise.
V(~A,w)
=
T ⇔ V(A,w) = F;
F otherwise.
V(A → B,w)
=
T ⇔ V(A,w) = F or V(B,w) = T;
F otherwise.
V(□A,w)
=
T ⇔ for all u such that wRu, V(A,u) = T;
F otherwise.
V(∀xA,w)
=
T ⇔ for any d ∈ Dw, V(t,d)(A(t/x),w) = T (where t is not in A and V(t,d) is the valuation function for the model just like M except that its interpretation function I* is such that for each world w, I*(t,w) = d);
F otherwise.
Under the stipulations that admissible models make all individual constants rigid designators and that I(P,w) ⊆ Don, the standard free logic PFL, together with the modal axioms and rules appropriate to whatever structure we assign to R, is sound and complete on this semantics.
Modal semantics thus defined call for free logic whenever worlds are allowed to have differing domains—that is whenever we may have worlds u and w such that Du ≠ Dw. For in that case there must be an object d that exists in one of these domains (let it be Dw), but not the other, so that any singular term t that rigidly designates d must be empty at world u. Hence ~∃x(x=t) (which is self-contradictory in classical logic) must be true at world u. Such a semantics also requires free logic when Do contains objects not in U, for in that case rigid designators of these objects are empty in all worlds. Finally, this semantics calls for inclusive logic if any world has an empty domain. Thus, given this semantics, the only way to make the resulting logic unfree is to require that domains be fixed—i.e., that all worlds have the same domain D, that D be non-empty, and that Do = D.
Just this trio of requirements was in effect proposed by Saul Kripke in his ground-breaking (1963) paper on modal logic as one of two strategies for retaining classical quantification. (The other, more draconian, strategy was to allow differing domains but ban individual constants and treat open formulas as if they were universally quantified.) But such fixed-domain semantics validate the implausible formula:
∀x□∃y(y = x),
which asserts that everything exists necessarily and the equally implausible Barcan formula:
∀x□A → □∀xA
(named for Ruth Barcan, later Ruth Barcan Marcus, who discussed it as early as the late 1940s). To see its implausibility, consider this instance: ‘If everything is necessarily a product of the big bang, then necessarily everything is a product of the big bang’. It may well be true that everything (in the actual world) is necessarily a product of the big bang—i.e., that nothing in this world would have existed without it. But it does not seem necessary that everything is a product of the big bang, for other universes are possible in which things that do not exist in the actual world have other ultimate origins. Because of the restrictiveness and implausibility of fixed-domain semantics, many modal logicians loosen Kripke's strictures and adopt free logics.
We may also drop the assumption that singular terms are rigid designators and thus allow nonrigid designators. On the semantics considered here, these are singular terms t such that for some worlds w1 and w2, I(t,w1) ≠ I(t,w2). Definite descriptions, understood attributively, are the best examples. Thus the description “the oldest person” designates different people at different times (worlds)—and no one at times before people existed (“worlds” w at which I(t,w) is undefined).
Nonrigid designators, if empty at some worlds, require free logics even with fixed domains. (Thus classical logic with nonrigid designators is possible only if we require for each singular term t that at each world w, t denotes some object in Dw.) On some semantics for nonrigid designators, the quantifier rule must differ from that given above, and other adjustments must be made. For details, see Garson 1991, Cocchiarella 1991, Schweitzer 2001 and Simons 2001.
Intuitionistic logic, too, has a Kripke semantics, though special valuation clauses are needed for ‘~’, ‘→’ and ‘∀’ in order to accommodate the special meanings these operators have for intuitionists, and ‘□’ is generally not used. The usual first-order intuitionistic logic, the Heyting predicate calculus (HPC)—also called the intuitionistic predicate calculus—has the theorem ∃x(x=t) and hence is not free. But intuitionists admit the existence only of objects that can in some sense be constructed, while classical mathematicians posit a wider range of objects. Therefore users of HPC cannot legitimately name all the objects that classical mathematicians can. Worse, they cannot legitimately name objects whose constructibility has yet to be determined. Yet some Kripke-style semantics for HPC do allow use of names for such objects (semantically, names of objects that “exist” at worlds accessible from the actual world but not at the actual world itself). Some such semantics, though intended for HPC, have turned out, unexpectedly, not to be adequate for HPC. An obvious fix, advocated by Posy (1982), is to adopt a free intuitionistic logic. For more on this issue, see Nolt 2007.
5.4 Logics of Fiction
Because fictions use names that do not refer to literally existing things, free logic has sometimes been employed in their analysis. So long as we engage in the pretense of a story, however, there is no special need for it. It is true, for example, in Tolkien's The Lord of the Rings that Gollum hates the sun, from which we can legitimately infer that in the story there exists something that hates the sun. Thus quantifiers may behave classically so long as we consider only what occurs and what exists “in the story.” (The general logic of fiction, however, is often regarded as nonclassical, for two reasons: (1) a story may be inconsistent and hence require a paraconsistent logic, and (2) the objects a story describes are typically (maybe always) incomplete; that is, the story does not determine for each such object o and every property P whether or not o has P.)
The picture changes, however, when we distinguish what is true in the story from what is literally true. For this purpose logics of fiction often deploy a sentence operator that may be read “in the story.” Here we shall use ‘Sx’ to mean “in the story x,” where ‘x’ is to be replaced by the name of a specific story. Anything within the scope of this operator is asserted to be true in the named story; what is outside its scope is to be understood literally. (For a summary of theories of what it means to be true in a story, see Woods 2006.)
With this operator the statement ‘In the story, The Lord of the Rings, Gollum hates the sun’ may be formalized as follows:
SThe Lord of the Rings(Gollum hates the sun).
The statement that in The Lord of the Rings something hates the sun is:
SThe Lord of the Rings∃x(x hates the sun).
This second statement follows from the first, even though Gollum does not literally exist. But it does not follow that there exists something such that it, in The Lord of the Rings, hates the sun:
∃xSThe Lord of the Rings(x hates the sun),
and indeed that statement is not true, for, literally, Gollum does not exist. Since the sun, however, exists both literally and in the story, the statement:
∃xSThe Lord of the Rings(Gollum hates x)
is true and follows by free existential generalization from ‘SThe Lord of the Rings(Gollum hates the sun)’ together with the true premise ‘E!the sun’. Thus free logic may play a role in reasoning that mixes fictional and literal discourse.
Terms for fictional entities also occur in statements that are entirely literal, making no mention of what is true “in the story.” Consider, for example, the statement:
(G) Gollum is more famous than Gödel.
Mark Sainsbury (2005, ch. 6) holds that reference failure invariably makes such statements false and hence that they are best represented in a negative free logic. Others, however—including Orlando 2008 and Dumitru and Kroon 2008—question Sainsbury's treatment, maintaining that statements like (G) are both atomic and true. If so, they require a positive free logic. The logic must be free because it deals with an empty singular term, and it must be positive, because only on a positive semantics can empty-termed atomic statements be true. One must still decide, however, whether the name ‘Gollum’ is to be understood as having no referent or as having a referent that does not exist.
If ‘Gollum’ has no referent, then (G) might be handled by a single-domain positive semantics. But that semantics would have to treat atomic formulas non-standardly; it could not, as usual, stipulate that (G) is true just in case the pair ⟨Gollum, Gödel⟩ is a member of the extension of the predicate ‘is more famous than’; for if there is no Gollum, there is no such pair. On such a semantics ‘Gollum is more famous than Gödel’ would not imply that something is more famous than Gödel.
If, on the other hand, terms such as ‘Gollum’ refer to non-existent objects, then those objects could inhabit the outer domain of a dual-domain positive free logic. If so, atomic formulas have their standard truth conditions: (G) is true just in case ⟨Gollum, Gödel⟩ is a member of the extension of ‘is more famous than’. Moreover, if we allow quantifiers over that outer domain, then ‘Something is more famous than Gödel’ (where the quantifier ranges over the outer domain) does follow from ‘Gollum is more famous than Gödel’, though ‘There literally exists something more famous than Gödel’ (where the quantifier ranges over the inner domain) does not. Meinongian logics of fiction employ this strategy.
5.5 Meinongian Logics
Alexius Meinong is best known for his view that some objects that do not exist nevertheless have being. His name has been associated with various developments in logic. Some free logicians use it to describe any dual-domain semantics. For others, Meinongian logic is something much more elaborate: a rich theory of all the sorts of objects we can think about—possible or impossible, abstract or concrete, literal or fictional, complete or incomplete. In this section the term is used to describe logics stronger than the first type but possibly weaker than the second: positive free logics with an extra set of quantifiers that range over the outer domain of a dual-domain semantics.
Whether such logics can legitimately be considered free is controversial. On older conceptions, free logic forbids any quantification over non-existing things (see Paśniczek 2001 and Lambert's reply in Morscher and Hieke 2001, pp. 246–8). But by anybody's definition, Meinongian logics in the sense intended here at least contain free logics when the inner domain is interpreted as the set of existing things. Moreover, on the strictly semantic definition used in this article (Section 1.1), which is also that of Lehman 2002, whether the members of D exist is irrelevant to the question of whether a logic is free. For a defense of this definition, see Nolt 2006, pp. 1054–1057.
Historically, quantification over domains containing objects that do not exist has been widely dismissed as ontologically irresponsible. Quine (1948) famously maintained that existence is just what an existential quantifier expresses. Yet nothing forces us to interpret “existential” quantification over every domain as expressing existence—or being of any sort. Semantically, an existential quantifier on a variable x is just a logical operator that takes open formulas on x into truth values; the value is T if and only if the open formula is satisfied by at least one object in the quantifier's domain. That the objects in the domain have or lack any particular ontological status is a philosophical interpretation of the formal semantics. Alex Orenstein (1990) argues that “existential” is a misnomer and that we should in general call such quantifiers “particular.” That suggestion is followed in the remainder of this section.
Quantifiers ranging over the outer domain of a dual-domain semantics are called outer quantifiers, and those ranging over the inner domain inner quantifiers. If the inner particular quantifier is interpreted to mean “there exists” and the members of the outer domain are possibilia, then the outer particular quantifier may mean something like “there is possible a thing such that” or “for at least one possible thing.” We shall use the generalized product symbol ‘Π’ for the outer universal quantifier and the generalized sum symbol ‘Σ’ for its particular dual. This notation enables us to formalize, for example, the notoriously puzzling but obviously true statement ‘Some things don't exist’ (Routley 1966) as:
Σx~E!x.
Since in a dual-domain semantics all singular terms denote members of the outer domain, the logic of outer quantifiers is not free but classical. With ‘E!’ as primitive, the free inner quantifiers can be defined in terms of the classical outer ones as follows:
∀xA =df Πx(E!x → A)
∃xA =df Σx(E!x & A).
The outer quantifiers, however, cannot be defined in terms of the inner.
Logics with both inner and outer quantifiers have various applications. They enable us, for example, to formalize substantive sufficient conditions for existence and hence adequately express the argument of Section 4.3, as follows:
Ti, Πx(Tx → E!x) ⊢ E!i.
This form is valid. The co-comprehensiveness of open formulas A and B in n free variables x1,…,xn (see Section 4.2), can likewise be formalized as:
Πx1…Πxn(A ↔ B).
Richard Grandy's (1972) theory of definite descriptions holds that ιxA=ιxB is true if and only if A and B are co-comprehensive and thus is readily expressible in a Meinongian logic. Free logics with outer quantifiers have also been employed in logics that are Meinongian in the richer sense of providing a theory of objects (including, in some cases, fictional objects) that is inspired by Meinong's work (Routley 1966 and 1980, Parsons 1980, Jacquette 1996, Paśniczek 2001, Priest 2005 and 2008, pp. 295–7).
6. History
Inclusive logic was conceived and formalized before free logic per se was. Thus, since inclusive logic with singular terms is de facto free, the inventors of inclusive logics were, perhaps unwittingly, the inventors of free logic. Bertrand Russell suggested the idea of an inclusive logic in (1919, p. 201, n.). Andrezej Mostowski (1951) seems to have been among the first to formalize such a logic (but see Morscher and Simons 2001, p. 27, note 3). Theodore Hailperin (1953), Czeslaw Lejewski (1954) and W. V. O. Quine (1954) made important early contributions. It was Quine who dubbed such logics “inclusive.”
Henry S. Leonard (1956) was the first to develop a free logic per se, though he used a defective definition of ‘E!’. Karel Lambert began his prolific series of contributions to the field in (1958), critiquing Leonard's definition, and then coining the term “free logic” in (1960). The early systems of free logic were positive. Negative free logic was developed by Rolf Schock in a series of papers during the 1960s, culminating in (1968). Timothy Smiley suggested the idea of a neutral free logic in (1960), but the first thoroughgoing treatment appeared in Lehman 1994. Supervaluations were described in Mehlberg 1958, pp. 256–260, as a device for handling, not neutral free logic, but vagueness. But their formalization and application to free logic began with van Fraassen 1966, in which the term “supervaluation” was introduced. Dual-domain semantics were discussed in lectures by Lambert, Nuel Belnap and others as early as the late 1950s, but it appears that Church 1965 and Cocchiarella 1966 were the first published accounts.
Much of the debate about identity in recent decades has been about personal identity, and specifically about personal identity over time, but identity generally, and the identity of things of other kinds, have also attracted attention. Various interrelated problems have been at the centre of discussion, but it is fair to say that recent work has focussed particularly on the following areas: the notion of a criterion of identity; the correct analysis of identity over time, and, in particular, the disagreement between advocates of perdurance and advocates of endurance as analyses of identity over time; the notion of identity across possible worlds and the question of its relevance to the correct analysis of de re modal discourse; the notion of contingent identity; the question of whether the identity relation is, or is similar to, the composition relation; and the notion of vague identity. A radical position, advocated by Peter Geach, is that these debates, as usually conducted, are void for lack of a subject matter: the notion of absolute identity they presuppose has no application; there is only relative identity. Another increasingly popular view is the one advocated by Lewis: although the debates make sense they cannot genuinely be debates about identity, since there are no philosophical problems about identity. Identity is an utterly unproblematic notion. What there are, are genuine problems which can be stated using the language of identity. But since these can be restated without the language of identity they are not problems about identity. (For example, it is a puzzle, an aspect of the so-called “problem of personal identity”, whether the same person can have different bodies at different times. But this is just the puzzle whether a person can have different bodies at different times. So since it can be stated without the language of personal “identity”, it is not a problem about identity, but about personhood.) This article provides an overview of the topics indicated above, some assessment of the debates and suggestions for further reading.
To say that things are identical is to say that they are the same. “Identity” and “sameness” mean the same; their meanings are identical. However, they have more than one meaning. A distinction is customarily drawn between qualitative and numerical identity or sameness. Things with qualitative identity share properties, so things can be more or less qualitatively identical. Poodles and Great Danes are qualitatively identical because they share the property of being a dog, and such properties as go along with that, but two poodles will (very likely) have greater qualitative identity. Numerical identity requires absolute, or total, qualitative identity, and can only hold between a thing and itself. Its name implies the controversial view that it is the only identity relation in accordance with which we can properly count (or number) things: x and y are to be properly counted as one just in case they are numerically identical (Geach 1973).
Numerical identity is our topic. As noted, it is at the centre of several philosophical debates, but to many seems in itself wholly unproblematic, for it is just that relation everything has to itself and nothing else – and what could be less problematic than that? Moreover, if the notion is problematic it is difficult to see how the problems could be resolved, since it is difficult to see how a thinker could have the conceptual resources with which to explain the concept of identity whilst lacking that concept itself. The basicness of the notion of identity in our conceptual scheme, and, in particular, the link between identity and quantification has been particularly noted by Quine (1964).
Numerical identity can be characterised, as just done, as the relation everything has to itself and to nothing else. But this is circular, since “nothing else” just means “no numerically non-identical thing”. It can be defined, equally circularly (because quantifying over all equivalence relations including itself), as the smallest equivalence relation (an equivalence relation being one which is reflexive, symmetric and transitive, for example, having the same shape). Other circular definitions are available. Usually it is defined as the equivalence relation (or: the reflexive relation) satisfying Leibniz's Law, the principle of the indiscernibility of identicals, that if x is identical with y then everything true of x is true of y. Intuitively this is right, but only picks out identity uniquely if “what is true of x” is understood to include “being identical with x”; otherwise it is too weak. Circularity is thus not avoided. Nevertheless, Leibniz's Law appears to be crucial to our understanding of identity, and, more particularly, to our understanding of distinctness: we exhibit our commitment to it whenever we infer from “Fa” and “Not-Fb” that a is not identical with b. Strictly, what is being employed in such inferences is the contrapositive of Leibniz's Law (if something true of a is false of b, a is not identical with b), which some (in the context of the discussion of vague identity) have questioned, but it appears as indispensable to our grip on the concept of identity as Leibniz's Law itself.
The converse of Leibniz's Law, the principle of the identity of indiscernibles, that if everything true of x is true of y, x is identical with y, is correspondingly trivial if “what is true of x” is understood to include “being identical with y” (as required if Leibniz's Law is to characterise identity uniquely among equivalence relations). But often it is read with “what is true of x” restricted, e.g., to qualitative, non-relational, properties of x. It then becomes philosophically controversial. Thus it is debated whether a symmetrical universe is possible, e.g., a universe containing two qualitatively indistinguishable spheres and nothing else (Black 1952).
Leibniz's Law has itself been subject to controversy in the sense that the correct explanation of apparent counter-examples has been debated. Leibniz's Law must be clearly distinguished from the substitutivity principle, that if “a” and “b” are codesignators (if “a=b” is a true sentence of English) they are everywhere substitutable salva veritate. This principle is trivially false. “Hesperus” contains eight letters, “Phosphorus” contains ten, but Hesperus (the Evening Star) is Phosphorus (the Morning Star). Again, despite the identity, it is informative to be told that Hesperus is Phosphorus, but not to be told that Hesperus is Hesperus (“On Sense and Reference” in Frege 1969). Giorgione was so-called because of his size, Barbarelli was not, but Giorgione was Barbarelli (Quine, “Reference and Modality”, in 1963) . It is a necessary truth that 9 is greater than 7, it is not a necessary truth that the number of planets is greater than 7, although 9 is the number of planets. The explanation of the failure of the substitutivity principle can differ from case to case. In the first example, it is plausible to say that “‘Hesperus’ contains eight letters” is not about Hesperus, but about the name, and the same is true, mutatis mutandis, of “‘Phosphorus’ contains ten letters”. Thus the names do not have the same referents in the identity statement and the predications. In the Giorgione/Barbarelli example this seems less plausible. Here the correct explanation is plausibly that “is so-called because of his size” expresses different properties depending on the name it is attached to, and so expresses the property of being called “Barbarelli” because of his size when attached to “Barbarelli” and being called “Giorgione” because of his size when attached to “Giorgione”. It is more controversial how to explain the Hesperus/Phosphorus and 9/the number of planets examples. Frege's own explanation of the former was to assimilate it to the “Hesperus”/“Phosphorus” case: in “It is informative to be told that Hesperus is Phosphorus” the names do not stand for their customary referent but for their senses. A Fregean explanation of the 9/number of planets example may also be offered: “it is necessary that” creates a context in which numerical designators stand for senses rather than numbers.
For present purposes the important point to recognise is that, however these counter-examples to the substitutivity principle are explained, they are not counter-examples to Leibniz's Law, which says nothing about substitutivity of codesignators in any language.
The view of identity just put forward (henceforth “the classical view”) characterises it as the equivalence relation which everything has to itself and to nothing else and which satisfies Leibniz's Law. These formal properties ensure that, within any theory expressible by means of a fixed stock of one- or many-place predicates, quantifiers and truth-functional connectives, any two predicates which can be regarded as expressing identity (i.e., any predicates satisfying the two schemata “for all x, Rxx” and “for all x, for all y, Rxy → (Fx → Fy)” for any one-place predicate in place of “F”) will be extensionally equivalent. They do not, however, ensure that any two-place predicate does express identity within a particular theory, for it may simply be that the descriptive resources of the theory are insufficiently rich to distinguish items between which the equivalence relation expressed by the predicate holds (“Identity” in Geach 1972).
Following Geach, call a two-place predicate with these properties in a theory an “I-predicate” in that theory. Relative to another, richer, theory the same predicate, interpreted in the same way, may not be an I-predicate. If so it will not, and did not even in the poorer theory, express identity. For example, “having the same income as” will be an I-predicate in a theory in which persons with the same income are indistinguishable, but not in a richer theory.
Quine (“Identity, Ostension and Hypostasis”in his 1963) has suggested that when a predicate is an I-predicate in a theory only because the language in which the theory is expressed does not allow one to distinguish items between which it holds, one can reinterpret the sentences of the theory so that the I-predicate in the newly interpreted theory does express identity. Every sentence will have just the same truth-conditions under the new interpretation and the old, but the references of its subsentential parts will be different. Thus, Quine suggests, if one has a language in which one speaks of persons and in which persons of the same income are indistinguishable the predicates of the language may be reinterpreted so that the predicate which previously expressed having the same income comes now to express identity. The universe of discourse now consists of income groups, not people. The extensions of the monadic predicates are classes of income groups, and, in general, the extension of an n-place predicate is a class of n-member sequences of income groups (Quine 1963: 65–79). Any two-place predicate expressing an equivalence relation could be an I-predicate relative to some sufficiently impoverished theory, and Quine's suggestion will be applicable to any such predicate if it is applicable at all.
But it remains that it is not guaranteed that a two-place predicate that is an I-predicate in the theory to which it belongs expresses identity. In fact, no condition can be stated in a first-order language for a predicate to express identity, rather than mere indiscernibility by the resources of the language. However, in a second-order language, in which quantification over all properties (not just those for which the language contains predicates) is possible and Leibniz's Law is therefore statable, identity can be uniquely characterised. Identity is thus not first-order, but only second-order definable.
This situation provides the basis for Geach's radical contention that the notion of absolute identity has no application and that there is only relative identity. This section contains a brief discussion of Geach's complex view. (For more details see the entry on relative identity, Deutsch 1997, Dummett 1981 and 1991, Hawthorne 2003 and Noonan 1997.) Geach maintains that since no criterion can be given by which a predicate expressing an I-predicate may be determined to express, not merely indiscernibility relative to the language to which it belongs, but also absolute indiscernibility, we should jettison the classical notion of identity (1991). He dismisses the possibility of defining identity in a second-order language on the ground of the paradoxical nature of unrestricted quantification over properties and aims his fire particularly at Quine's proposal that an I-predicate in a first-order theory may always be interpreted as expressing absolute identity (even if such an interpretation is not required). Geach objects that Quine's suggestion leads to a “Baroque Meinongian ontology” and is inconsistent with Quine's own expressed preference for “desert landscapes” (“Identity” in Geach 1972: 245).
We may usefully state Geach's thesis using the terminology of absolute and relative equivalence relations. Let us say that an equivalence relation R is absolute if and only if, if x stands in it to y, there cannot be some other equivalence relation S, holding between anything and either x or y, but not holding between x and y. If an equivalence relation is not absolute it is relative. Classical identity is an absolute equivalence relation. Geach's main contention is that any expression for an absolute equivalence relation in any possible language will have the null class as its extension, and so there can be no expression for classical identity in any possible language. This is the thesis he argues against Quine.
Geach also maintains the sortal relativity of identity statements, that “x is the same A as y” does not “split up” into “x is an A and y is an A and x=y”. More precisely stated, what Geach denies is that whenever a term “A” is interpretable as a sortal term in a language L (a term which makes (independent) sense following “the same”) the expression (interpretable as) “x is the same A as y” in language L will be satisfied by a pair <x,y> only if the I-predicate of L is satisfied by <x,y>. Geach's thesis of the sortal relativity of identity thus neither entails nor is entailed by his thesis of the inexpressibility of identity. It is the sortal relativity thesis that is the central issue between Geach and Wiggins (1967 and 1980). It entails that a relation expressible in the form “x is the same A as y” in a language L, where “A” is a sortal term in L, need not entail indiscernibility even by the resources of L.
Geach's argument against Quine exists in two versions, an earlier and a later.
In its earlier version the argument is merely that following Quine's suggestion to interpret a language in which some expression is an I-predicate so that the I-predicate expresses classical identity sins against a highly intuitive methodological programme enunciated by Quine himself, namely that as our knowledge expands we should unhesitatingly expand our ideology, our stock of predicables, but should be much more wary about altering our ontology, the interpretation of our bound name variables (1972: 243).
Geach's argument is that in view of the mere possibility of carving out of a language L, in which the relational expressions, E1, E2, E3… are not I-predicates, sub-languages L1, L2, L3… in which these expressions are I-predicates, if Quine's suggested proposal of reinterpretation is possible for each Ln, the user of L will be committed to any number of entities not quantified over in L, namely, for each Ln, those entities for which the I-predicate of Ln (En) gives a criterion of absolute identity. This will be so because any sentence of L will retain its truth conditions in any Ln to which it belongs, reinterpreted as Quine proposes, but “of course, it is flatly inconsistent to say that as a member of a large theory a sentence retains its truth-conditions but not its ontological commitment” (1973:299).
The crucial premiss of this argument is thus that sameness of truth-conditions entails sameness of ontological commitment. But this is not true. The ontological commitments of a theory (according to Quine, whose notion this is) are those entities that must lie within the domain of quantification of the theory if the theory is to be true; or, the entities the predicates of the theory have to be true of if the theory is to be true. A theory is not ontologically committed, we may say, to whatever has to be in the universe for it to be true, but only to whatever has to be in its universe for it to be true. Thus there is no argument from sameness of truth-conditions to sameness of ontological commitments.
The later version of Geach's argument needs a different response. The difference between the earlier version and the later one is that in the later (to be found in Geach 1973) Geach's claim is not merely that Quine's thesis about possible reinterpretation has a consequence which is unpalatable, but that it leads to an out-and-out logical absurdity, the existence of what he calls “absolute surmen” (entities for which having the same surname constitutes a criterion of absolute identity, ie., entails indiscernibility in all respects). Because Geach is now making this stronger claim, the objection that his argument depends upon the incorrect assumption that sameness of truth-conditions entails sameness of ontological commitment is no longer relevant. In order to make out his case Geach has to establish just two points. First, that there are sentences of English supplemented by the predicate “is the same surman as” (explained to mean “is a man and has the same surname as”), which are evidently true and which, considered as sentences of that fragment of English in which “is the same surman as” is an I-predicate, when this is interpreted in the way Quine suggests, can be true only if absolute surmen exist. And secondly, that the existence of absolute surmen is absurd.
But in the end Geach fails to establish these two points. Quine would say that, for the fragment of English in question, the domain of the variables can be considered to consist of classes of men with the same surname and the predicates interpreted as holding of such classes. Thus, the predicate “is the same surman as” will no longer be true of pairs of men if we adopt Quine's suggestion (I am writing, remember in English, not in the fragment of English under discussion), but rather of pairs of classes of men with the same surname – these then will be Geach's “absolute surmen”. Now, Geach attempts to rule this out by the argument that “whatever is a surman is by definition a man.” But this argument fails. The predicate “is a man” will also be in the language-fragment in which “is the same surman as” is the I-predicate; and so it, too, will, be reinterpreted, if we follow Quine's suggestion, as holding of classes of men with the same surname. Thus the sentence “Whatever is a surman is a man” will be true in the language fragment interpreted in Quine's way, just as it is in English as a whole. What will not be true, however, is that whatever the predicate “is a surman” is true of, as it occurs in the language-fragment reinterpreted in Quine's way, is a thing of which “is a man”, as it occurs in English as a whole, is true of. But Geach has no right to demand that this should be the case. Even so, this demand can be met. For the domain of the interpretation of the language fragment in which “is the same surman as” is the I-predicate can, in fact, be taken to consist of men, namely, to be a class containing exactly one representative man for each class of men with the same surname. Thus, as Geach says, absolute surmen will be just some among men (1973:100). Geach goes on, “there will, for example, be just one surman with the surname “Jones”, but if this is an absolute surman, and he is a certain man, then which of the Jones boys is he?” But this question, which is, of course, only answerable using predicates which belong to the part of English not included in the language fragment in which “is the same surman as” is the I-predicate, is not an impossible one to answer. It is merely that the answer will depend upon the particular interpretation that the language fragment has, in fact, been given. Geach is, therefore not entitled to go on, “Surely we have run into an absurdity.” It thus seems that his argument for the non-existence of absolute identity fails.
Geach's argument for his second thesis, that of the sortal relativity of identity, is that it provides the best solution to a variety of well known puzzles about identity and counting at a time and over time. The most well known puzzle is that of the cat on the mat, which comes in two versions.
The first version goes like this. (Wiggins 1968 contains the first appearance of this version in present-day philosophical literature; an equivalent puzzle is that of Dion and Theon, see Burke 1995). Suppose a cat, Tibbles, is sitting on a mat. Now consider that portion of Tibbles that includes everything except its tail – its “tail complement” – and call it “Tib”. Tib is smaller than Tibbles so they are not identical. But what if we now amputate the cat's tail? (A time-reversed, or “growing”, version can be considered in which a tail is grafted on to a tailless cat; the same responses considered below will be available, but may differ in relative plausibility.) Tibbles and Tib will now coincide. If Tibbles is still a cat, it is hard to see by what criterion one could deny that Tib is a cat. Yet they are distinct individuals, since they have different histories. But there is just one cat on the mat. So they cannot be distinct cats. They must be the same cat, even though they are distinct individuals; and so identity under the sortal concept cat must be a relative identity relation.
The second version (presented in Geach 1980, compare Unger 1980) goes as follows. Tibbles is sitting on the mat and is the only cat sitting on the mat. But Tibbles has at least 1,000 hairs. Geach continues: “Now let c be the largest continuous mass of feline tissue on the mat. Then for any of our 1,000 hairs, say hn, there is a proper part cn of c which contains precisely all of c except that hair hn; and every such part cn differs in a describable way both from any other such part say cm, and from c as a whole. Moreover, fuzzy as the concept cat may be, it is clear that not only is c a cat, but also any part cn is a cat: cn would clearly be a cat were the hair hn to be plucked out, and we cannot reasonably suppose that plucking out a hair generates a cat, so cn must already have been a cat.”
The conclusion, of course, is the same as in the previous version of the argument: there is only one cat on the mat so all the distinct entities that qualify as cats must be the same cat.
This version of the argument can be resisted by insisting that the concept of a cat is maximal, i.e. no proper part of a cat is a cat. The first version may be resisted in a variety of ways. Some deny the existence of the tail-complement at all (van Inwagen 1981, Olson 1995); others deny that the tail-complement survives the amputation (Burke 1995). Another possibility is to say that certain of the historical and/or modal predicates possessed by Tibbles and not Tib are essential to being a cat, so that Tib is not (predicatively) a cat (Wiggins 1980). Again, it can be accepted that both Tib and Tibbles are cats, but deny that in counting them as one we are counting by identity (even cat identity), rather, we are counting by “almost identity” (Lewis 1993). Another possibility is to accept that both Tib and Tibbles are cats, but deny that they are distinct: rather “Tib” and “Tibbles” are two names of the same cat-stage (Hawley 2001, Sider 2001).
There is, then, no very compelling argument for Geach's sortal relativity thesis to be based on such examples, given the variety of responses available, some of which will be returned to below. On the other hand, no alternative solution to the puzzle of the cat on the mat stands out as clearly superior to the rest, or clearly superior to the sortal relativity thesis as a solution. We should conclude that this component of Geach's position, though not proven, is not refuted either; and, possibly that the linguistic data provide no basis for a decision for or against.
4. Criteria of identity
A notion that Geach deploys extensively, and which is also in common use by his opponents, is that of a criterion of identity, a standard by which identity is to be judged. This section will attempt to untangle some of the complexities this notion involves.
The notion of a criterion of identity was introduced into philosophical terminology by Frege (1950) and strongly emphasised by Wittgenstein (1958). Exactly how it is to be interpreted and the extent of its applicability are still matters of debate.
A considerable obstacle to understanding contemporary philosophical usage of the term, however, is that the notion does not seem to be a unitary one. In the case of abstract objects (the case discussed by Frege) the criterion of identity for Fs is thought of as an equivalence relation holding between objects distinct from Fs. Thus the criterion of identity for directions is parallelism of lines, that is, the direction of line a is identical with the direction of line b if and only if line a is parallel to line b. The criterion of identity for numbers is equinumerosity of concepts, that is, the number of Fs is identical with the number of Gs if and only if there are exactly as many Fs as Gs. The relation between the criterion of identity for Fs and the criterion of application for the concept F (the standard for the application of the concept to an individual) is then that to be an F is just to be something for which questions of identity and distinctness are to settled by appeal to the criterion of identity for Fs. (Thus, when Frege went on to give an explicit definition of numbers as extensions of concepts he appealed to it only to deduce what has come to be called Hume's principle – his statement of his criterion of identity for numbers in terms of equinumerosity of concepts, and emphasised that he regarded the appeal to extensions as inessential.) In the case of concrete objects, however, things seem to stand differently. Often the criterion of identity for a concrete object of type F is said to be a relation R such that for any Fs, x and y, x=y if and only if Rxy. In this case the criterion of identity for Fs is not stated as a relation between entities distinct from Fs and the criterion of identity cannot plausibly be thought of as determining the criterion of application. Another example of the lack of uniformity in the case of the notion of a criterion of idenity in contemporary philosophy is, in the case of concrete objects, a distinction customarily made between a criterion of diachronic identity and a criterion of synchronic identity; the former taking the form “x is at t the same F as y is at t′ if and only if…”, where what fills the gap is some statement of a relation holding between objects x and y and times t and t′. (In the case of persons, for example, a candidate criterion of diachronic identity is: x is at t the same person as y is at t′ if and only if x at t is psychologically continuous with y at t′.) A criterion of synchronic identity, by contrast, will typically specify how the parts of an F-thing existing at a time must be related, or how one F at a time is marked off from another.
One way of bringing system into the discussion of criteria of identity is to make use of the distinction between one-level and two-level criteria of identity (Williamson 1990). The Fregean criteria of identity for directions and numbers are two-level. The objects for which the criterion is given are distinct from, and can be pictured as at a higher level than, the objects between which the relation specified holds. On the other hand, the criterion of identity for sets given by the Axiom of Extensionality (sets are the same if they have the same members) and Davidson's criterion of event identity (events are the same if they have the same causes and effects) (“The Individuation of Events” in his 1980) are one-level: the objects for which the criterion of identity is stated are the same as those between which the criterial relation obtains. Not all criteria of identity can be two-level (on pain of infinite regress), and it is tempting to think that the distinction between objects for which a two-level criterion is appropriate and those for which a one-level criterion is appropriate coincides with that between abstract and concrete objects. However, a more general application of the two-level notion is possible. In fact, it can be applied to any type of object K, such that the criterion of identity for Ks can be thought of as an equivalence relation between a distinct type of object, K*s, but some such objects may intuitively be regarded as concrete.
How general this makes its application is a matter of controversy. In particular, if persisting things are thought of as composed of (instantaneous) temporal parts (see discussion below), the problem of supplying a diachronic criterion of identity for concrete objects can be regarded as the problem of providing a two-level criterion. But if persisting things are not thought of in this way then not all persisting things can be provided with two-level criteria. (Though some can. For example, it is quite plausible that the criterion of identity over time for persons should be thought of as given by a relation between bodies.)
Any two-level criterion can be restated in a one-level form (though, of course, not conversely). For example, to say that the direction of line a is identical with the direction of line b if and only if line a is parallel to line b is to say that directions are the same if and only if the lines they are of are parallel, which is the form of a one-level criterion. A way of unifying the various different ways of talking of criteria of identity is thus to take as the paradigmatic form of a statement of a criterion of identity a statement of the form: for any x, for any y, if x is an F and y is an F then x=y if and only if Rxy (Lowe 1989, 1997).
If the notion is interpreted in this way then the relation between the criterion of identity and the criterion of application will be that of one-way determination. The criterion of identity will be determined by, but not determine, the criterion of application.
For, in general, a one-level criterion of identity for Fs will be equivalent to a conjunction of a statement of necessary, and a statement of sufficient, conditions.
A statement of necessary conditions will take the form:
(1) for any x, for any y, if x is an F and y is an F then x=y only if Rxy,
which is equivalent to:
(2) for any x, if x is an F then Rxx.
But (2), of course, says nothing about F-identity; rather it simply specifies a necessary condition of being an F. So, therefore, does (1). Once the criterion of application for the concept of an F is specified (i.e. the necessary and sufficient conditions for its application), there is no need for any further specification of F-identity in a statement of form (1).
What of sufficient conditions of F-identity?
A specification of a sufficient condition, corresponding to a one-level criterion of F-identity, would presumably have to take the form:
(3) for any x, for any y, if x is an F and y is an F then x=y if Rxy.
This is equivalent to:
(4) for any x and y, if Rxy and it is not the case that x=y then (x is not an F or y is not an F).
(4), which denies the existence of distinct R-related Fs, cannot be represented as specifying either a necessary or a sufficient condition of Fness in identity-free terms. But what (4) does do is to specify a necessary condition on a concept being the concept of an F. In this respect it is like the proposition “there is at most one divine being” (i.e., there are no distinct, coexistent, divine beings), which specifies a condition any concept has to satisfy to be that of a divine being.
However, a specification of the necessary and sufficient conditions of divinity, together with the facts, will determine the truth-value of “there is at most one divine being”.
The same is true, mutatis mutandis, of the concept of an F and assertions of form (4). Once the necessary and sufficient conditions of being an F have been laid down, no further stipulation is required to determine which assertions of form (4) are true. In short, specifying the necessary and sufficient conditions of being an F leaves no more room for further specification of the sufficient conditions of F-identity than it does for further specification of necessary conditions of F-identity.
This conclusion is, of course, in agreement with Lewis's view that there are no genuine problems about identity as such (Lewis 1986, Ch. 4), but it is in tension with the thought that sortal concepts, as distinct from adjectival concepts, are to be characterised by their involvement of criteria of identity as well as criteria of application.
A conception of identity criteria which allows this characterisation of the notion of a sortal concept, and which has so far not been mentioned, is that of Dummett (1981). Dummett denies that a criterion of identity must always be regarded as a criterion of identity for a type of object. There is a basic level, he suggests, at which what a criterion of identity is a criterion of, is the truth of a statement in which no objects are referred to. Such a statement can be expressed using demonstratives and pointing gestures, for instance, by saying “This is the same cat as that”, pointing first to a head and then a tail. In such a statement, which he calls a statement of identification, in Dummett's view, there need be no reference to objects made by the use of the demonstratives, any more than reference is made to any object in a feature-placing sentence like “It's hot here”. A statement of identification is merely, as it were, a feature-placing relational statement, like “This is darker than that”. A grasp of a sortal concept F involves both grasp of the truth-conditions of such statements of identification involving “F” and also grasp of the truth-conditions of what Dummett calls “crude predications” involving “F”, statements of the form “this is F”, in which the demonstrative again does not serve to refer to any object. Adjectival terms, which have only a criterion of application and no criterion of identity, are ones which have a use in such crude predications, but no use in statements of identification. Sortal terms, as just noted, have a use in both contexts, and sortal terms may share their criteria of application but differ in their criteria of identity since grasp of the truth-conditions of the crude predication “This is F” does not determine grasp of the truth-conditions of the statement of identification “This is the same F as that” (thus I can know when it is right to say “This is a book” without knowing when it is right to say “This is the same book as that”).
On Dummett's account, then, it may be possible to accept that whenever a criterion of identity for a type of object is to be given it must be (expressible as) a two-level criterion. Essentially one-level criteria (one-level criteria not expressible in a two-level form) are redundant, replaceable by (what we might call) Dummettian zero-level criteria.
Criteria of identity can be employed synchronically, as in the examples just given, to determine whether two coexistent objects are parts of the same object of a kind, or diachronically, to determine identity over time. Identity over time is a controversial notion, however, because time involves change. Heraclitus argued that one could not bathe in the same river twice because new waters were ever flowing in. Hume argued that identity over time was a fiction we substitute for a collection of related objects. Such views can be seen as based on a misunderstanding of Leibniz's Law: if a thing changes something is true of it at the later time that is not true of it at the earlier, so it is not the same. The answer is that what is true of it at the later time is, say, “being muddy at the later time”, which was always true of it; similarly, what is true of it at the earlier time, suitably expressed, remains true of it. But the question remains how to characterise identity through time and across change given that there is such a thing.
One topic which has always loomed large in this debate has been the issue (in the terminology of Lewis 1986, Ch. 4) of perdurance versus endurance. (Others, for which there is no space for discussion here, include the debate over Ship of Theseus and reduplication or fission problems and associated issues about “best candidate” or “no rival candidate” accounts of identity over time, and the debate over Humean supervenience – see articles on relative identity, personal identity, Hawley 2001 and Sider 2001.)
According to one view, material objects persist by having temporal parts or stages, which exist at different times and are to be distinguished by the times at which they exist – this is known as the view that material objects perdure. Other philosophers deny that this is so; according to them, when a material object exists at different times, it is wholly present at those times, for it has no temporal parts, but only spatial parts, which likewise are wholly present at the different times they exist. This is known as the view that material objects endure.
Perdurance theorists, as Quine puts it, reject the point of view inherent in the tenses of our natural language. From that point of view persisting things endure and change through time, but do not extend through time, but only through space. Thus persisting things are to be sharply distinguished from events or processes, which precisely do extend through time. One way of describing the position of the perdurance theorist, then, is to say that he denies the existence of a distinct ontological category of persisting things, or substances. Thus, Quine writes, “physical objects, conceived thus four-dimensionally in space-time, are not to be distinguished from events, or, in the concrete sense of the term, processes. Each comprises simply the content, however heterogeneous, of some portion of space-time, however disconnected or gerrymandered” (1960:171).
In recent controversy two arguments have been at the centre of the endurance/perdurance debate, one employed by perdurance theorists and the other by endurance theorists (for other arguments and issues see the separate article on temporal parts, Hawley 2001 and Sider 2001).
An argument for perdurance which has been hotly debated is due to David Lewis (1986). If perdurance is rejected, the ascription of dated or tensed properties to objects must be regarded as assertions of irreducible relations between objects and times. If Tabby is fat on Monday, that is a relation between Tabby and Monday, and if perdurance is rejected it is an irreducible relation between Tabby and Monday. According to perdurance theory, however, while it is still, of course, a relation between Tabby and Monday it is not irrreducible; it holds between Tabby and Monday because the temporal part of Tabby on Monday, Tabby-on-Monday, is intrinsically fat. If perdurance is rejected, however, no such intrinsic possessor of the property of fatness can be recognised: Tabby's fatness on Monday must be regarded as an irreducible state of affairs.
According to Lewis, this consequence of the rejection of the perdurance theory is incredible. Whether he is right about this is the subject of intense debate (Haslanger 2003).
Even if Lewis is right, however, the perdurance theory may still be found wanting, since it does not secure the most commonsensical position: that fatness is a property of a cat (Haslanger 2003). According to perdurance theory, rather, it is a property of a (temporal) cat part. Those known as stage theorists (Hawley 2001, Sider 2001), accepting the ontology of perdurance theory, but modifying its semantics, offer a way to secure this desirable result. Every temporal part of a cat is a cat, they say, so Tabby-on-Monday (which is what we refer to by “Tabby”, on Monday) is a cat and is fat, just as we would like. Stage theorists have to pay a price for this advantage over perdurance theory, however. For they must accept either that our reports of the cross-temporal number of cats are not always reports of the counting of cats (as when I say, truly, that I have only ever owned three cats) or that two cat-stages (cats) may be counted as one and the same cat, so that counting cats is not always counting in accordance with absolute identity.
An argument against the perdurance theory that has been the focus of interest is one presented in various guises by a number of writers, including Wiggins (1980), Thomson (1983) and van Inwagen (1990). Applied to persons (it can equally well be applied to other persisting things), it asserts that persons have different properties, in particular, different modal properties, from the summations of person-stages with which the perdurance theory identifies them. Thus, by Leibniz's Law, this identification must be mistaken. As David Wiggins states the argument: “Anything that is part of a Lesniewskian sum [a mereological whole defined by its parts] is necessarily part of it…But no person or normal material object is necessarily in the total state that will correspond to the person- or object-moment postulated by the theory under discussion” (1980: 168).
To elaborate a little. I might have died when I was five years old. But that maximal summation of person-stages which, according to perdurance theory, is me and has a temporal extent of at least fifty years, could not have had a temporal extent of a mere five years. So I cannot be such a summation of stages.
This argument illustrates the interdependence of the various topics discussed under the rubric of identity. Whether it is valid, of course, depends on the correct analysis of modal predication, and, in particular, on whether it should be analysed in terms of “identity across possible worlds” or in terms of Lewisean counterpart theory. This is the topic to which we next turn.
In the interpretation of modal discourse recourse is often made to the idea of “identity across possible worlds”. If modal discourse is interpreted in this way it becomes natural to regard a statement ascribing a modal property to an individual as asserting the identity of that individual across worlds: “John might have been a millionaire”, on this view, asserts that there is a possible world in which an individual identical with John is a millionaire. “John could not have been a millionaire” asserts that in any world in which an individual identical with John exists that individual is not a millionaire.
However, though this is perhaps the most natural way to interpret de re modal statements (once it has been accepted that the apparatus of possible worlds is to be used as an interpretative tool), there are well-known difficulties that make the approach problematic.
For example, it seems reasonable to suppose that a complex artefact like a bicycle could have been made of different parts. On the other hand, it does not seem right that the same bicycle could have been constructed out of completely different parts.
But now consider a series of possible worlds, beginning with the actual world, each containing a bicycle just slightly different from the one in the previous world, the last world in the sequence being one in which there is a bicycle composed of completely different parts from the one in the actual world. One cannot say that each bicycle is identical with the one in the neighbouring world, but not identical with the corresponding bicycle in distant worlds, since identity is transitive. Hence it seems one must either adopt an extreme mereological essentialism, according to which no difference of parts is possible for an individual, or reject the interpretation of de re modal discourse as asserting identity across possible worlds.
This and other problems with cross-world identity suggest that some other weaker relation, of similarity or what David Lewis calls counterparthood, should be employed in a possible world analysis of modal discourse. Since similarity is not transitive this allows us to say that the bicycle might have had some different parts without having to say that it might have been wholly different. On the other hand, such a substitution does not seem unproblematic, for a claim about what I might have done hardly seems, at first sight, to be correctly interpretable as a claim about what someone else (however similar to me) does in another possible world (Kripke 1980, note 13).
An assessment of the counterpart theoretic analysis is vital not just to understanding modal discourse, however, but also to getting to the correct account of identity over time. For, as we have just seen, the argument against perdurance theory outlined at the end of the last section depends on the correct interpretation of modal discourse. In fact, it is invalid on a counterpart theoretic analysis which allows different counterpart relations (different similarity relations) to be invoked according to the sense of the singular term which is the subject of the de re modal predication (Lewis 1986, Ch. 4), since the counterpart relation relevant to the assessment of a de re modal predication with a singular term whose sense determines that it refers to a person will be different from that relevant to the assessment of a de re modal predication with a singular term whose sense determines that it refers to a sum of person-stages. “I might have existed for only five years” means on the Lewisean account “There is a person in some possible world similar to me in those respects important to personhood who exists for only five years”; “The maximal summation of person stages of which this current stage is a stage might have existed for only five years” means “There is a summation of person stages similar to this one in those respects important to the status of an entity as a summation of stages which exists for only five years”. Since the two similarity relations in question are distinct the first modal statement may be true and the second false even if I am identical with the sum of stages in question.
Counterpart theory is also significant to the topic of identity over time in another way, since it provides the analogy to which the stage theorist (who regards all everyday reference as reference to momentary stages rather than to perdurers) appeals to explain de re temporal predication. Thus, according to the stage theorist, just as “I might have been fat” does not require the existence of a possible world in which an object identical with me is fat, but only the existence of a world in which a (modal) counterpart of me is fat, so “I used to be fat” does not require the existence of a past time at which someone identical with (the present momentary stage which is) me was fat, but only the existence of a past time at which a (temporal) counterpart of me was fat. The problem of identity over time for things of a kind, for stage theorists, is just the problem of characterizing the appropriate temporal counterpart relation for things of that kind.
For a more detailed discussion of the topic, see the entry transworld identity. Whether de re modal discourse is to be interpreted in terms of identity across possible worlds or counterpart theoretically (or in some other way entirely) is also relevant to our next topic, that of contingent identity.
Before Kripke's writings (1980), it seemed a platitude that statements of identity could be contingent – when they contained two terms differing in sense but identical in reference and so were not analytic. Kripke challenged this platitude, though, of course, he did not reject the possibility of contingent statements of identity. But he argued that when the terms flanking the sign of identity were what he called rigid designators, an identity statement, if true at all, had to be necessarily true, but need not be knowable a priori, as an analytic truth would be. Connectedly, Kripke argued that identity and distinctness were themselves necessary relations: if an object is identical with itself it is necessarily so, and if it is distinct from another it is necessarily so.
Kripke's arguments were very persuasive, but there are examples that suggest that his conclusion is too sweeping – that even identity statements containing rigid designators can be, in a sense, contingent. The debate over contingent identity is concerned with the assessment and proper analysis of these examples.
One of the earliest examples is provided by Gibbard (1975). Consider a statue, Goliath, and the clay, Lumpl, from which it is composed. Imagine that Lumpl and Goliath coincide in their spatiotemporal extent. It is tempting to conclude that they are identical. But they might not have been. Goliath might have been rolled into a ball and destroyed; Lumpl would have continued to exist. The two would have been distinct. Thus it seems that the identity of Lumpl and Goliath, if admitted, must be acknowledged as merely contingent.
One reaction to this argument available to the convinced Kripkean is simply to deny that Lumpl and Goliath are identical. But to accept this is to accept that purely material entities, like statues and lumps of clay, of admittedly identical material constitution at all times, may nonetheless be distinct, though distinguished only by modal, dispositional or counterfactual properties. To many, however, this seems highly implausible, which provides the strength of the argument for contingent identity.
David Lewis (in “Counterparts of Persons and their Bodies” in his 1983) suggests that the identity of a person with his body (assuming the person and the body, like Goliath and Lumpl, are at all times coincident) is contingent, since bodily interchange is a possibility. He appeals to counterpart theory, modified to allow a variety of counterpart relations, to explain this. Contingent identity then makes sense, since “I and my body might not have been identical” now translates into counterpart theory as “There is a possible world, w, a unique personal counterpart x in w of me and a unique bodily counterpart y in w of my body, such that x and y are not identical”.
What is crucial to making sense of contingent identity is an acceptance that modal predicates are inconstant in denotation (that is, stand for different properties when attached to different singular terms or different quantifying expressions). Counterpart theory provides one way of explaining this inconstancy, but is not necessarily the only way (Gibbard 1975, Noonan 1991, 1993). However, whether the examples of contingent identity in the literature are persuasive enough to make it reasonable to accept the certainly initially surprising idea that modal predications are inconstant in denotation is still a matter of considerable controversy.
Finally, in this section, it is worth noting explicitly the interdependence of the topics under discussion: only if the possibility of contingent identity is secured, by counterpart theory or some other account of de re modality which does not straightforwardly analyse de re modal predication in terms of identity across possible worlds, can perdurance theory (or stage theory) as an account of identity across time be sustained against the modal arguments of Wiggins, Thomson and van Inwagen.
A thesis that has a long pedigree but has only recently been gathering attention in the contemporary literature is the “Composition as Identity” thesis. The thesis comes in a weak and a strong form. In its weak form the thesis is that the mereological composition relation is analogous in a number of important ways to the identity relation and so deserves to be called a kind of identity. In its strong form the thesis is that the composition relation is strictly identical with the identity relation, viz. that the parts of a whole are literally (collectively) identical with the whole itself. The strong thesis was considered by Plato in Parmenides and versions of the thesis have been discussed by many historical figures since (Harte 2002, Normore and Brown 2014). The progenitor of the modern version of the thesis is Baxter (1988a, 1988b, 2001) but it is most often discussed under the formulation of it given by Lewis (1991), who first considers the strong thesis before rejecting it in favour of the weak thesis.
Both the strong and the weak versions of the thesis are motivated by the fact that there is an especially intimate relation between a whole and its parts (a whole is “nothing over and above” its parts), buttressed by claims that identity and composition are alike in various ways. Lewis (1991: 85) makes five likeness claims:
Ontological Innocence. If one believes that some object x exists, one does not gain a commitment to a further object by believing that something identical with x exists. Likewise, if one believes that some objects x1, x2, …, xn exist, one does not gain a commitment to a further object by claiming that something composed of x1, x2, …, xn exists.
Automatic Existence. If some object x exists, then it automatically follows that something identical with x exists. Likewise, if some objects x1, x2, …, xn exist, then it automatically follows that something composed of x1, x2, …, xn exists.
Unique Composition. If something y is identical with x, then anything identical with x is identical with y, and anything identical with y is identical with x. Likewise, if some things y1, y2, …, yn compose x, then any things that compose x are identical with y1, y2, …, yn, and anything identical with x is composed of y1, y2, …, yn.
Exhaustive Description. If y is identical with x, then an exhaustive description of y is an exhaustive description of x, and vice versa. Likewise, if y1, y2, …, yn compose x, then an exhaustive description of y1, y2, …, yn is an exhaustive description of x, and vice versa.
Same Location. If y is identical with x, then necessarily, x and y fill the same region of spacetime. Likewise, if y1, y2, …, yn compose x, then necessarily, y1, y2, …, yn and x fill the same region of spacetime.
Clearly not all will agree with each of Lewis's likeness claims. Anyone who denies unrestricted mereological composition, for example, will deny 2. And the defender of strong pluralism in the material constitution debate (i.e. one who defends the view that there can be all-time coincident entities) will deny 3. And some endurantists who think that ordinary material objects can have distinct parts at distinct times will deny 5. But there is a more general problem with 1, as van Inwagen has made clear (1994: 213). Consider a world w1 that contains just two simples s1 and s2. Now consider the difference between someone p1 who believes that s1 and s2 compose something and someone p2 who does not. Ask: how many objects do p1 and p2 believe there to be in w1? The answer, it seems, is that p1 believes that there are three things and p2 only two. So how can a commitment to the existence of fusions be ontologically innocent? One recent suggestion is that although a commitment to the existence of fusions is not ontologically innocent, it almost is: to commit oneself to fusions is to commit oneself to further entities, but because they are not fundamental entities they are not ones that matter for the purpose of theory choice (Cameron 2014, Schaffer 2008, Williams 2010, and see also Hawley 2014).
If one believes Lewis's likeness claims one will be tempted by at least the weak Composition as Identity thesis. If composition is a type of identity this gives some kind of explanation of why the parallels between the two hold. But the strong thesis, that the composition relation is the identity relation, gives a fuller explanation. So why not hold the strong thesis? Because, many think, there are additional challenges that face anyone who wishes to defend the strong thesis.
The classical identity relation is one that can only have single objects as relata (as in: “George Orwell = Eric Blair”). If we adopt a language that allows the formation of plural terms we can unproblematically define a plural identity relation that holds between pluralities of objects too. Plural identity statements such as “the hunters are identical with the gatherers” are understood to mean that for all x, x is one of the hunters iff x is one of the gatherers. But, according to the strong Composition as Identity thesis, there can also be true hybrid identity statements that relate pluralities and single objects. That is, sentences such as “the bricks = the wall” are taken by the defender of strong Composition as Identity to be well-formed sentences that express strict identities.
The first challenge facing the defender of the strong thesis is the least troublesome. It is the syntactic problem that hybrid identity statements are ungrammatical in English (Van Inwagen, 1994: 211). Whilst “George Orwell is identical with Eric Blair” and “the hunters are identical with the gatherers” are well-formed, it seems that “the bricks are identical with the wall” is not. However, there is in fact some doubt about whether hybrid identity statements are ungrammatical in English, and some have pointed out that this is anyway a mere grammatical artefact of English that is not present in other languages (e.g. Norwegian and Hungarian). So it seems that the most this challenge calls for is a mild form of grammatical revisionism. And we have, at any rate, formal languages that allow hybrid constructions to be made in which to express the claims made by the defender of the strong Composition as Identity thesis. (Sider 2007, Cotnoir 2013) (NB The claims regarding Norwegian and Hungarian are to be found in these two papers.)
The second challenge is more troublesome. It is the semantic problem of providing coherent truth-conditions for hybrid identity statements. The standard way to provide the truth-conditions for the classical identity relation is to say that an identity statement of the form “a=b” is true iff “a” and “b” have the same referents. But this account clearly does not work for hybrid identity statements, for there is no (single) referent for a plural term. Moreover, the standard way of giving the truth-conditions for plural identity statements (mentioned above) does not work for hybrid identity statements either. To say that “x is one of the ys” is to say that x is (classically) identical with one of the things in the plurality, i.e., that x is identical with y1, or identical with y2… or identical with yn. But then “the bricks = the wall” is true only if the wall is (classically) identical with one of the bricks, i.e. with b1, or with b2… or with bn, which it isn't.
The third challenge is the most troublesome of all. In section 2 it was noted that Leibniz's Law (and its contrapositive) appear to be crucial to our understanding of identity and distinctness. But it seems that the defender of strong Composition as Identity must deny this. After all, the bricks are many, but the wall is one. The onus is thus on the defender of strong Composition as Identity to explain why we should think the “are” in hybrid identity statements really expresses the relation of identity.
The second and the third challenges have been thought by many to be insurmountable (Lewis, for example, rejects strong Composition as Identity on the basis of them). But, in recent semantic work in this area, accounts have emerged that promise to answer both challenges. (Wallace 2011a, 2011b, Cotnoir 2013). Whether they do so, however, remains to be seen.
9. Vague identity
Like the impossibility of contingent identity, the impossibility of vague identity appears to be a straightforward consequence of the classical concept of identity (Evans 1978, see also Salmon 1982). For if a is only vaguely identical with b, something is true of it – that it is only vaguely identical with b – that is not true of b, so, by Leibniz's Law, it is not identical with b at all. Of course, there are vague statements of identity – “Princeton is Princeton Borough” (Lewis 1988) – but the conclusion appears to follow that such vagueness is only possible when one or both of the terms flanking the sign of identity is an imprecise designator. Relatedly, it appears to follow that identity itself must be a determinate relation.
But some examples suggest that this conclusion is too sweeping – that even identity statements containing precise designators may be, in some sense, indeterminate. Consider Everest and some precisely defined hunk of rock, ice and snow, Rock, of which it is indeterminate whether its boundaries coincide with those of Everest. It is tempting to think that “Everest” and “Rock” are both precise designators (if “Everest” is not, is anything? (Tye 2000)) and that “Everest is Rock” is nonetheless in some sense indeterminate.
Those who take this view have to respond to Evans's original argument, about which there has been intense debate (see separate article on vagueness, Edgington 2000, Lewis 1988, Parsons 2000, van Inwagen 1990, Williamson 2002 and 2003), but also to more recent variants. There is no space to go into these matters here, but one particular variant of the Evans argument worth briefly noting is given by Hawley (2001). Alpha and Omega are (two?) people, the first of whom steps into van Inwagen's (1990) fiendish cabinet which disrupts whatever features are relevant to personal identity, and the second of whom then steps out:
(1) It is indeterminate whether Alpha steps out of the cabinet
(2) Alpha is such that it is indeterminate whether she steps out of the cabinet
(3) It is not indeterminate whether Omega steps out of the cabinet
(4) Omega is not such that it is indeterminate whether she steps out of the cabinet
(5) Alpha is not identical to Omega.
This argument differs from the standard version of Evans's argument by not depending upon identity-involving properties (e.g. being such that it is indeterminate whether she is Omega) to establish distinctness, and this removes some sources of controversy. Others, of course, remain.
The debate over vague identity is too vast to survey here, but to finish we can relate this debate to the previously discussed debate about identity over time.
For some putative cases of vagueness in synchronic identity it seems reasonable to accept the conclusion of Evans's argument and locate the indeterminacy in language (see the “Reply” by Shoemaker in Shoemaker and Swinburne 1984 for the following example). A structure consists of two halls, Alpha Hall and Beta Hall, linked by a flimsy walkway, Smith is located in Alpha Hall, Jones in Beta Hall. The nature of the structure is such that the identity statement “The building in which Smith is located is the building in which Jones is located” is neither true nor false because it is indeterminate whether Alpha Hall and Beta Hall count as two distinct buildings or merely as two parts of one and the same building. Here it is absolutely clear what is going on. The term “building” is vague in a way that makes it indeterminate whether it applies to the whole structure or just to the two halls. Consequently, it is indeterminate what “the building in which Smith is located” and “the building in which Jones is located” denote.
Perdurance theorists, who assimilate identity over time to identity over space, can accommodate vagueness in identity over time in the same way. In Hawley's example they can say that there are several entities present: one that exists before and after the identity-obscuring occurrences in the cabinet, one that exists only before, and one that exists only after. It is indeterminate which of these is a person and so it is indeterminate what the singular terms “Alpha” and “Omega” refer to.
This involves taking on an ontology that is larger than we ordinarily recognise, but that is not uncongenial to the perdurance theorist, who is happy to regard any, however spatiotemporally disconnected, region as containing a physical object (Quine 1960:171).
But what of endurance theorists?
One option for them is to adopt the same response and to accept a multiplicity of entities partially coinciding in space and time where to common sense there seems to be only one. But this is to give up on one of the major advantages claimed by the endurance theorist, his consonance with common sense.
The endurance theorist has several other options. He may simply deny the existence of the relevant entities and restrict his ontology to entities which are not complex; he may insist that any change destroys identity so that in a strict and philosophical sense Alpha is distinct from Omega; or he may reject the case as one of vagueness, insisting that, though we do not know the answer, either Alpha is Omega or she is not.
However, the most tempting option for the endurance theorist, which keeps closest to common sense, is to accept that the case is one of vagueness, deny the multiplicity of entities embraced by the perdurance theorist and reject Evans's argument against vague identity.
That this is so highlights the fact that there is no easy solution to the problem consonant in every respect with common sense. Locating the vagueness in language requires us to acknowledge a multiplicity of entities of which we would apparently otherwise have to take no notice. Whilst locating it in the world requires an explanation of how, contrary to Evans's argument, the impossibility of vague identity is not a straightforward consequence of the classical conception of identity, or else the abandonment of that conception.
1. History of the Correspondence Theory
The correspondence theory is often traced back to Aristotle’s well-known definition of truth (Metaphysics 1011b25): “To say of what is that it is not, or of what is not that it is, is false, while to say of what is that it is, and of what is not that it is not, is true”—but virtually identical formulations can be found in Plato (Cratylus 385b2, Sophist 263b). It is noteworthy that this definition does not highlight the basic correspondence intuition. Although it does allude to a relation (saying something of something) to reality (what is), the relation is not made very explicit, and there is no specification of what on the part of reality is responsible for the truth of a saying. As such, the definition offers a muted, relatively minimal version of a correspondence theory. (For this reason it has also been claimed as a precursor of deflationary theories of truth.) Aristotle sounds much more like a genuine correspondence theorist in the Categories (12b11, 14b14), where he talks of underlying things that make statements true and implies that these things (pragmata) are logically structured situations or facts (viz., his sitting and his not sitting are said to underlie the statements “He is sitting” and “He is not sitting”, respectively). Most influential is Aristotle’s claim in De Interpretatione (16a3) that thoughts are “likenessess” (homoiomata) of things. Although he nowhere defines truth in terms of a thought’s likeness to a thing or fact, it is clear that such a definition would fit well into his overall philosophy of mind. (Cf. Crivelli 2004; Szaif 2006.)
1.1 Metaphysical and Semantic Versions
In medieval authors we find a division between “metaphysical” and “semantic” versions of the correspondence theory. The former are indebted to the truth-as-likeness theme suggested by Aristotle’s overall views, the latter are modeled on Aristotle’s more austere definition from Metaphysics 1011b25.
The metaphysical version presented by Thomas Aquinas is the best known: “Veritas est adaequatio rei et intellectus” (Truth is the equation of thing and intellect), which he restates as: “A judgment is said to be true when it conforms to the external reality”. He tends to use “conformitas” and “adaequatio”, but also uses “correspondentia”, giving the latter a more generic sense (De Veritate, Q.1, A.1-3; cf. Summa Theologiae, Q.16). Aquinas credits the Neoplatonist Isaac Israeli with this definition, but there is no such definition in Isaac. Correspondence formulations can be traced back to the Academic skeptic Carneades, 2nd century B.C., whom Sextus Empiricus (Adversos Mathematicos, vii, 168) reports as having taught that a presentation “is true when it is in accord (symphonos) with the object presented, and false when it is in discord with it”. Similar accounts can be found in various early commentators on Plato and Aristotle (cf. Künne 2003, chap. 3.1), including some Neoplatonists: Proklos (In Tim., II 287, 1) speaks of truth as the agreement or adjustment (epharmoge) between knower and the known. Philoponus (In Cat., 81, 25-34) emphasizes that truth is neither in the things or states of affairs (pragmata) themselves, nor in the statement itself, but lies in the agreement between the two. He gives the simile of the fitting shoe, the fit consisting in a relation between shoe and foot, not to be found in either one by itself. Note that his emphasis on the relation as opposed to its relata is laudable but potentially misleading, because x’s truth (its being true) is not to be identified with a relation, R, between x and y, but with a general relational property of x, taking the form (∃y)(xRy & Fy). Further early correspondence formulations can be found in Avicenna (Metaphysica, 1.8-9) and Averroes (Tahafut, 103, 302). They were introduced to the scholastics by William of Auxerre, who may have been the intended recipient of Aquinas’ mistaken attribution (cf. Boehner 1958; Wolenski 1994).
Aquinas’ balanced formula “equation of thing and intellect” is intended to leave room for the idea that “true” can be applied not only to thoughts and judgments but also to things or persons (e.g. a true friend). Aquinas explains that a thought is said to be true because it conforms to reality, whereas a thing or person is said to be true because it conforms to a thought (a friend is true insofar as, and because, she conforms to our, or God’s, conception of what a friend ought to be). Medieval theologians regarded both, judgment-truth as well as thing/person-truth, as somehow flowing from, or grounded in, the deepest truth which, according to the Bible, is God: “I am the way and the truth and the life” (John 14, 6). Their attempts to integrate this Biblical passage with more ordinary thinking involving truth gave rise to deep metaphysico-theological reflections. The notion of thing/person-truth, which thus played a very important role in medieval thinking, is disregarded by modern and contemporary analytic philosophers but survives to some extent in existentialist and continental philosophy.
Medieval authors who prefer a semantic version of the correspondence theory often use a peculiarly truncated formula to render Aristotle’s definition: A (mental) sentence is true if and only if, as it signifies, so it is (sicut significat, ita est). This emphasizes the semantic relation of signification while remaining maximally elusive about what the “it” is that is signified by a true sentence and de-emphasizing the correspondence relation (putting it into the little words “as” and “so”). Foreshadowing a favorite approach of the 20th century, medieval semanticists like Ockham (Summa Logicae, II) and Buridan (Sophismata, II) give exhaustive lists of different truth-conditional clauses for sentences of different grammatical categories. They refrain from associating true sentences in general with items from a single ontological category. (Cf. Moody 1953; Adams McCord 1987; Perler 2006.)
Authors of the modern period generally convey the impression that the correspondence theory of truth is far too obvious to merit much, or any, discussion. Brief statements of some version or other can be found in almost all major writers; see e.g.: Descartes 1639, ATII 597; Spinoza, Ethics, axiom vi; Locke, Essay, 4.5.1; Leibniz, New Essays, 4.5.2; Hume, Treatise, 3.1.1; and Kant 1787, B82. Berkeley, who does not seem to offer any account of truth, is a potentially significant exception. Due to the influence of Thomism, metaphysical versions of the theory are much more popular with the moderns than semantic versions. But since the moderns generally subscribe to a representational theory of the mind (the theory of ideas), they would seem to be ultimately committed to spelling out relations like correspondence or conformity in terms of a psycho-semantic representation relation holding between ideas, or sentential sequences of ideas (Locke’s “mental propositions”), and appropriate portions of reality, thereby effecting a merger between metaphysical and semantic versions of the correspondence theory.
1.2 Object-Based and Fact-Based Versions
It is helpful to distinguish between “object-based” and “fact-based” versions of correspondence theories, depending on whether the corresponding portion of reality is said to be an object or a fact (cf. Künne 2003, chap. 3).
Traditional versions of object-based theories assumed that the truth-bearing items (usually taken to be judgments) have subject-predicate structure. An object-based definition of truth might look like this:
A judgment is true if and only if its predicate corresponds to its object (i.e., to the object referred to by the subject term of the judgment).
Note that this actually involves two relations to an object: (i) a reference relation, holding between the subject term of the judgment and the object the judgment is about (its object); and (ii) a correspondence relation, holding between the predicate term of the judgment and a property of the object. Owing to its reliance on the subject-predicate structure of truth-bearing items, the account suffers from an inherent limitation: it does not cover truthbearers that lack subject-predicate structure (e.g. conditionals, disjunctions), and it is not clear how the account might be extended to cover them. The problem is obvious and serious; it was nevertheless simply ignored in most writings. Object-based correspondence was the norm until relatively recently.
Object-based correspondence became the norm through Plato’s pivotal engagement with the problem of falsehood, which was apparently notorious at its time. In a number of dialogues, Plato comes up against an argument, advanced by various Sophists, to the effect that false judgment is impossible—roughly: To judge falsely is to judge what is not. But one cannot judge what is not, for it is not there to be judged. To judge something that is not is to judge nothing, hence, not to judge at all. Therefore, false judgment is impossible. (Cf. Euthydemus 283e-288a; Cratylus 429c-e; Republic 478a-c; Theaetetus 188d-190e.) Plato has no good answer to this patent absurdity until the Sophist (236d-264b), where he finally confronts the issue at length. The key step in his solution is the analysis of truthbearers as structured complexes. A simple sentence, such as “Theaetetus sits.”, though simple as a sentence, is still a complex whole consisting of words of different kinds—a name (onoma) and a verb (rhema)—having different functions. By weaving together verbs with names the speaker does not just name a number of things, but accomplishes something: meaningful speech (logos) expressive of the interweaving of ideas (eidon symploken). The simple sentence is true when Theaetetus, the person named by the name, is in the state of sitting, ascribed to him through the verb, and false, when Theaetetus is not in that state but in another one (cf. 261c-263d; see Denyer 1991; Szaif 1998). Only things that are show up in this account: in the case of falsehood, the ascribed state still is, but it is a state different from the one Theaetetus is in. The account is extended from speech to thought and belief via Plato’s well known thesis that “thought is speech that occurs without voice, inside the soul in conversation with itself” (263e)—the historical origin of the language-of-thought hypothesis. The account does not take into consideration sentences that contain a name of something that is not (“Pegasus flies”), thus bequeathing to posterity a residual problem that would become more notorious than the problem of falsehood.
Aristotle, in De Interpretatione, adopts Plato’s account without much ado—indeed, the beginning of De Interpretatione reads like a direct continuation of the passages from the Sophist mentioned above. He emphasizes that truth and falsehood have to do with combination and separation (cf. De Int. 16a10; in De Anima 430a25, he says: “where the alternative of true and false applies, there we always find a sort of combining of objects of thought in a quasi-unity”). Unlike Plato, Aristotle feels the need to characterize simple affirmative and negative statements (predications) separately—translating rather more literally than is usual: “An affirmation is a predication of something toward something, a negation is a predication of something away from something” (De Int. 17a25). This characterization reappears early in the Prior Analytics (24a). It thus seems fair to say that the subject-predicate analysis of simple declarative sentences—the most basic feature of Aristotelian term logic which was to reign supreme for many centuries—had its origin in Plato’s response to a sophistical argument against the possibility of falsehood. One may note that Aristotle’s famous definition of truth (see Section 1) actually begins with the definition of falsehood.
Fact-based correspondence theories became prominent only in the 20th century, though one can find remarks in Aristotle that fit this approach (see Section 1)—somewhat surprisingly in light of his repeated emphasis on subject-predicate structure wherever truth and falsehood are concerned. Fact-based theories do not presuppose that the truth-bearing items have subject-predicate structure; indeed, they can be stated without any explicit reference to the structure of truth-bearing items. The approach thus embodies an alternative response to the problem of falsehood, a response that may claim to extricate the theory of truth from the limitations imposed on it through the presupposition of subject-predicate structure inherited from the response to the problem of falsehood favored by Plato, Aristotle, and the medieval and modern tradition.
The now classical formulation of a fact-based correspondence theory was foreshadowed by Hume (Treatise, 3.1.1) and Mill (Logic, 1.5.1). It appears in its canonical form early in the 20th century in Moore (1910-11, chap. 15) and Russell: “Thus a belief is true when there is a corresponding fact, and is false when there is no corresponding fact” (1912, p. 129; cf. also his 1905, 1906, 1910, and 1913). The self-conscious emphasis on facts as the corresponding portions of reality—and a more serious concern with problems raised by falsehood—distinguishes this version from its foreshadowings. Russell and Moore’s forceful advocacy of truth as correspondence to a fact was, at the time, an integral part of their defense of metaphysical realism. Somewhat ironically, their formulations are indebted to their idealist opponents, F. H. Bradley (1883, chaps. 1&2), and H. H. Joachim (1906), the latter was an early advocate of the competing coherence theory, who had set up a correspondence-to-fact account of truth as the main target of his attack on realism. Later, Wittgenstein (1921) and Russell (1918) developed “logical atomism”, which introduces an important modification of the fact-based correspondence approach (see below, Section 7.1). Further modifications of the correspondence theory, bringing a return to more overtly semantic and broadly object-based versions, were influenced by Tarski’s (1935) technical work on truth (cf. Field 1972, Popper 1972).
2. Truthbearers, Truthmakers, Truth
2.1 Truthbearers
Correspondence theories of truth have been given for beliefs, thoughts, ideas, judgments, statements, assertions, utterances, sentences, and propositions. It has become customary to talk of truthbearers whenever one wants to stay neutral between these choices. Five points should be kept in mind:
The term “truthbearer” is somewhat misleading. It is intended to refer to bearers of truth or falsehood (truth-value-bearers), or alternatively, to things of which it makes sense to ask whether they are true or false, thus allowing for the possibility that some of them might be neither.
One distinguishes between secondary and primary truthbearers. Secondary truthbearers are those whose truth-values (truth or falsehood) are derived from the truth-values of primary truthbearers, whose truth-values are not derived from any other truthbearers. Consequently, the term “true” is usually regarded as ambiguous, taking its primary meaning when applied to primary truthbearers and various secondary meanings when applied to other truthbearers. This is, however, not a brute ambiguity, since the secondary meanings are supposed to be derived, i.e. definable from, the primary meaning together with additional relations. For example, one might hold that propositions are true or false in the primary sense, whereas sentences are true or false in a secondary sense, insofar as they express propositions that are true or false (in the primary sense). The meanings of “true”, when applied to truthbearers of different kinds, are thus connected in a manner familiar from what Aristotelians called “analogical” uses of a term—nowadays one would call this “focal meaning”; e.g., “healthy” in “healthy organism” and “healthy food”, the latter being defined as healthy in the secondary sense of contributing to the healthiness (primary sense) of an organism.
It is often unproblematic to advocate one theory of truth for bearers of one kind and another theory for bearers of a different kind (e.g., a deflationary theory of truth, or an identity theory, applied to propositions, could be a component of some form of correspondence theory of truth for sentences). Different theories of truth applied to bearers of different kinds do not automatically compete. The standard segregation of truth theories into competing camps (found in textbooks, handbooks, and dictionaries) proceeds under the assumption—really a pretense—that they are intended for primary truthbearers of the same kind.
Confusingly, there is little agreement as to which entities are properly taken to be primary truthbearers. Nowadays, the main contenders are public language sentences, sentences of the language of thought (sentential mental representations), and propositions. Popular earlier contenders—beliefs, judgments, statements, and assertions—have fallen out of favor, mainly for two reasons:
The problem of logically complex truthbearers. A subject, S, may hold a disjunctive belief (the baby will be a boy or the baby will be a girl), while believing only one, or neither, of the disjuncts. Also, S may hold a conditional belief (if whales are fish, then some fish are mammals) without believing the antecedent or the consequent. Also, S will usually hold a negative belief (not everyone is lucky) without believing what is negated. In such cases, the truth-values of S’s complex beliefs depend on the truth-values of their constituents, although the constituents may not be believed by S or by anyone. This means that a view according to which beliefs are primary truthbearers seems unable to account for how the truth-values of complex beliefs are connected to the truth-values of their simpler constituents—to do this one needs to be able to apply truth and falsehood to belief-constituents even when they are not believed. This point, which is equally fundamental for a proper understanding of logic, was made by all early advocates of propositions (cf. Bolzano 1837, I.§§22, 34; Frege 1879, §§2-5; Husserl 1900, I.§11; Meinong 1902, §6). The problem arises in much the same form for views that would take judgments, statements, or assertions as primary truthbearers. The problem is not easily evaded. Talk of unbelieved beliefs (unjudged judgments, unstated statements, unasserted assertions) is either absurd or simply amounts to talk of unbelieved (unjudged, unstated, unasserted) propositions or sentences. It is noteworthy, incidentally, that quite a few philosophical proposals (concerning truth as well as other matters) run afoul of the simple observation that there are unasserted and unbelieved truthbearers (cf. Geach 1960 & 1965).
The duality of state/content a.k.a. act/object. The noun “belief” can refer to the state of believing or to its content, i.e., to what is believed. If the former, the state of believing, can be said to be true or false at all, which is highly questionable, then only insofar as the latter, what is believed, is true or false. Similarly for nouns referring to mental acts or their objects (contents), such as “judgment”, “statement”, and “assertion”.
Mental sentences were the preferred primary truthbearers throughout the medieval period. They were neglected in the first half of the 20th century, but made a comeback in the second half through the revival of the representational theory of the mind (especially in the form of the language-of-thought hypothesis, cf. Fodor 1975). Somewhat confusingly (to us now), for many centuries the term “proposition” (propositio) was reserved exclusively for sentences, written, spoken or mental. This use was made official by Boethius in the 6th century, and is still found in Locke’s Essay in 1705 and in Mill’s Logic in 1843. Some time after that, e.g., in Moore’s 1901-01, “proposition” switched sides, the term now being used for what is said by uttering a sentence, for what is believed, judged, stated, assumed (etc.)—with occasional reversions to medieval usage, e.g. in Russell (1918, 1919).
2.2 Truthmakers
Talk of truthmakers serves a function similar, but correlative, to talk of truthbearers. A truthmaker is anything that makes some truthbearer true. Different versions of the correspondence theory will have different, and often competing, views about what sort of items true truthbearers correspond to (facts, states of affairs, events, things, tropes, properties). It is convenient to talk of truthmakers whenever one wants to stay neutral between these choices. Four points should be kept in mind:
The notion of a truthmaker is tightly connected with, and dependent on, the relational notion of truthmaking: a truthmaker is whatever stands in the truthmaking relation to some truthbearer. Despite the causal overtones of “maker” and “making”, this relation is usually not supposed to be a causal relation.
The terms “truthmaking” and “truthmaker” are ambiguous. For illustration, consider a classical correspondence theory on which x is true if and only if x corresponds to some fact. One can say (a) that x is made true by a fact, namely the or a fact x corresponds to. One can also say (b) that x is made true by x’s correspondence to a fact. Both uses of “is made true by” are correct and both occur in discussions of truth. But they are importantly different and must be distinguished. The (a)-use is usually the intended one; it expresses a relation peculiar to truth and leads to a use of “truthmaker” that actually picks out the items that would normally be intended by those using the term. The (b)-use does not express a relation peculiar to truth; it is just an instance (for “F” = “true”) of the generic formula “what makes an F-thing an F” that can be employed to elicit the definiens of a proposed definition of F. Compare: what makes an even number even is its divisibility by 2; what makes a right action right is its having better consequences than available alternative actions. Note that anyone proposing a definition or account of truth can avail themselves of the notion of truthmaking in the (b)-sense; e.g., a coherence theorist, advocating that a belief is true if and only if it coheres with other beliefs, can say: what makes a true belief true is its coherence with other beliefs. So, on the (b)-use, “truthmaking” and “truthmaker” do not signal any affinity with the basic idea underlying the correspondence theory of truth, whereas on the (a)-use these terms do signal such an affinity.
Talk of truthmaking and truthmakers goes well with the basic idea underlying the correspondence theory; hence, it might seem natural to describe a traditional fact-based correspondence theory as maintaining that the truthmakers are facts and that the correspondence relation is the truthmaking relation. However, the assumption that the correspondence relation can be regarded as (a species of) the truthmaking relation is dubious. Correspondence appears to be a symmetric relation (if x corresponds to y, then y corresponds to x), whereas it is usually taken for granted that truthmaking is an asymmetric relation, or at least not a symmetric one. It is hard to see how a symmetric relation could be (a species of) an asymmetric or non-symmetric relation (cf. David 2009.)
Talk of truthmaking and truthmakers is frequently employed during informal discussions involving truth but tends to be dropped when a more formal or official formulation of a theory of truth is produced (one reason being that it seems circular to define or explain truth in terms of truthmakers or truthmaking). However, in recent years, the informal talk has been turned into an official doctrine: “truthmaker theory”. This theory should be distinguished from informal truthmaker talk: not everyone employing the latter would subscribe to the former. Moreover, truthmaker theory should not simply be assumed to be a version of the correspondence theory; indeed, some advocates present it as a competitor to the correspondence theory (see below, Section 8.5).
2.3 Truth
The abstract noun “truth” has various uses. (a) It can be used to refer to the general relational property otherwise referred to as being true; though the latter label would be more perspicuous, it is rarely used, even in philosophical discussions. (b) The noun “truth” can be used to refer to the concept that “picks out” the property and is expressed in English by the adjective “true”. Some authors do not distinguish between concept and property; others do, or should: an account of the concept might differ significantly from an account of the property. To mention just one example, one might maintain, with some plausibility, that an account of the concept ought to succumb to the liar paradox (see the entry on the liar paradox), otherwise it wouldn’t be an adequate account of our concept of truth; this idea is considerably less plausible in the case of the property. Any proposed “definition of truth” might be intend as a definition of the property or of the concept or both; its author may or may not be alive to the difference. (c) The noun “truth” can be used, finally, to refer to some set of true truthbarers (possibly unknown), as in: “The truth is out there”, and: “The truth about this matter will never be known”.
3. Simple Versions of the Correspondence Theory
The traditional centerpiece of any correspondence theory is a definition of truth. Nowadays, a correspondence definition is most likely intended as a “real definition”, i.e., as a definition of the property, which does not commit its advocate to the claim that the definition provides a synonym for the term “true”. Most correspondence theorists would consider it implausible and unnecessarily bold to maintain that “true” means the same as “corresponds with a fact”. Some simple forms of correspondence definitions of truth should be distinguished (“iff” means “if and only if”; the variable, “x”, ranges over whatever truthbearers are taken as primary; the notion of correspondence might be replaced by various related notions):
(1)
x is true iff x corresponds to some fact;
x is false iff x does not correspond to any fact.
(2)
x is true iff x corresponds to some state of affairs that obtains;
x is false iff x corresponds to some state of affairs that does not obtain.
Both forms invoke portions of reality—facts/states of affairs—that are typically denoted by that-clauses or by sentential gerundives, viz. the fact/state of affairs that snow is white, or the fact/state of affairs of snow’s being white. (2)’s definition of falsehood is committed to there being (existing) entities of this sort that nevertheless fail to obtain, such as snow’s being green. (1)’s definition of falsehood is not so committed: to say that a fact does not obtain means, at best, that there is no such fact, that no such fact exists. It should be noted that this terminology is not standardized: some authors use “state of affairs” much like “fact” is used here (e.g. Armstrong 1997). The question whether non-obtaining beings of the relevant sort are to be accepted is the substantive issue behind such terminological variations. The difference between (2) and (1) is akin to the difference between Platonism about properties (embraces uninstantiated properties) and Aristotelianism about properties (rejects uninstantiated properties).
Advocates of (2) hold that facts are states of affairs that obtain, i.e., they hold that their account of truth is in effect an analysis of (1)’s account of truth. So disagreement turns largely on the treatment of falsehood, which (1) simply identifies with the absence of truth.
The following points might be made for preferring (2) over (1): (a) Form (2) does not imply that things outside the category of truthbearers (tables, dogs) are false just because they don’t correspond to any facts. One might think this “flaw” of (1) is easily repaired: just put an explicit specification of the desired category of truthbearers into both sides of (1). However, some worry that truthbearer categories, e.g. declarative sentences or propositions, cannot be defined without invoking truth and falsehood, which would make the resultant definition implicitly circular. (b) Form (2) allows for items within the category of truthbearers that are neither true nor false, i.e., it allows for the failure of bivalence. Some, though not all, will regard this as a significant advantage. (c) If the primary truthbearers are sentences or mental states, then states of affairs could be their meanings or contents, and the correspondence relation in (2) could be understood accordingly, as the relation of representation, signification, meaning, or having-as-content. Facts, on the other hand, cannot be identified with the meanings or contents of sentences or mental states, on pain of the absurd consequence that false sentences and beliefs have no meaning or content. (d) Take a truth of the form ‘p or q’, where ‘p’ is true and ‘q’ false. What are the constituents of the corresponding fact? Since ‘q’ is false, they cannot both be facts (cf. Russell 1906-07, p. 47f.). Form (2) allows that the fact corresponding to ‘p or q’ is an obtaining disjunctive state of affairs composed of a state of affairs that obtains and a state of affairs that does not obtain.
The main point in favor of (1) over (2) is that (1) is not committed to counting non-obtaining states of affairs, like the state of affairs that snow is green, as constituents of reality.
(One might observe that, strictly speaking, (1) and (2), being biconditionals, are not ontologically committed to anything. Their respective commitments to facts and states of affairs arise only when they are combined with claims to the effect that there is something that is true and something that is false. The discussion assumes some such claims as given.)
Both forms, (1) and (2), should be distinguished from:
(3)
x is true iff x corresponds to some fact that exists;
x is false iff x corresponds to some fact that does not exist,
which is a confused version of (1), or a confused version of (2), or, if unconfused, signals commitment to Meinongianism, i.e., the thesis that there are things/facts that do not exist. The lure of (3) stems from the desire to offer more than a purely negative correspondence account of falsehood while avoiding commitment to non-obtaining states of affairs. Moore at times succumbs to (3)’s temptations (1910-11, pp. 267 & 269, but see p. 277). It can also be found in the 1961 translation of Wittgenstein (1921, 4.25), who uses “state of affairs” (Sachverhalt) to refer to (atomic) facts. The translation has Wittgenstein saying that an elementary proposition is false, when the corresponding state of affairs (atomic fact) does not exist—but the German original of the same passage looks rather like a version of (2). Somewhat ironically, a definition of form (3) reintroduces Plato’s problem of falsehood into a fact-based correspondence theory, i.e., into a theory of the sort that was supposed to provide an alternative solution to that very problem (see Section 1.2).
A fourth simple form of correspondence definition was popular for a time (cf. Russell 1918, secs. 1 & 3; Broad 1933, IV.2.23; Austin 1950, fn. 23), but seems to have fallen out of favor:
(4)
x is true iff x corresponds (agrees) with some fact;
x is false iff x mis-corresponds (disagrees) with some fact.
This formulation attempts to avoid (2)’s commitment to non-obtaining states of affairs and (3)’s commitment to non-existent facts by invoking the relation of mis-correspondence, or disagreement, to account for falsehood. It differs from (1) in that it attempts to keep items outside the intended category of x’s from being false: supposedly, tables and dogs cannot mis-correspond with a fact. Main worries about (4) are: (a) its invocation of an additional, potentially mysterious, relation, which (b) seems difficult to tame: Which fact is the one that mis-corresponds with a given falsehood? and: What keeps a truth, which by definition corresponds with some fact, from also mis-corresponding with some other fact, i.e., from being a falsehood as well?
In the following, I will treat definitions (1) and (2) as paradigmatic; moreover, since advocates of (2) agree that obtaining states of affairs are facts, it is often convenient to condense the correspondence theory into the simpler formula provided by (1), “truth is correspondence to a fact”, at least as long as one is not particularly concerned with issues raised by falsehood.
4. Arguments for the Correspondence Theory
The main positive argument given by advocates of the correspondence theory of truth is its obviousness. Descartes: “I have never had any doubts about truth, because it seems a notion so transcendentally clear that nobody can be ignorant of it...the word ‘truth’, in the strict sense, denotes the conformity of thought with its object” (1639, AT II 597). Even philosophers whose overall views may well lead one to expect otherwise tend to agree. Kant: “The nominal definition of truth, that it is the agreement of [a cognition] with its object, is assumed as granted” (1787, B82). William James: “Truth, as any dictionary will tell you, is a property of certain of our ideas. It means their ‘agreement’, as falsity means their disagreement, with ‘reality’” (1907, p. 96). Indeed, The Oxford English Dictionary tells us: “Truth, n. Conformity with fact; agreement with reality”.
In view of its claimed obviousness, it would seem interesting to learn how popular the correspondence theory actually is. There are some empirical data. The PhilPapers Survey (conducted in 2009; cf. Bourget and Chalmers 2014), more specifically, the part of the survey targeting all regular faculty members in 99 leading departments of philosophy, reports the following responses to the question: “Truth: correspondence, deflationary, or epistemic?” Accept or lean toward: correspondence 50.8%; deflationary 24.8%; other 17.5%; epistemic 6.9%. The data suggest that correspondence-type theories may enjoy a weak majority among professional philosophers and that the opposition is divided. This fits with the observation that typically, discussions of the nature of truth take some version of the correspondence theory as the default view, the view to be criticized or to be defended against criticism.
Historically, the correspondence theory, usually in an object-based version, was taken for granted, so much so that it did not acquire this name until comparatively recently, and explicit arguments for the view are very hard to find. Since the (comparatively recent) arrival of apparently competing approaches, correspondence theorists have developed negative arguments, defending their view against objections and attacking (sometimes ridiculing) competing views.
5. Objections to the Correspondence Theory
Objection 1: Definitions like (1) or (2) are too narrow. Although they apply to truths from some domains of discourse, e.g., the domain of science, they fail for others, e.g. the domain of morality: there are no moral facts.
The objection recognizes moral truths, but rejects the idea that reality contains moral facts for moral truths to correspond to. Logic provides another example of a domain that has been “flagged” in this way. The logical positivists recognized logical truths but rejected logical facts. Their intellectual ancestor, Hume, had already given two definitions of “true”, one for logical truths, broadly conceived, the other for non-logical truths: “Truth or falsehood consists in an agreement or disagreement either to the real relations of ideas, or to real existence and matter of fact” (Hume, Treatise, 3.1.1, cf. 2.3.10; see also Locke, Essay, 4.5.6, for a similarly two-pronged account but in terms of object-based correspondence).
There are four possible responses to objections of this sort: (a) Noncognitivism, which says that, despite appearances to the contrary, claims from the flagged domain are not truth-evaluable to begin with, e.g., moral claims are commands or expressions of emotions disguised as truthbearers; (b) Error theory, which says that all claims from the flagged domain are false; (c) Reductionism, which says that truths from the flagged domain correspond to facts of a different domain regarded as unproblematic, e.g., moral truths correspond to social-behavioral facts, logical truths correspond to facts about linguistic conventions; and (d) Standing firm, i.e., embracing facts of the flagged domain.
The objection in effect maintains that there are different brands of truth (of the property being true, not just different brands of truths) for different domains. On the face of it, this conflicts with the observation that there are many obviously valid arguments combining premises from flagged and unflagged domains. The observation is widely regarded as refuting non-cognitivism, once the most popular (concessive) response to the objection.
In connection with this objection, one should take note of the recently developed “multiple realizability” view of truth, according to which truth is not to be identified with correspondence to fact but can be realized by correspondence to fact for truthbearers of some domains of discourse and by other properties for truthbearers of other domains of discourse, including “flagged” domains. Though it retains important elements of the correspondence theory, this view does not, strictly speaking, offer a response to the objection on behalf of the correspondence theory and should be regarded as one of its competitors (see below, Section 8.2).
Objection 2: Correspondence theories are too obvious. They are trivial, vacuous, trading in mere platitudes. Locutions from the “corresponds to the facts”-family are used regularly in everyday language as idiomatic substitutes for “true”. Such common turns of phrase should not be taken to indicate commitment to a correspondence theory in any serious sense. Definitions like (1) or (2) merely condense some trivial idioms into handy formulas; they don’t deserve the grand label “theory”: there is no theoretical weight behind them (cf. Woozley 1949, chap. 6; Davidson 1969; Blackburn 1984, chap. 7.1).
In response, one could point out: (a) Definitions like (1) or (2) are “mini-theories”—mini-theories are quite common in philosophy—and it is not at all obvious that they are vacuous merely because they are modeled on common usage. (b) There are correspondence theories that go beyond these definitions. (c) The complaint implies that definitions like (1) and/or (2) are generally accepted and are, moreover, so shallow that they are compatible with any deeper theory of truth. This makes it rather difficult to explain why some thinkers emphatically reject all correspondence formulations. (d) The objection implies that the correspondence of S’s belief with a fact could be said to consist in, e.g., the belief’s coherence with S’s overall belief system. This is wildly implausible, even on the most shallow understanding of “correspondence” and “fact”.
Objection 3: Correspondence theories are too obscure.
Objections of this sort, which are the most common, protest that the central notions of a correspondence theory carry unacceptable commitments and/or cannot be accounted for in any respectable manner. The objections can be divided into objections primarily aimed at the correspondence relation and its relatives (3.C1, 3.C2), and objections primarily aimed at the notions of fact or state of affairs (3.F1, 3.F2):
3.C1: The correspondence relation must be some sort of resemblance relation. But truthbearers do not resemble anything in the world except other truthbearers—echoing Berkeley’s “an idea can be like nothing but an idea”.
3.C2: The correspondence relation is very mysterious: it seems to reach into the most distant regions of space (faster than light?) and time (past and future). How could such a relation possibly be accounted for within a naturalistic framework? What physical relation could it possibly be?
3.F1: Given the great variety of complex truthbearers, a correspondence theory will be committed to all sorts of complex “funny facts” that are ontologically disreputable. Negative, disjunctive, conditional, universal, probabilistic, subjunctive, and counterfactual facts have all given cause for complaint on this score.
3.F2: All facts, even the most simple ones, are disreputable. Fact-talk, being wedded to that-clauses, is entirely parasitic on truth-talk. Facts are too much like truthbearers. Facts are fictions, spurious sentence-like slices of reality, “projected from true sentences for the sake of correspondence” (Quine 1987, p. 213; cf. Strawson 1950).
6. Correspondence as Isomorphism
Some correspondence theories of truth are two-liner mini-theories, consisting of little more than a specific version of (1) or (2). Normally, one would expect a bit more, even from a philosophical theory (though mini-theories are quite common in philosophy). One would expect a correspondence theory to go beyond a mere definition like (1) or (2) and discharge a triple task: it should tell us about the workings of the correspondence relation, about the nature of facts, and about the conditions that determine which truthbearers correspond to which facts.
One can approach this by considering some general principles a correspondence theory might want to add to its central principle to flesh out her theory. The first such principle says that the correspondence relation must not collapse into identity—“It takes two to make a truth” (Austin 1950, p. 118):
Nonidentity:
No truth is identical with a fact correspondence to which is sufficient for its being a truth.
It would be much simpler to say that no truth is identical with a fact. However, some authors, e.g. Wittgenstein 1921, hold that a proposition (Satz, his truthbearer) is itself a fact, though not the same fact as the one that makes the proposition true (see also King 2007). Nonidentity is usually taken for granted by correspondence theorists as constitutive of the very idea of a correspondence theory—authors who advance contrary arguments to the effect that correspondence must collapse into identity regard their arguments as objections to any form of correspondence theory (cf. Moore 1901/02, Frege 1918-19, p. 60).
Concerning the correspondence relation, two aspects can be distinguished: correspondence as correlation and correspondence as isomorphism (cf. Pitcher 1964; Kirkham 1992, chap. 4). Pertaining to the first aspect, familiar from mathematical contexts, a correspondence theorist is likely to adopt claim (a), and some may in addition adopt claim (b), of:
Correlation:
(a) Every truth corresponds to exactly one fact;
(b) Different truths correspond to different facts.
Together, (a) and (b) say that correspondence is a one-one relation. This seems needlessly strong, and it is not easy to find real-life correspondence theorists who explicitly embrace part (b): Why shouldn’t different truths correspond to the same fact, as long as they are not too different? Explicit commitment to (a) is also quite rare. However, correspondence theorists tend to move comfortably from talk about a given truth to talk about the fact it corresponds to—a move that signals commitment to (a).
Correlation does not imply anything about the inner nature of the corresponding items. Contrast this with correspondence as isomorphism, which requires the corresponding items to have the same, or sufficiently similar, constituent structure. This aspect of correspondence, which is more prominent (and more notorious) than the previous one, is also much more difficult to make precise. Let us say, roughly, that a correspondence theorist may want to add a claim to her theory committing her to something like the following:
Structure:
If an item of kind K corresponds to a certain fact, then they have the same or sufficiently similar structure: the overall correspondence between a true K and a fact is a matter of part-wise correspondences, i.e. of their having corresponding constituents in corresponding places in the same structure, or in sufficiently similar structures.
The basic idea is that truthbearers and facts are both complex structured entities: truthbearers are composed of (other truthbearers and ultimately of) words, or concepts; facts are composed of (other facts or states of affairs and ultimately of) things, properties, and relations. The aim is to show how the correspondence relation is generated from underlying relations between the ultimate constituents of truthbearers, on the one hand, and the ultimate constituents of their corresponding facts, on the other. One part of the project will be concerned with these correspondence-generating relations: it will lead into a theory that addresses the question how simple words, or concepts, can be about things, properties, and relations; i.e., it will merge with semantics or psycho-semantics (depending on what the truthbearers are taken to be). The other part of the project, the specifically ontological part, will have to provide identity criteria for facts and explain how their simple constituents combine into complex wholes. Putting all this together should yield an account of the conditions determining which truthbearers correspond to which facts.
Correlation and Structure reflect distinct aspects of correspondence. One might want to endorse the former without the latter, though it is hard to see how one could endorse the latter without embracing at least part (a) of the former.
The isomorphism approach offers an answer to objection 3.C1. Although the truth that the cat is on the mat does not resemble the cat or the mat (the truth doesn’t meow or smell, etc.), it does resemble the fact that the cat is on the mat. This is not a qualitative resemblance; it is a more abstract, structural resemblance.
The approach also puts objection 3.C2 in some perspective. The correspondence relation is supposed to reduce to underlying relations between words, or concepts, and reality. Consequently, a correspondence theory is little more than a spin-off from semantics and/or psycho-semantics, i.e. the theory of intentionality construed as incorporating a representational theory of the mind (cf. Fodor 1989). This reminds us that, as a relation, correspondence is no more—but also no less—mysterious than semantic relations in general. Such relations have some curious features, and they raise a host of puzzles and difficult questions—most notoriously: Can they be explained in terms of natural (causal) relations, or do they have to be regarded as irreducibly non-natural aspects of reality? Some philosophers have claimed that semantic relations are too mysterious to be taken seriously, usually on the grounds that they are not explainable in naturalistic terms. But one should bear in mind that this is a very general and extremely radical attack on semantics as a whole, on the very idea that words and concepts can be about things. The common practice to aim this attack specifically at the correspondence theory seems misleading. As far as the intelligibility of the correspondence relation is concerned, the correspondence theory will stand, or fall, with the general theory of reference and intentionality.
It should be noted, though, that these points concerning objections 3.C1 and 3.C2 are not independent of one’s views about the nature of the primary truthbearers. If truthbearers are taken to be sentences of an ordinary language (or an idealized version thereof), or if they are taken to be mental representations (sentences of the language of thought), the above points hold without qualification: correspondence will be a semantic or psycho-semantic relation. If, on the other hand, the primary truthbearers are taken to be propositions, there is a complication:
On a broadly Fregean view of propositions, propositions are constituted by concepts of objects and properties (in the logical, not the psychological, sense of “concept”). On this view, the above points still hold, since the relation between concepts, on the one hand, and the objects and properties they are concepts of, on the other, appears to be a semantic relation, a concept-semantic relation.
On the so-called Russellian view of propositions (which the early Russell inherited mostly from early Moore), propositions are constituted, not of concepts of objects and properties, but of the objects and properties themselves (cf. Russell 1903). On this view, the points above will most likely fail, since the correspondence relation would appear to collapse into the identity relation when applied to true Russellian propositions. It is hard to see how a true Russellian proposition could be anything but a fact: What would a fact be, if not this sort of thing? So the principle of Nonidentity is rejected, and with it goes the correspondence theory of truth: “Once it is definitely recognized that the proposition is to denote, not a belief or form of words, but an object of belief, it seems plain that a truth differs in no respect from the reality with which it was supposed merely to correspond” (Moore 1901-02, p. 717). A simple, fact-based correspondence theory, applied to propositions understood in the Russellian way, thus reduces to an identity theory of truth, on which a proposition is true iff it is a fact, and false, iff it is not a fact. See below, Section 8.3; and the entries on propositions, singular propositions, and structured propositions in this encyclopedia.
But Russellians don’t usually renounce the correspondence theory entirely. Though they have no room for (1) from Section 3, when applied to propositions as truthbearers, correspondence will enter into their account of truth for sentences, public or mental. The account will take the form of Section 3’s (2), applied to categories of truthbearers other than propositions, where Russellian propositions show up on the right-hand side in the guise of states of affairs that obtain or fail to obtain. Commitment to states of affairs in addition to propositions is sometimes regarded with scorn, as a gratuitous ontological duplication. But Russellians are not committed to states of affairs in addition to propositions, for propositions, on their view, must already be states of affairs. This conclusion is well nigh inevitable, once true propositions have been identified with facts. If a true proposition is a fact, then a false proposition that might have been true would have been a fact, if it had been true. So, a (contingent) false proposition must be the same kind of being as a fact, only not a fact—an unfact; but that just is a non-obtaining state of affairs under a different name. Russellian propositions are states of affairs: the false ones are states of affairs that do not obtain, and the true ones are states of affairs that do obtain.
The Russellian view of propositions is popular nowadays. Somewhat curiously, contemporary Russellians hardly ever refer to propositions as facts or states of affairs. This is because they are much concerned with understanding belief, belief attributions, and the semantics of sentences. In such contexts, it is more natural to talk proposition-language than state-of-affairs-language. It feels odd (wrong) to say that someone believes a state of affairs, or that states of affairs are true or false. For that matter, it also feels odd (wrong) to say that some propositions are facts, that facts are true, and that propositions obtain or fail to obtain. Nevertheless, all of this must be the literal truth, according to the Russellians. They have to claim that “proposition” and “state of affairs”, much like “evening star” and “morning star”, are different names for the same things—they come with different associations and are at home in somewhat different linguistic environments, which accounts for the felt oddness when one name is transported to the other’s environment.
Returning to the isomorphism approach in general, on a strict or naïve implementation of this approach, correspondence will be a one-one relation between truths and corresponding facts, which leaves the approach vulnerable to objections against funny facts (3.F1): each true truthbearer, no matter how complex, will be assigned a matching fact. Moreover, since a strict implementation of isomorphism assigns corresponding entities to all (relevant) constituents of truthbearers, complex facts will contain objects corresponding to the logical constants (“not”, “or”, “if-then”, etc.), and these “logical objects” will have to be regarded as constituents of the world. Many philosophers have found it hard to believe in the existence of all these funny facts and funny quasi-logical objects.
The isomorphism approach has never been advocated in a fully naïve form, assigning corresponding objects to each and every wrinkle of our verbal or mental utterings. Instead, proponents try to isolate the “relevant” constituents of truthbearers through meaning analysis, aiming to uncover the logical form, or deep structure, behind ordinary language and thought. This deep structure might then be expressed in an ideal-language (typically, the language of predicate logic), whose syntactic structure is designed to mirror perfectly the ontological structure of reality. The resulting view—correspondence as isomorphism between properly analyzed truthbearers and facts—avoids assigning strange objects to such phrases as “the average husband”, “the sake of”, and “the present king of France”; but the view remains committed to logically complex facts and to logical objects corresponding to the logical constants.
Austin (1950) rejects the isomorphism approach on the grounds that it projects the structure of our language onto the world. On his version of the correspondence theory (a more elaborated variant of (4) applied to statements), a statement as a whole is correlated to a state of affairs by arbitrary linguistic conventions without mirroring the inner structure of its correlate (cf. also Vision 2004). This approach appears vulnerable to the objection that it avoids funny facts at the price of neglecting systematicity. Language does not provide separate linguistic conventions for each statement: that would require too vast a number of conventions. Rather, it seems that the truth-values of statements are systematically determined, via a relatively small set of conventions, by the semantic values (relations to reality) of their simpler constituents. Recognition of this systematicity is built right into the isomorphism approach.
Critics frequently echo Austin’s “projection”-complaint, 3.F2, that a traditional correspondence theory commits “the error of reading back into the world the features of language” (Austin 1950, p. 155; cf. also, e.g., Rorty 1981). At bottom, this is a pessimistic stance: if there is a prima facie structural resemblance between a mode of speech or thought and some ontological category, it is inferred, pessimistically, that the ontological category is an illusion, a matter of us projecting the structure of our language or thought into the world. Advocates of traditional correspondence theories can be seen as taking the opposite stance: unless there are specific reasons to the contrary, they are prepared to assume, optimistically, that the structure of our language and/or thought reflects genuine ontological categories, that the structure of our language and/or thought is, at least to a significant extent, the way it is because of the structure of the world.
7. Modified Versions of the Correspondence Theory
7.1 Logical Atomism
Wittgenstein (1921) and Russell (1918) propose modified fact-based correspondence accounts of truth as part of their program of logical atomism. Such accounts proceed in two stages. At the first stage, the basic truth-definition, say (1) from Section 3, is restricted to a special subclass of truthbearers, the so-called elementary or atomic truthbearers, whose truth is said to consist in their correspondence to (atomic) facts: if x is elementary, then x is true iff x corresponds to some (atomic) fact. This restricted definition serves as the base-clause for truth-conditional recursion-clauses given at the second stage, at which the truth-values of non-elementary, or molecular, truthbearers are explained recursively in terms of their logical structure and the truth-values of their simpler constituents. For example: a sentence of the form ‘not-p’ is true iff ‘p’ is false; a sentence of the form ‘p and q’ is true iff ‘p’ is true and ‘q’ is true; a sentence of the form ‘p or q’ is true iff ‘p’ is true or ‘q’ is true, etc. These recursive clauses (called “truth conditions”) can be reapplied until the truth of a non-elementary, molecular sentence of arbitrary complexity is reduced to the truth or falsehood of its elementary, atomic constituents.
Logical atomism exploits the familiar rules, enshrined in the truth-tables, for evaluating complex formulas on the basis of their simpler constituents. These rules can be understood in two different ways: (a) as tracing the ontological relations between complex facts and constituent simpler facts, or (b) as tracing logico-semantic relations, exhibiting how the truth-values of complex sentences can be explained in terms of their logical relations to simpler constituent sentences together with the correspondence and non-correspondence of simple, elementary sentences to atomic facts. Logical atomism takes option (b).
Logical atomism is designed to go with the ontological view that the world is the totality of atomic facts (cf. Wittgenstein 1921, 2.04); thus accommodating objection 3.F2 by doing without funny facts: atomic facts are all the facts there are—although real-life atomists tend to allow conjunctive facts, regarding them as mere aggregates of atomic facts. An elementary truth is true because it corresponds to an atomic fact: correspondence is still isomorphism, but it holds exclusively between elementary truths and atomic facts. There is no match between truths and facts at the level of non-elementary, molecular truths; e.g., ‘p’, ‘p or q’, and ‘p or r’ might all be true merely because ‘p’ corresponds to a fact). The trick for avoiding logically complex facts lies in not assigning any entities to the logical constants. Logical complexity, so the idea goes, belongs to the structure of language and/or thought; it is not a feature of the world. This is expressed by Wittgenstein in an often quoted passage (1921, 4.0312): “My fundamental idea is that the ‘logical constants’ are not representatives; that there can be no representatives of the logic of facts”; and also by Russell (1918, p. 209f.): “You must not look about the real world for an object which you can call ‘or’, and say ‘Now look at this. This is ‘or’’”.
Though accounts of this sort are naturally classified as versions of the correspondence theory, it should be noted that they are strictly speaking in conflict with the basic forms presented in Section 3. According to logical atomism, it is not the case that for every truth there is a corresponding fact. It is, however, still the case that the being true of every truth is explained in terms of correspondence to a fact (or non-correspondence to any fact) together with (in the case of molecular truths) logical notions detailing the logical structure of complex truthbearers. Logical atomism attempts to avoid commitment to logically complex, funny facts via structural analysis of truthbearers. It should not be confused with a superficially similar account maintaining that molecular facts are ultimately constituted by atomic facts. The latter account would admit complex facts, offering an ontological analysis of their structure, and would thus be compatible with the basic forms presented in Section 3, because it would be compatible with the claim that for every truth there is a corresponding fact. (For more on classical logical atomism, see Wisdom 1931-1933, Urmson 1953, and the entries on Russell's logical atomism and Wittgenstein's logical atomism in this encyclopedia.)
While Wittgenstein and Russell seem to have held that the constituents of atomic facts are to be determined on the basis of a priori considerations, Armstrong (1997, 2004) advocates an a posteriori form of logical atomism. On his view, atomic facts are composed of particulars and simple universals (properties and relations). The latter are objective features of the world that ground the objective resemblances between particulars and explain their causal powers. Accordingly, what particulars and universals there are will have to be determined on the basis of total science.
Problems: Logical atomism is not easy to sustain and has rarely been held in a pure form. Among its difficulties are the following: (a) What, exactly, are the elementary truthbearers? How are they determined? (b) There are molecular truthbearers, such as subjunctives and counterfactuals, that tend to provoke the funny-fact objection but cannot be handled by simple truth-conditional clauses, because their truth-values do not seem to be determined by the truth-values of their elementary constituents. (c) Are there universal facts corresponding to true universal generalizations? Wittgenstein (1921) disapproves of universal facts; apparently, he wants to re-analyze universal generalizations as infinite conjunctions of their instances. Russell (1918) and Armstrong (1997, 2004) reject this analysis; they admit universal facts. (d) Negative truths are the most notorious problem case, because they clash with an appealing principle, the “truthmaker principle” (cf. Section 8.5), which says that for every truth there must be something in the world that makes it true, i.e., every true truthbearer must have a truthmaker. Suppose ‘p’ is elementary. On the account given above, ‘not-p’ is true iff ‘p’ is false iff ‘p’ does not correspond to any fact; hence, ‘not-p’, if true, is not made true by any fact: it does not seem to have a truthmaker. Russell finds himself driven to admit negative facts, regarded by many as paradigmatically disreputable portions of reality. Wittgenstein sometimes talks of atomic facts that do not exist and calls their very nonexistence a negative fact (cf. 1921, 2.06)—but this is hardly an atomic fact itself. Armstrong (1997, chap. 8.7; 2004, chaps. 5-6) holds that negative truths are made true by a second-order “totality fact” which says of all the (positive) first-order facts that they are all the first-order facts.
Atomism and the Russellian view of propositions (see Section 6). By the time Russell advocated logical atomism (around 1918), he had given up on what is now referred to as the Russellian conception of propositions (which he and G. E. Moore held around 1903). But Russellian propositons are popular nowadays. Note that logical atomism is not for the friends of Russellian propositions. The argument is straightforward. We have logically complex beliefs some of which are true. According to the friends of Russellian propositions, the contents of our beliefs are Russellian propositions, and the contents of our true beliefs are true Russellian propositions. Since true Russellian propositions are facts, there must be at least as many complex facts as there are true beliefs with complex contents (and at least as many complex states of affairs as there are true or false beliefs with complex contents). Atomism may work for sentences, public or mental, and for Fregean propositions; but not for Russellian propositions.
Logical atomism is designed to address objections to funny facts (3.F1). It is not designed to address objections to facts in general (3.F2). Here logical atomists will respond by defending (atomic) facts. According to one defense, facts are needed because mere objects are not sufficiently articulated to serve as truthmakers. If a were the sole truthmaker of ‘a is F’, then the latter should imply ‘a is G’, for any ‘G’. So the truthmaker for ‘a is F’ needs at least to involve a and Fness. But since Fness is a universal, it could be instantiated in another object, b, hence the mere existence of a and Fness is not sufficient for making true the claim ‘a is F’: a and Fness need to be tied together in the fact of a’s being F. Armstrong (1997) and Olson (1987) also maintain that facts are needed to make sense of the tie that binds particular objects to universals.
In this context it is usually emphasized that facts do not supervene on, hence, are not reducible to, their constituents. Facts are entities over and above the particulars and universals of which they are composed: a’s loving b and b’s loving a are not the same fact even though they have the very same constituents.
Another defense of facts, surprisingly rare, would point out that many facts are observable: one can see that the cat is on the mat; and this is different from seeing the cat, or the mat, or both. The objection that many facts are not observable would invite the rejoinder that many objects are not observable either. (See Austin 1961, Vendler 1967, chap. 5, and Vision 2004, chap. 3, for more discussion of anti-fact arguments; see also the entry facts in this encyclopedia.)
Some atomists propose an atomistic version of definition (1), but without facts, because they regard facts as slices of reality too suspiciously sentence-like to be taken with full ontological seriousness. Instead, they propose events and/or objects-plus-tropes (a.k.a. modes, particularized qualities, moments) as the corresponding portions of reality. It is claimed that these items are more “thingy” than facts but still sufficiently articulated—and sufficiently abundant—to serve as adequate truthmakers (cf. Mulligan, Simons, and Smith 1984).
7.2 Logical “Subatomism”
Logical atomism aims at getting by without logically complex truthmakers by restricting definitions like (1) or (2) from Section 3 to elementary truthbearers and accounting for the truth-values of molecular truthbearers recursively in terms of their logical structure and atomic truthmakers (atomic facts, events, objects-plus-tropes). More radical modifications of the correspondence theory push the recursive strategy even further, entirely discarding definitions like (1) or (2), and hence the need for atomic truthmakers, by going, as it were, “subatomic”.
Such accounts analyze truthbearers, e.g., sentences, into their subsentential constituents and dissolve the relation of correspondence into appropriate semantic subrelations: names refer to, or denote, objects; predicates (open sentences) apply to, or are satisfied by objects. Satisfaction of complex predicates can be handled recursively in terms of logical structure and satisfaction of simpler constituent predicates: an object o satisfies ‘x is not F’ iff o does not satisfy ‘x is F’; o satisfies ‘x is F or x is G’ iff o satisfies ‘x is F’ or o satisfies ‘x is G’; and so on. These recursions are anchored in a base-clause addressing the satisfaction of primitive predicates: an object o satisfies ‘x is F’ iff o instantiates the property expressed by ‘F’. Some would prefer a more nominalistic base-clause for satisfaction, hoping to get by without seriously invoking properties. Truth for singular sentences, consisting of a name and an arbitrarily complex predicate, is defined thus: A singular sentence is true iff the object denoted by the name satisfies the predicate. Logical machinery provided by Tarski (1935) can be used to turn this simplified sketch into a more general definition of truth—a definition that handles sentences containing relational predicates and quantifiers and covers molecular sentences as well. Whether Tarski’s own definition of truth can be regarded as a correspondence definition, even in this modified sense, is under debate (cf. Popper 1972; Field 1972, 1986; Kirkham 1992, chaps. 5-6; Soames 1999; Künne 2003, chap. 4; Patterson 2008.)
Subatomism constitutes a return to (broadly) object-based correspondence. Since it promises to avoid facts and all similarly articulated, sentence-like slices of reality, correspondence theorists who take seriously objection 3.F2 favor this approach: not even elementary truthbearers are assigned any matching truthmakers. The correspondence relation itself has given way to two semantic relations between constituents of truthbearers and objects: reference (or denotation) and satisfaction—relations central to any semantic theory. Some advocates envision causal accounts of reference and satisfaction (cf. Field 1972; Devitt 1982, 1984; Schmitt 1995; Kirkham 1992, chaps. 5-6). It turns out that relational predicates require talk of satisfaction by ordered sequences of objects. Davidson (1969, 1977) maintains that satisfaction by sequences is all that remains of the traditional idea of correspondence to facts; he regards reference and satisfaction as “theoretical constructs” not in need of causal, or any, explanation.
Problems: (a) The subatomistic approach accounts for the truth-values of molecular truthbearers in the same way as the atomistic approach; consequently, molecular truthbearers that are not truth-functional still pose the same problems as in atomism. (b) Belief attributions and modal claims pose special problems; e.g., it seems that “believes” is a relational predicate, so that “John believes that snow is white” is true iff “believes” is satisfied by John and the object denoted by “that snow is white”; but the latter appears to be a proposition or state of affairs, which threatens to let in through the back-door the very sentence-like slices of reality the subatomic approach was supposed to avoid, thus undermining the motivation for going subatomic. (c) The phenomenon of referential indeterminacy threatens to undermine the idea that the truth-values of elementary truthbearers are always determined by the denotation and/or satisfaction of their constituents; e.g., pre-relativistic uses of the term “mass” are plausibly taken to lack determinate reference (referring determinately neither to relativistic mass nor to rest mass); yet a claim like “The mass of the earth is greater than the mass of the moon” seems to be determinately true even when made by Newton (cf. Field 1973).
Problems for both versions of modified correspondence theories: (a) It is not known whether an entirely general recursive definition of truth, one that covers all truthbearers, can be made available. This depends on unresolved issues concerning the extent to which truthbearers are amenable to the kind of structural analyses that are presupposed by the recursive clauses. The more an account of truth wants to exploit the internal structure of truthbearers, the more it will be hostage to the (limited) availability of appropriate structural analyses of the relevant truthbearers. (b) Any account of truth employing a recursive framework may be virtually committed to taking sentences (maybe sentences of the language of thought) as primary truthbearers. After all, the recursive clauses rely heavily on what appears to be the logico-syntactic structure of truthbearers, and it is unclear whether anything but sentences can plausibly be said to possess that kind of structure. But the thesis that sentences of any sort are to be regarded as the primary truthbearers is contentious. Whether propositions can meaningfully be said to have an analogous (albeit non-linguistic) structure is under debate (cf. Russell 1913, King 2007). (c) If clauses like “‘p or q’ is true iff ‘p’ is true or ‘q’ is true” are to be used in a recursive account of our notion of truth, as opposed to some other notion, it has to be presupposed that ‘or’ expresses disjunction: one cannot define “or” and “true” at the same time. To avoid circularity, a modified correspondence theory (be it atomic or subatomic) must hold that the logical connectives can be understood without reference to correspondence truth.
7.3 Relocating Correspondence
Definitions like (1) and (2) from Section 3 assume, naturally, that truthbearers are true because they, the truthbearers themselves, correspond to facts. There are however views that reject this natural assumption. They propose to account for the truth of truthbearers of certain kinds, propositions, not by way of their correspondence to facts, but by way of the correspondence to facts of other items, the ones that have propositions as their contents. Consider the state of believing that p (or the activity of judging that p). The state (the activity) is not, strictly speaking, true or false; rather, what is true or false is its content, the proposition that p. Nevertheless, on the present view, it is the state of believing that p that corresponds or fails to correspond to a fact. So truth/falsehood for propositions can be defined in the following manner: x is a true/false proposition iff there is a belief state B such that x is the content of B and B corresponds/fails to correspond to a fact.
Such a modification of fact-based correspondence can be found in Moore (1927, p. 83) and Armstrong (1973, 4.iv & 9). It can be adapted to atomistic (Armstrong) and subatomistic views, and to views on which sentences (of the language of thought) are the primary bearers of truth and falsehood. However, by taking the content-carrying states as the primary corresponders, it entails that there are no truths/falsehoods that are not believed by someone. Most advocates of propositions as primary bearers of truth and falsehood will regard this as a serious weakness, holding that there are very many true and false propositions that are not believed, or even entertained, by anyone. Armstrong (1973) combines the view with an instrumentalist attitude towards propositions, on which propositions are mere abstractions from mental states and should not be taken seriously, ontologically speaking.
8. The Correspondence Theory and Its Competitors
8.1 Traditional Competitors
Against the traditional competitors—coherentist, pragmatist, and verificationist and other epistemic theories of truth—correspondence theorists raise two main sorts of objections. First, such accounts tend to lead into relativism. Take, e.g., a coherentist account of truth. Since it is possible that ‘p’ coheres with the belief system of S while ‘not-p’ coheres with the belief system of S*, the coherentist account seems to imply, absurdly, that contradictories, ‘p’ and ‘not-p’, could both be true. To avoid embracing contradictions, coherentists often commit themselves (if only covertly) to the objectionable relativistic view that ‘p’ is true-for-S and ‘not-p’ is true-for-S*. Second, the accounts tend to lead into some form of idealism or anti-realism, e.g., it is possible for the belief that p to cohere with someone’s belief system, even though it is not a fact that p; also, it is possible for it to be a fact that p, even if no one believes that p at all or if the belief does not cohere with anyone’s belief system. Cases of this sort are frequently cited as counterexamples to coherentist accounts of truth. Dedicated coherentists tend to reject such counterexamples, insisting that they are not possible after all. Since it is hard to see why they would not be possible, unless its being a fact that p were determined by the belief’s coherence with other beliefs, this reaction commits them to the anti-realist view that the facts are (largely) determined by what we believe.
This offers a bare outline of the overall shape the debates tend to take. For more on the correspondence theory vs. its traditional competitors see, e.g., Vision 1988; Kirkham 1992, chaps. 3, 7-8; Schmitt 1995; Künne 2003, chap. 7; and essays in Lynch 2001. Walker 1989 is a book-lenght discussion of coherence theories of truth. See also the entries on pragmatism, relativism, the coherence theory of truth, in this encyclopedia.
8.2 Pluralism
The correspondence theory is sometimes accused of overreaching itself: it does apply, so the objection goes, to truths from some domains of discourse, e.g., scientific discourse and/or discourse about everyday midsized physical things, but not to truths from various other domains of discourse, e.g., ethical and/or aesthetic discourse (see the first objection in Section 5 above). Alethic pluralism grows out of this objection, maintaining that truth is constituted by different properties for true propositions from different domains of discourse: by correspondence to fact for true propositions from the domain of scientific or everyday discourse about physical things; by some epistemic property, such as coherence or superassertibility, for true propositions from the domain of ethical and aesthetic discourse, and maybe by still other properties for other domains of discourse. This suggests a position on which the term “true” is multiply ambiguous, expressing different properties when applied to propositions from different domains. However, contemporary pluralists reject this problematic idea, maintaining instead that truth is “multiply realizable”. That is, the term “true” is univocal, it expresses one concept or property, truth (being true), but one that can be realized by or manifested in different properties (correspondence to fact, coherence or superassertibility, and maybe others) for true propositions from different domains of discourse. Truth itself is not to be identified with any of its realizing properties. Instead, it is characterized, quasi axiomatically, by a set of alleged “platitudes”, including, according to Crispin Wright’s (1999) version, “transparency” (to assert is to present as true), “contrast” (a proposition may be true without being justified, and v.v.), “timelesness” (if a proposition is ever true, then it always is), “absoluteness” (there is no such thing as a proposition being more or less true), and others.
Though it contains the correspondence theory as one ingredient, alethic pluralism is nevertheless a genuine competitor, for it rejects the thesis that truth is correspondence to reality. Moreover, it equally contains competitors of the correspondence theory as further ingredients.
Alethic pluralism in its contemporary form is a relatively young position. It was inaugurated by Crispin Wright (1992; see also 1999) and was later developed into a somewhat different form by Lynch (2009). Critical discussion is still at a relatively nascent stage (but see Vision 2004, chap. 4, for extended discussion of Wright). It will likely focus on two main problem areas.
First, it seems difficult to sort propositions into distinct kinds according to the subject matter they are about. Take, e.g., the proposition that killing is morally wrong, or the proposition that immoral acts happen in space-time. What are they about? Intuitively, their subject matter is mixed, belonging to the physical domain, the biological domain, and the domain of ethical discourse. It is hard to see how pluralism can account for the truth of such mixed propositions, belonging to more than one domain of discourse: What will be the realizing property?
Second, pluralists are expected to explain how the platitudes can be “converted” into an account of truth itself. Lynch (2009) proposes to construe truth as a functional property, defined in terms of a complex functional role which is given by the conjunction of the platitudes (somewhat analogous to the way in which functionalists in the philosophy of mind construe mental states as functional states, specified in terms of their functional roles—though in their case the relevant functional roles are causal roles, which is not a feasible option when it comes to the truth-role). Here the main issue will be to determine (a) whether such an account really works, when the technical details are laid out, and (b) whether it is plausible to claim that properties as different as correspondence to a fact, on the one hand, and coherence or superassertibilty, on the other, can be said to play one and the same role—a claim that seems required by the thesis that these different properties all realize the same property, being true.
For more on pluralism, see e.g. the essays in Monnoyer (2007) and in Pedersen & Wright (2013); and the entry on pluralist theories of truth in this encyclopedia.
8.3 The Identity Theory of Truth
According to the identity theory of truth, true propositions do not correspond to facts, they are facts: the true proposition that snow is white = the fact that snow is white. This non-traditional competitor of the correspondence theory threatens to collapse the correspondence relation into identity. (See Moore 1901-02; and Dodd 2000 for a book-length defense of this theory and discussion contrasting it with the correspondence theory; and see the entry the identity theory of truth: in this encyclopedia.)
In response, a correspondence theorist will point out: (a) The identity theory is defensible only for propositions as truthbearers, and only for propositions construed in a certain way, namely as having objects and properties as constituents rather than ideas or concepts of objects and properties; that is, for Russellian propositions. Hence, there will be ample room (and need) for correspondence accounts of truth for other types of truthbearers, including propositions, if they are construed as constituted, partly or wholly, of concepts of objects and properties. (b) The identity theory is committed to the unacceptable consequence that facts are true. (c) The identity theory rests on the assumption that that-clauses always denote propositions, so that the that-clause in “the fact that snow is white” denotes the proposition that snow is white. The assumption can be questioned. That-clauses can be understood as ambiguous names, sometimes denoting propositions and sometimes denoting facts. The descriptive phrases “the proposition…” and “the fact…” can be regarded as serving to disambiguate the succeeding ambiguous that-clauses—much like the descriptive phrases in “the philosopher Socrates” and “the soccer-player Socrates” serve to disambiguate the ambiguous name “Socrates” (cf. David 2002).
8.4 Deflationism About Truth
At present the most noticeable competitors to correspondence theories are deflationary accounts of truth (or ‘true’). Deflationists maintain that correspondence theories need to be deflated; that their central notions, correspondence and fact (and their relatives), play no legitimate role in an adequate account of truth and can be excised without loss. A correspondence-type formulation like
(5) “Snow is white” is true iff it corresponds to the fact that snow is white,
is to be deflated to
(6) “Snow is white” is true iff snow is white,
which, according to deflationists, says all there is to be said about the truth of “Snow is white”, without superfluous embellishments (cf. Quine 1987, p. 213).
Correspondence theorists protest that (6) cannot lead to anything deserving to be regarded as an account of truth. It is concerned with only one particular sentence (“Snow is white”), and it resists generalization. (6) is a substitution instance of the schema
(7) “p” is true iff p,
which does not actually say anything itself (it is not truth-evaluable) and cannot be turned into a genuine generalization about truth, because of its essential reliance on the schematic letter “p”, a mere placeholder. The attempt to turn (7) into a generalization produces nonsense along the lines of “For every x, “x” is true iff x”, or requires invocation of truth: “Every substitution instance of the schema ““p” is true iff p” is true”. Moreover, no genuine generalizations about truth can be accounted for on the basis of (7). Correspondence definitions, on the other hand, do yield genuine generalizations about truth. Note that definitions like (1) and (2) in Section 3 employ ordinary objectual variables (not mere schematic placeholders); the definitions are easily turned into genuine generalizations by prefixing the quantifier phrase “For every x”, which is customarily omitted in formulations intended as definitions.
It should be noted that the deflationist’s starting point, (5), which lends itself to deflating excisions, actually misrepresents the correspondence theory. According to (5), corresponding to the fact that snow is white is sufficient and necessary for “Snow is white” to be true. Yet, according to (1) and (2), it is sufficient but not necessary: “Snow is white” will be true as long as it corresponds to some fact or other. The genuine article, (1) or (2), is not as easily deflated as the impostor (5).
The debate turns crucially on the question whether anything deserving to be called an “account” or “theory” of truth ought to take the form of a genuine generalization (and ought to be able to account for genuine generalizations involving truth). Correspondence theorists tend to regard this as a (minimal) requirement. Deflationists argue that truth is a shallow (sometimes “logical”) notion—a notion that has no serious explanatory role to play: as such it does not require a full-fledged account, a real theory, that would have to take the form of a genuine generalization.
There is now a substantial body of literature on truth-deflationism in general and its relation to the correspondence theory in particular; the following is a small selection: Quine 1970, 1987; Devitt 1984; Field 1986; Horwich 1990 & 19982; Kirkham 1992; Gupta 1993; David 1994, 2008; Schmitt 1995; Künne 2003, chap. 4; Rami 2009. Relevant essays are contained in Blackburn and Simmons 1999; Schantz 2002; Armour-Garb and Beall 2005; and Wright and Pedersen 2010. See also the entry the deflationary theory of truth in this encyclopedia.
8.5 Truthmaker Theory
This approach centers on the truthmaker or truthmaking principle: Every truth has a truthmaker; or alternatively: For every truth there is something that makes it true. The principle is usually understood as an expression of a realist attitude, emphasizing the crucial contribution the world makes to the truth of a proposition. Advocates tend to treat truthmaker theory primarily as a guide to ontology, asking: To entities of what ontological categories are we committed as truthmakers of the propositions we accept as true? Most advocates maintain that propositions of different logical types can be made true by items from different ontological categories: e.g., propositions of some types are made true by facts, others just by individual things, others by events, others by tropes (cf., e.g. Armstrong 1997). This is claimed as a significant improvement over traditional correspondence theories which are understood—correctly in most but by no means all cases—to be committed to all truthmakers belonging to a single ontological category (albeit disagreeing about which category that is). All advocates of truthmaker theory maintain that the truthmaking relation is not one-one but many-many: some truths are made true by more than one truthmaker; some truthmakers make true more than one truth. This is also claimed as a significant improvement over traditional correspondence theories which are often portrayed as committed to correspondence being a one-one relation. This portrayal is only partly justified. While it is fairly easy to find real-life correspondence theorists committing themselves to the view that each truth corresponds to exactly one fact (at least by implication, talking about the corresponding fact), it is difficult to find real-life correspondence theorists committing themselves to the view that only one truth can correspond to a given fact (but see Moore 1910-11, p. 256).
A truthmaker theory may be presented as a competitor to the correspondence theory or as a version of the correspondence theory. This depends considerably on how narrowly or broadly one construes “correspondence theory”, i.e. on terminological issues. Some advocates would agree with Dummett (1959, p. 14) who said that, although “we have nowadays abandoned the correspondence theory of truth”, it nevertheless “expresses one important feature of the concept of truth…: that a statement is true only if there is something in the world in virtue of which it is true”. Other advocates would follow Armstrong who tends to present his truthmaker theory as a liberal form of correspondence theory; indeed, he seems committed to the view that the truth of a (contingent) elementary proposition consists in its correspondence with some (atomic) fact (cf. Armstrong 1997; 2004, pp. 22-3, 48-50).
It is not easy to find a substantive difference between truthmaker theory and various brands of the sort of modified correspondence theory treated above under the heading “Logical Atomism” (see Section 7.1). Logical atomists, such as Russell (1918) and Wittgenstein (1921), will hold that the truth or falsehood of every truth-value bearer can be explained in terms of (can be derived from) logical relations between truth-value bearers, by way of the recursive clauses, together with the base clauses, i.e., the correspondence and non-correspondence of elementary truth-value bearers with facts. This recursive strategy could be pursued with the aim to reject the truthmaker principle: not all truths have truthmakers, only elementary truths have truthmakers (here understood as corresponding atomic facts). But it could also be pursued—and this seems to have been Russell’s intention at the time—with the aim to secure the truthmaker principle, even though the simple correspondence definition has been abandoned: not every truth corresponds to a fact, only elementary truths do, but every truth has a truthmaker; where the recursive clauses are supposed to show how truthmaking without correspondence, but grounded in correspondence, comes about.
There is one straightforward difference between truthmaker theory and most correspondence theories. The latter are designed to answer the question “What is truth?”. Simple (unmodified) correspondence theories center on a biconditional, such as “x is true iff x corresponds to a fact”, intended to convey a definition of truth (at least a “real definition” which does not commit them to the claim that the term “true” is synonymous with “corresponds to a fact”—especially nowadays most correspondence theorists would consider such a claim to be implausibly and unnecessarily bold). Modified correspondence theories also aim at providing a definition of truth, though in their case the definition will be considerably more complex, owing to the recursive character of the account. Truthmaker theory, on the other hand, centers on the truthmaker principle: For every truth there is something that makes it true. Though this principle will deliver the biconditional “x is true iff something makes x true” (since “something makes x true” trivially implies “x is true”), this does not yield a promising candidate for a definition of truth: defining truth in terms of truthmaking would appear to be circular. Unlike most correspondence theories, truthmaker theory is not equipped, and usually not designed, to answer the question “What is truth?”—at least not if one expects the answer to take the form of a feasible candidate for a definition of truth.
There is a growing body of literature on truthmaker theory; see for example: Russell 1918; Mullligan, Simons, and Smith 1984; Fox 1987; Armstrong 1997, 2004; Merricks 2007; and the essays in Beebe and Dodd 2005; Monnoyer 2007; and in Lowe and Rami 2009. See also the entry on truthmakers in this encyclopedia.
9. More Objections to the Correspondence Theory
Two final objections to the correspondence theory deserve separate mention.
9.1 The Big Fact
Inspired by an allegedly similar argument of Frege’s, Davidson (1969) argues that the correspondence theory is bankrupt because it cannot avoid the consequence that all true sentences correspond to the same fact: the Big Fact. The argument is based on two crucial assumptions: (i) Logically equivalent sentences can be substituted salva veritate in the context ‘the fact that...’; and (ii) If two singular terms denoting the same thing can be substituted for each other in a given sentence salva veritate, they can still be so substituted if that sentence is embedded within the context ‘the fact that...’. In the version below, the relevant singular terms will be the following: ‘(the x such that x = Diogenes & p)’ and ‘(the x such that x = Diogenes & q)’. Now, assume that a given sentence, s, corresponds to the fact that p; and assume that ‘p’ and ‘q’ are sentences with the same truth-value. We have:
s corresponds to the fact that p
which, by (i), implies
s corresponds to the fact that [(the x such that x = Diogenes & p) = (the x such that x = Diogenes)],
which, by (ii), implies
s corresponds to the fact that [(the x such that x = Diogenes & q) = (the x such that x = Diogenes)],
which, by (i), implies
s corresponds to the fact that q.
Since the only restriction on ‘q’ was that it have the same truth-value as ‘p’, it would follow that any sentence s that corresponds to any fact corresponds to every fact; so that all true sentences correspond to the same facts, thereby proving the emptiness of the correspondence theory—the conclusion of the argument is taken as tantamount to the conclusion that every true sentence corresponds to the totality of all the facts, i.e, the Big Fact, i.e., the world as a whole.
This argument belongs to a type now called “slingshot arguments” (because a giant opponent is brought down by a single small weapon, allegedly). The first versions of this type of argument were given by Church (1943) and Gödel (1944); it was later adapted by Quine (1953, 1960) in his crusade against quantified modal logic. Davidson is offering yet another adaption, this time involving the expression “corresponds to the fact that”. The argument has been criticized repeatedly. Critics point to the two questionable assumptions on which it relies, (i) and (ii). It is far from obvious why a correspondence theorist should be tempted by either one of them. Opposition to assumption (i) rests on the view that expressibility by logically equivalent sentences may be a necessary, but is not a sufficient condition for fact identity. Opposition to assumption (ii) rests on the observation that the (alleged) singular terms used in the argument are definite descriptions: their status as genuine singular terms is in doubt, and it is well-known that they behave rather differently than proper names for which assumption (ii) is probably valid (cf. Follesdal 1966/2004; Olson 1987; Künne 2003; and especially the extended discussion and criticism in Neale 2001.)
The objection that may well have been the most effective in causing discontent with the correspondence theory is based on an epistemological concern. In a nutshell, the objection is that a correspondence theory of truth must inevitably lead into skepticism about the external world, because the required correspondence between our thoughts and reality is not ascertainable. Ever since Berkeley’s attack on the representational theory of the mind, objections of this sort have enjoyed considerable popularity. It is typically pointed out that we cannot step outside our own minds to compare our thoughts with mind-independent reality. Yet—so the objection continues—on the correspondence theory of truth, this is precisely what we would have to do to gain knowledge. We would have to access reality as it is in itself, independently of our cognition, and determine whether our thoughts correspond to it. Since this is impossible, since all our access to the world is mediated by our cognition, the correspondence theory makes knowledge impossible (cf. Kant 1800, intro vii). Assuming that the resulting skepticism is unacceptable, the correspondence theory has to be rejected, and some other account of truth, an epistemic (anti-realist) account of some sort, has to be put in its place (cf., e.g., Blanshard 1941.)
This type of objection brings up a host of issues in epistemology, the philosophy of mind, and general metaphysics. All that can be done here is to hint at a few pertinent points (cf. Searle 1995, chap. 7; David 2004, 6.7). The objection makes use of the following line of reasoning: “If truth is correspondence, then, since knowledge requires truth, we have to know that our beliefs correspond to reality, if we are to know anything about reality”. There are two assumptions implicit in this line of reasoning, both of them debatable.
(i) It is assumed that S knows x, only if S knows that x is true—a requirement not underwritten by standard definitions of knowledge, which tell us that S knows x, only if x is true and S is justified in believing x. The assumption may rest on confusing requirements for knowing x with requirements for knowing that one knows x.
(ii) It is assumed that, if truth = F, then S knows that x is true, only if S knows that x has F. This is highly implausible. By the same standard it would follow that no one who does not know that water is H2O can know that the Nile contains water—which would mean, of course, that until fairly recently nobody knew that the Nile contained water (and that, until fairly recently, nobody knew that there were stars in the sky, whales in the sea, or that the sun gives light). Moreover, even if one does know that water is H2O, one’s strategy for finding out whether the liquid in one’s glass is water does not have to involve chemical analysis: one could simply taste it, or ask a reliable informant. Similarly, as far as knowing that x is true is concerned, the correspondence theory does not entail that we have to know that a belief corresponds to a fact in order to know that it is true, or that our method of finding out whether a belief is true has to involve a strategy of actually comparing a belief with a fact—although the theory does of course entail that one obtains knowledge only if one obtains a belief that corresponds to a fact.
More generally, one might question whether the objection still has much bite once the metaphors of “accessing” and “comparing” are spelled out with more attention to the psychological details of belief formation and to epistemological issues concerning the conditions under which beliefs are justified or warranted. For example, it is quite unclear how the metaphor of “comparing” applies to knowledge gained through perceptual belief-formation. A perceptual belief that p may be true, and by having acquired that belief, one may have come to know that p, without having “compared” (the content of) one’s belief with anything.
One might also wonder whether its competitors actually enjoy any significant advantage over the correspondence theory, once they are held to the standards set up by this sort of objection. For example, why should it be easier to find out whether one particular belief coheres with all of one’s other beliefs than it is to find out whether a belief corresponds with a fact?
In one form or other, the “No independent access to reality”-objection against correspondence theoretic approaches has been one of the, if not the, main source and motivation for idealist and anti-realist stances in philosophy (cf. Stove 1991). However, the connection between correspondence theories of truth and the metaphysical realism vs. anti-realism (or idealism) debate is less immediate than is often assumed. On the one hand, deflationists and identity theorists can be, and typically are, metaphysical realists while rejecting the correspondence theory. On the other hand, advocates of a correspondence theory can, in principle, be metaphysical idealists (e.g. McTaggart 1921) or anti-realists, for one might advocate a correspondence theory while maintaining, at the same time, (a) that all facts are constituted by mind or (b) that what facts there are depends somehow on what we believe or are capable of believing, or (c) that the correspondence relation between true propositions and facts depends somehow on what we believe or are capable of believing (claiming that the correspondence relation between true beliefs or true sentences and facts depends on what we believe can hardly count as a commitment to anti-realism). Keeping this point in mind, one can nevertheless acknowledge that advocacy of a correspondence theory of truth comes much more naturally when combined with a metaphysically realist stance and usually signals commitment to such a stance.
Bibliography
Academic Tools
Other Internet Resources
Related Entries
1. What is the Language of Thought Hypothesis?
LOTH is an empirical thesis about the nature of thought and thinking. According to LOTH, thought and thinking are done in a mental language, i.e., in a symbolic system physically realized in the brain of the relevant organisms. In formulating LOTH, philosophers have in mind primarily the variety of thoughts known as ‘propositional attitudes’. Propositional attitudes are the thoughts described by such sentence forms as ‘S believes that P’, ‘S hopes that P’, ‘S desires that P’, etc., where ‘S’ refers to the subject of the attitude, ‘P’ is any sentence, and ‘that P’ refers to the proposition that is the object of the attitude. If we let ‘A’ stand for such attitude verbs as ‘believe’, ‘desire’, ‘hope’, ‘intend’, ‘think’, etc., then the propositional attitude statements all have the form: S As that P.
LOTH can now be formulated more exactly as a hypothesis about the nature of propositional attitudes and the way we entertain them. It can be characterized as the conjunction of the following three theses (A), (B) and (C):
Representational Theory of Mind (RTM) (cf. Field 1978:37, Fodor 1987:17):
Representational Theory of Thought: For each propositional attitude A, there is a unique and distinct (i.e. dedicated)[1] psychological relation R, and for all propositions P and subjects S, S As that P if and only if there is a mental representation #P# such that
S bears R to #P#, and
#P# means that P.
Representational Theory of Thinking: Mental processes, thinking in particular, consists of causal sequences of tokenings of mental representations.
Mental representations, which, as per (A1), constitute the direct “objects” of propositional attitudes, belong to a representational or symbolic system which is such that (cf. Fodor and Pylyshyn 1988:12–3)
representations of the system have a combinatorial syntax and semantics: structurally complex (molecular) representations are systematically built up out of structurally simple (atomic) constituents, and the semantic content of a molecular representation is a function of the semantic content of its atomic constituents together with its syntactic/formal structure, and
the operations on representations (constituting, as per (A2), the domain of mental processes, thinking) are causally sensitive to the syntactic/formal structure of representations defined by this combinatorial syntax.
Functionalist Materialism. Mental representations so characterized are, at some suitable level, functionally characterizable entities that are (possibly, multiply) realized by the physical properties of the subject having propositional attitudes (if the subject is an organism, then the realizing properties are presumably the neurophysiological properties of the brain).
The relation R in (A1), when RTM is combined with (B), is meant to be understood as a computational/functional relation. The idea is that each attitude is identified with a characteristic computational/functional role played by the mental sentence that is the direct “object” of that kind of attitude. (Scare quotes are necessary because it is more appropriate to reserve ‘object’ for a proposition as we have done above, but as long as we keep this in mind, it is harmless to use it in this way for LOT sentences.) For instance, what makes a certain mental sentence an (occurrent) belief might be that it is characteristically the output of perceptual systems and input to an inferential system that interacts decision-theoretically with desires to produce further sentences or action commands. Or equivalently, we may think of belief sentences as those that are accessible only to certain sorts of computational operations appropriate for beliefs, but not to others. Similarly, desire-sentences (and sentences for other attitudes) may be characterized by a different set of operations that define a characteristic computational role for them. In the literature it is customary to use the metaphor of a “belief-box” (cf. Schiffer 1981) as a blanket term to cover whatever specific computational role belief sentences turn out to have in the mental economy of their possessors. (Similarly for “desire-box”, etc.)
The Language of Thought Hypothesis is so-called because of (B): token mental representations are like sentences in a language in that they have a syntactically and semantically regimented constituent structure. Put differently, the mental representations that are the direct “objects” of attitudes are structurally complex symbols whose complexity lends itself to a syntactic and semantic analysis. This is also why the LOT is sometimes called Mentalese.
It is (B2) that makes LOTH a species of the so-called Computational Theory of Mind (CTM). This is why LOTH is sometimes called the Computational/Representational Theory of Mind or Thought (CRTM/CRTT) (cf. Rey 1991, 1997). Indeed, LOTH seems to be the most natural product when RTM is combined with a view that treats mental processes or thinking as computational when computation is understood traditionally or classically (this is a recent term emphasizing the contrast with connectionist processing, which we will discuss later).
According to LOTH, when someone believes that P, there is a sense in which the immediate “object” of one's belief can be said to be a complex symbol, a sentence in one's LOT physically realized in the neurophysiology of one's brain, that has both syntactic structure and a semantic content, namely the proposition that P. So, contrary to the orthodox view that takes the belief relation as a dyadic relation between an agent and a proposition, LOTH takes it to be a triadic relation among an agent, a Mentalese sentence, and a proposition. The Mentalese sentence can then be said to have the proposition as its semantic/intentional content. Within the framework of LOTH, it is only in this sense can it be said that what is believed is a proposition, and thus the proper object of the attitude.
This triadic view seems to have several advantages over the orthodox dyadic view. It is a puzzle in the dyadic view how intentional organisms can stand in direct relation to abstract objects like propositions in such a way as to influence their causal powers. According to folk psychology (ordinary commonsense psychology that we rely on daily in our dealings with others), it is because those states have the propositional content they do that they have the causal powers they do. LOTH makes this relatively non-mysterious by introducing a physical intermediary that is capable of having the relevant causal powers in virtue of its syntactic structure that encodes its semantic content. Another advantage of this is that the thought processes can be causally guided by the syntactic forms of the sentences in a way that respect their semantic contents. This is the virtue of (B) to which we'll come back below. Mainly because of these features, LOTH is said to be poised to scientifically vindicate folk psychology if it turns out to be true.
2. Status of LOTH
LOTH has primarily been advanced as an empirical thesis (although some have argued for the truth of LOTH on a priori or conceptual grounds following the natural conceptual contours of folk psychology—see Davies 1989, 1991; Lycan 1993; Rey 1995; Jacob 1997; Markic 2001 argues against Jacob. Harman 1973 develops and defends LOTH on both empirical and conceptual grounds). It is not meant to be taken as an analysis of what the folk mean (or, for that matter, what the scientists ought to mean) when they talk about various propositional attitudes and their role in thinking. In this regard, LOT theorists typically view themselves as engaged in some sort of a proto-science, or at least in some empirical research program continuous with scientific psychology. Indeed, as we will see in more detail below, when Jerry Fodor first explicitly articulated and elaborated LOTH in some considerable detail in his (1975), he basically defended it on the ground that it was assumed by our best scientific theories or models in cognitive psychology and psycholinguistics. This empirical status generally accorded to LOTH should be kept firmly in mind when assessing its plausibility and especially its prospects in the light of new evidence and developments in scientific psychology. Nevertheless, it would be more appropriate to see LOTH more as a foundational thesis rather than as an ongoing research project guided by a set of concrete empirical methods, specific theses and principles. In this regard, LOTH stands to specific scientific theories of the (various aspects of the) mind somewhat like the “Atomic Hypothesis” stands to a whole bunch specific scientific theories about the particulate nature of the world (some of which may be—and certainly historically, have been—incompatible with each other).
When viewed this way, scientific theories advanced within the LOTH framework are not, strictly speaking, committed to preserving the folk taxonomy of the mental states in any very exact way. Notions like belief, desire, hope, fear, etc. are folk notions and, as such, it may not be utterly plausible to expect (eliminativist arguments aside) that a scientific psychology will preserve the exact contours of these concepts. On the contrary, there is every reason to believe that scientific counterparts of these notions will carve the mental space somewhat differently. For instance, it has been noted that the folk notion of belief harbors many distinctions. For example, it has both a dispositional and an occurrent sense. In the occurrent sense, it seems to mean something like consciously entertaining and accepting a thought (proposition) as true. There is quite a bit of literature and controversy on the dispositional sense.[2] Beliefs are also capable of being explicitly stored in long term memory as opposed to being merely dispositional or tacit. Compare, for instance: I believe that there was a big surprise party for my 24th birthday vs. I have always believed that lions don't eat their food with forks and knives, or that 13652/4=3413, even though until now these latter two thoughts had never occurred to me. There is furthermore the issue of degree of belief: while I may believe that George will come to dinner with his new girlfriend even though I wouldn't bet on it, you, thinking that you know him better than I do, may nevertheless go to the wall for it. It is unlikely that there will be one single construct of scientific psychology that will exactly correspond to the folk notion of belief in all these ways.
For LOTH to vindicate folk psychology it is sufficient that a scientific psychology with a LOT architecture come up with scientifically grounded psychological states that are recognizably like the propositional attitudes of folk psychology, and that play more or less similar roles in psychological explanations.[3]
3. Scope of LOTH
LOTH is an hypothesis about the nature of thought and thinking with propositional content. As such, it may or may not be applicable to other aspects of mental life. Officially, it is silent about the nature of some mental phenomena such as experience, qualia,[4] sensory processes, mental images, visual and auditory imagination, sensory memory, perceptual pattern-recognition capacities, dreaming, hallucinating, etc. To be sure, many LOT theorists hold views about these aspects of mental life that sometimes make it seem that they are also to be explained by something similar to LOTH.[5]
For instance, Fodor (1983) seems to think that many modular input systems have their own LOT to the extent to which they can be explained in representational and computational terms. Indeed, many contemporary psychological models treat perceptual input systems in just these terms.[6] There is indeed some evidence that this kind of treatment might be appropriate for many perceptual processes. But it is to be kept in mind that a system may employ representations and be computational without necessarily satisfying any or both of the clauses in (B) above in any full-fledged way. Just think of finite automata theory where there are plenty of examples of a computational process defined over states or symbols which lack full-blown syntactic and/or semantic structural complexity. (For a useful discussion of varieties of computational processes and their classification, see Piccinini 2008.) Whether sensory or perceptual processes are to be treated within the framework of full-blown LOTH is again an open empirical question. It might be that the answer to this question is affirmative. If so, there may be more than one LOT realized in different subsystems or mechanisms in the mind/brain. So LOTH is not committed to there being a single representational system realized in the brain, nor is it committed to the claim that all mental representations are complex or language-like, nor would it be falsified if it turns out that most aspects of mental life other than the ones involving propositional attitudes don't require a LOT.
Similarly, there is strong evidence that the mind also exploits an image-like representational medium for certain kinds of mental tasks.[7] LOTH is non-committal about the existence of an image-like representational system for many mental tasks other than the ones involving propositional attitudes. But it is committed to the claim that propositional thought and thinking cannot be successfully accounted for in its entirety in purely imagistic terms. It claims that a combinatorial sentential syntax is necessary for propositional attitudes and a purely imagistic medium is not adequate for capturing that.[8]
There are in fact some interesting and difficult issues surrounding these claims. The adequacy of an imagistic system seems to turn on the nature of syntax at the sentential level. For instance, Fodor, in Chapter 4 of his (1975) book, allows that many lexical items in one's LOT may be image-like; he introduces the notion of a mental image/picture under description to avoid some obvious inadequacies of pictures (e.g., what makes a picture a picture of an overweight woman rather than a pregnant one, or vice versa, etc.). This is an attempt to combine discursive and imagistic representational elements at the lexical level. There may even be a well defined sense in which pictures can be combined to produce structurally complex pictures (as in British Empiricism: image-like simple ideas are combined to produce complex ideas, e.g., the idea of a unicorn—see also Prinz 2002). But what is absolutely essential for LOTH, and what Fodor insists on, is the claim that there is no adequate way in which a purely image-like system can capture what is involved in making judgments, i.e., in judging propositions to be true. This seems to require a discursive syntactic approach at the sentential level. The general problem here is the inadequacy of pictures or image-like representations to express propositions. I can judge that the blue box is on top of the red one without judging that the red box is under the blue one. I can judge that Mary kisses John without judging that John kisses Mary, and so on for indefinitely many such cases. It is hard to see how images or pictures can do that without using any syntactic structure or discursive elements, to say nothing of judging, e.g., conditionals, disjunctive or negative propositions, quantifications, negative existentials, etc.[9]
Moreover, there are difficulties with imagistic representations arising from demands on processing representations. As we will see below, (B2) turns out to provide the foundations for one of the most important arguments for LOTH: it makes it possible to mechanize thinking understood as a semantically coherent thought process, which, as per (A2), consists of a causal sequence of tokenings of mental representations. It is not clear, however, how an equivalent of (B2) could be provided for images or pictures in order to accommodate operations defined over them, even if something like an equivalent of (B1) could be given. On the other hand, there are truly promising attempts to integrate discursive symbolic theorem-proving with reasoning with image-like symbols. They achieve impressive efficiency in theorem-proving or in any deductive process defined over the expressions of such an integrated system. Such attempts, if they prove to be generalizable to psychological theorizing, are by no means threats to LOTH; on the contrary, such systems have every feature to make them a species of a LOT system: they satisfy (B).[10]
4. Nativism and LOTH
In the book (1975) in which Fodor introduced the LOTH, he also argued that all concepts are innate. As a result, the connection between LOTH and an implausibly strong version of conceptual nativism looked very much internal. This historical coincidence has led some people to think that LOTH is essentially committed to a very strong form of nativism, so strong in fact that it seems to make a reductio of itself (see, for instance, P.S. Churchland 1986, H. Putnam 1988, A. Clark 1994). The gist of his argument was that since learning concepts is a form of hypothesis formation and confirmation, it requires a system of mental representations in which formation and confirmation of hypotheses are to be carried out, but then there is a non-trivial sense in which one already has (albeit potentially) the resources to express the extension of the concepts to be learned.
In his LOT 2 (2008), Fodor continues to claim that concepts cannot be learned and that the very idea of concept learning is “confused”:
Now, according to HF [the Hypothesis Formation and Confirmation model], the process by which one learns C must include the inductive evaluation of some such hypothesis as ‘The C things are the ones that are green or triangular’. But the inductive evaluation of that hypothesis itself requires (inter alia) bringing the property green or triangular before the mind as such. ... Quite generally, you can't represent anything as such and such unless you already have the concept such and such. All that being so, it follows, on pain of circularity, that ‘concept learning’ as HF understands it can't be a way of acquiring concept C. ... Conclusion: If concept learning is as HF understands it, there can be no such thing. This conclusion is entirely general; it doesn't matter whether the target concept is primitive (like GREEN) or complex (like GREEN OR TRIANGULAR). (LOT 2, 2008:139)
Note that this argument and the predecessors Fodor articulated in his previous writings and especially in his (1975) are entirely general, applicable to any hypothesis that identifies concepts with mental representations whether or not these representations belong to a LOT.
The crux of the issue seems to be that learning concepts is a rational process. There seem to be non-arbitrary semantic and epistemic liaisons between the target concept to be acquired and its “evidence” base. This evidence base needs to be represented and rationally tied to the target concept. This target concept needs also to be expressed in terms of representations one already possesses. Fodor thinks that any model of concept learning understood in this sense will have to be a form of hypothesis formation and confirmation. But not every form of concept acquisition is learning. There are non-rational ways of acquiring concepts whose explanation need not be at the cognitive level (e.g., brute triggering mechanisms that can be activated in sorts of ways that can presumably be explained at the sub-cognitive or neurophysiological levels). If concepts cannot be learned, then they are either innate or non-rationally acquired. Whereas early Fodor used to think that concepts must therefore be innate (maybe he thought that non-learning concept acquisition forms are limited to sensory or certain classes of perceptual concepts), he now thinks that they may be acquired but the explanation of this is not the business of cognitive psychology.
Whatever one may think of the merits of Fodor's arguments for concept nativism or of his recent anti-learning stance, it should be emphasized that LOTH per se has very little to do with it. LOTH is not committed to such a strong version of nativism, especially about concepts. It also need not be committed to any anti-learning stance about concepts. It is certainly plausible to assume that LOTH will turn out to have some empirically (as well as theoretically/a priori) motivated nativist commitments about the structural organization and dynamic management of the entire representational system. But this much is to be expected especially in the light of recent empirical findings and trends. This, however, does not constitutes a reductio. It is an open empirical question how much nativism is true about concepts, and LOTH should be so taken as to be capable of accommodating whatever turns out to be true in this matter. LOTH, therefore, when properly conceived, is independent of any specific proposal about conceptual nativism.[11]
5. Naturalism and LOTH
One of the most attractive features of LOTH is that it is a central component of an ongoing research program in philosophy of psychology to naturalize the mind, that is, to give a theoretical framework in which the mind could naturally be seen as part of the physical world without postulating irreducibly psychic entities, events, processes or properties. Fodor, historically the most important defender of LOTH, once identified the major mysteries in philosophy of mind thus:
How could anything material have conscious states? How could anything material have semantical properties? How could anything material be rational? (where this means something like: how could the state transitions of a physical system preserve semantical properties?). (1991: 285, Reply to Devitt)
LOTH is a full-blown attempt to give a naturalist answer to the third question, an attempt to solve at least part of the problem underlying the second one, and is almost completely silent about the first.[12]
According to RTM, propositional attitudes are relations to meaningful mental representations whose causally sequenced tokenings constitute the process of thinking. This much can, in principle, be granted by an intentional realist who might nevertheless reject LOTH. Indeed, there are plenty of theorists who accept RTM in some suitable form (and also happily accept (C) in many cases) but reject LOTH either by explicitly rejecting (B) or simply by remaining neutral about it. Among some of the prominent philosophers who choose the former option are Searle (1984, 1990, 1992), Stalnaker (1984), Lewis (1972), Barwise and Perry (1983).[13] Some who want to remain neutral include Loar (1982a, 1982b), Dretske (1981), Armstrong (1980), and many contemporary functionalists including some connectionists.[14]
But RTM per se doesn't so much propose a naturalistic solution to intentionality and mechanization of thinking as simply assert a framework to emphasize intentional realism and, perhaps, with (C), a declaration of a commitment to naturalism or physicalism at best. How, then, is the addition of (B) supposed to help? Let us first try to see in a bit more detail what the problem is supposed to be in the first place to which (B) is proposed as a solution. Let us start by reflecting on thinking and see what it is about thinking that makes it a mystery in Fodor's list. This will give rise to one of the most powerful (albeit still nondemonstrative) arguments for LOTH.
RTM's second clause (A2), in effect, says that thinking is at least the tokenings of states that are (a) intentional (i.e. have representational/propositional content) and (b) causally connected. But, surely, thinking is more. There could be a causally connected series of intentional states that makes no sense at all. Thinking, therefore, is causally proceeding from states to states that makes semantic sense: the transitions among states must preserve some of their semantic properties to count as thinking. In the ideal case, this property would be the truth value of the states. But in most cases, any interesting intentional or epistemic property would do (e.g., warrantedness, degree of confirmation, semantic coherence given a certain practical context like satisfaction of goals in a specific context, etc.). In general, it is hard to spell out what this requirement of “making sense” comes to. The intuitive idea, however, should be clear. Thinking is not proceeding from thoughts to thoughts in arbitrary fashion: thoughts that are causally connected are in some fashion semantically (rationally, epistemically) connected too. If this were not so, there would be little point in thinking—thinking couldn't serve any useful purpose. Call this general phenomenon, then, the semantic coherence of causally connected thought processes. LOTH is offered as a solution to this puzzle: how is thinking, conceived this way, physically possible? This is the problem of thinking, thus the problem of mechanization of rationality in Fodor's version. How does LOTH propose to solve this problem and bring us one big step closer to the naturalization of the mind?
5.2 Syntactic Engine Driving a Semantic Engine: Computation
The two most important achievements of 20th century that are at the foundations of LOTH as well as most of modern Artificial Intelligence (AI) research and most of the so-called information processing approaches to cognition are (i) the developments in modern symbolic (formal) logic, and (ii) Alan Turing's idea of a Turing Machine and Turing computability. It is putting these two ideas together that gives LOTH its enormous explanatory power within a naturalistic framework. Modern logic showed that most of deductive reasoning can be formalized, i.e. most semantic relations among symbols can be entirely captured by the symbols' formal/syntactic properties and the relations among them. And Turing showed, roughly, that if a process has a formally specifiable character then it can be mechanized. So we can appreciate the implications of (i) and (ii) for the philosophy of psychology in this way: if thinking consists in processing representations physically realized in the brain (in the way the internal data structures are realized in a computer) and these representations form a formal system, i.e., a language with its proper combinatorial syntax (and semantics) and a set of derivations rules formally defined over the syntactic features of those representations (allowing for specific but powerful programs to be written in terms of them), then the problem of thinking, as described above, can in principle be solved in completely naturalistic terms, thus the mystery surrounding how a physical device can ever have semantically coherent state transitions (processes) can be removed. Thus, given the commitment to naturalism, the hypothesis that the brain is a kind of computer trafficking in representations in virtue of their syntactic properties is the basic idea of LOTH (and the AI vision of cognition).
Computers are environments in which symbols are manipulated in virtue of their formal features, but what is thus preserved are their semantic properties, hence the semantic coherence of symbolic processes. Slightly paraphrasing Haugeland (cf. 1985: 106), who puts the same point nicely in the form of a motto:
The Formalist Motto:
If you take care of the syntax of a representational system, its semantics will take care of itself.
This is in virtue of the mimicry or mirroring relation between the semantic and formal properties of symbols. As Dennett once put it in describing LOTH, we can view the thinking brain as a syntactically driven engine preserving semantic properties of its processes, i.e. driving a semantic engine. What is so nice about this picture is that if LOTH is true we have a naturalistically adequate causal treatment of thinking that respects the semantic properties of the thoughts involved: it is in virtue of the physically coded syntactic/formal features that thoughts cause each other while the coherence of their semantic properties is preserved precisely in virtue of this.
Whether or not LOTH actually turns out to be empirically true in the details or in its entire vision of rational thinking, this picture of a syntactic engine driving a semantic one can at least be taken to be an important philosophical demonstration of how Descartes' challenge can be met (cf. Rey 1997: chp.8). Descartes claimed that rationality in the sense of having the power “to act in all the contingencies of life in the way in which our reason makes us act” cannot possibly be possessed by a purely physical device: “The rational soul … could not be in any way extracted from the power of matter … but must … be expressly created” (1637/1970: 117–18). Descartes was completely puzzled by just this rational character and semantic coherence of thought processes so much so that he failed to even imagine a possible mechanistic explication of it. He thus was forced to appeal to Divine creation. But we can now see/imagine at least a possible mechanistic/naturalistic scenario.[15]
5.3 Intentionality and LOTH
But where do the semantic properties of the mental representations come from in the first place? How can they mean anything? This is Brentano's challenge to a naturalist. Brentano's bafflement was with the intentionality of the human mind, its apparently mysterious power to represent things, events, properties in the world. He thought that nothing physical can have this property: “The reference to something as an object is a distinguishing characteristic of all mental phenomena. No physical phenomenon exhibits anything similar” (Brentano 1874/1973: 97). This problem of intentionality is the second problem or mystery in Fodor's list quoted above. I said that LOTH officially offers only a partial solution to it and perhaps proposes a framework within which the remainder of the solution can be couched and elaborated in a naturalistically acceptable way.
Recall that RTM contains a clause (A1b) that says that the immediate “object” of a propositional attitude that P is a mental representation #P# that means that P. Again, (B1) attributes a compositional semantics to the syntactically complex symbols belonging to one's LOT that are, as per (C), realized by the physical properties of a thinking system. According to LOTH, the semantic content of propositional attitudes is inherited from the semantic content of the mental symbols. So Brentano's questions for a LOT theorist becomes: how do the symbols in one's LOT get their meanings in the first place? There are two levels or stages at which this question can be raised and answered:
(1) At the level of atomic symbols (non-logical primitives): how do the atomic symbols represent what they do?
(2) At the level of molecular symbols (phrasal complexes or sentences): how do molecular symbols represent what they do?
There have been at least two major lines LOT theorists have taken regarding these questions. The one that is least committal might perhaps be usefully described as the official position regarding LOTH's treatment of intentionality. Most LOT theorists seem to have taken this line. The official line doesn't propose any theory about the first stage, but simply assumes that the first question can be answered in a naturalistically acceptable way. In other words, officially LOTH simply assumes that the atomic symbols/expressions in one's LOT have whatever meanings they have.[16]
But, the official line continues, LOTH has a lot to say about the second stage, the stage where the semantic contents are computed or assigned to complex (molecular) symbols on the basis of their combinatorial syntax or grammar together with whatever meanings atomic symbols are assumed to have in the first stage. This procedure is familiar from a Tarski-style[17] definition of truth conditions of sentences. The truth-value of complex sentences in propositional logic are completely determined by the truth-values of the atomic sentences they contain together with the rules fixed by the truth-tables of the connectives occurring in the complex sentences. Example: ‘P and Q’ is true just in case both ‘P’ and ‘Q’ are true, but false otherwise. This process is similar but more complex in first-order languages, and even more so for natural languages—in fact, we don't have a completely working compositional semantics for the latter at the moment. So, if we have a semantic interpretation of atomic symbols (if we have symbols whose reference and extension are fixed at the first stage by whatever naturalistic mechanism turns out to govern it), then the combinatorial syntax will take over and effectively determine the semantic interpretation (truth-conditions) of the complex sentences they are constituents of. So officially LOTH would only contribute to a complete naturalization project if there is a naturalistic story at the atomic level.
Early Fodor (1975, 1978, 1978a, 1980), for instance, envisaged a science of psychology which, among other things, would reasonably set for itself the goal of discovering the combinatorial syntactic principles of LOT and the computational rules governing its operations, without worrying much about semantic matters, especially about how to fix the semantics of atomic symbols (he probably thought that this was not a job for LOTH). Similarly, Field (1978) is very explicit about the combinatorial rules for assigning truth-conditions to the sentences of the internal code. In fact, Field's major argument for LOTH is that, given a naturalistic causal theory of reference for atomic symbols, about which he is optimistic (Field 1972), it is the only naturalistic theory that has a chance of solving Brentano's puzzle. For the moment, this is not much more than a hope, but, according to the LOT theorist, it is a well-founded hope based on a number of theoretical and empirical assumptions and data. Furthermore, it is a framework defining a naturalistic research program in which there have been promising successes.[18]
As I said, this official and, in a way, least committal line has been the more standard way of conceiving LOTH's role in the project of naturalizing intentionality. But some have gone beyond it and explored the ways in which the resources of LOTH can be exploited even in answering the first question (1) about the semantics of atomic symbols.
Now, there is a weak version of an answer to (1) on the part of LOTH and a strong version. On the weak version, LOTH may be untendentiously viewed as inevitably providing some of the resources in giving the ultimate naturalistic theory in naturalizing the meaning of atomic symbols. The basic idea is that whatever the ultimate naturalistic theory turns out to be true about atomic expressions, computation as conceived by LOTH will be part of it. For instance, it may be that, as with nomic covariation theories of meaning (Fodor 1987, 1990a; Dretske 1981), the meaning of an atomic predicate may consist in its potential to get tokened in the presence of (or, in causal response to) something that instantiates the property the predicate is said to express. A natural way of explicating this potential may partly but ultimately rely on certain computational principles the symbol may be subjected to within a LOT framework, or principles that in some sense govern the “behavior” of the symbol. Insofar as computation is naturalistically understood in the way LOTH proposes, a complete answer to the first question about the semantics of atomic symbols may plausibly involve an explicatory appeal to computation within a system of symbols. This is the weak version because it doesn't see LOTH as proposing a complete solution to the first question (1) above, but only helping it.
A strong version would have it that LOTH provides a complete naturalistic solution to both questions: given the resources of LOTH we don't need to look any further to meet Brentano's challenge. The basic idea lies in so-called functional or conceptual role semantics, according to which a concept is the concept it is precisely in virtue of the particular causal/functional potential it has in interacting with other concepts. Each concept may be thought of as having a certain distinctive set of epistemic/semantic relations or liaisons to other concepts. We can conceive of this set as determining a certain “conceptual role” for each concept. We can then take these roles to determine the semantic identity of concepts: concepts are the concepts they are because they have the conceptual roles they have; that is to say, among other things, concepts represent whatever they do precisely in virtue of these roles. The idea then is to reduce each conceptual role to causal/functional role of atomic symbols (now conceived as primitive terms in LOTH), and then use the resources of LOTH to reduce it in turn to computational role. Since computation is naturalistically well-defined, the argument goes, and since causal interactions between thoughts and concepts can be understood completely in terms of computation, we can completely naturalize intentionality if we can successfully treat meanings as arising out of thoughts/concepts' internal interactions with each other. In other words, the strong version of LOTH would claim that atomic symbols in LOT have the content they do in virtue of their potential for causal interactions with other tokens, and cashing out this potential in mechanical/naturalistic terms is what, among other things, LOTH is for. LOTH then comes as a naturalistic rescuer for conceptual role semantics.
It is not clear whether any one holds this strong version of LOTH in this rather naive form. But certainly some people have elaborated the basic idea in quite subtle ways, for which Cummins (1989: chp.8) is perhaps the best example. (But also see Block 1986 and Field 1978.) But even in the best hands, the proposal turns out to be very problematic and full of difficulties nobody seems to know how to straighten out. In fact, some of the most ardent critics of taking LOTH as incorporating a functional role semantics turn out to be some of the most ardent defenders of LOTH understood in a weak, non-committal sense we have explored above—see Fodor (1987: chp.3), Fodor and Lepore (1991), Fodor's attack (1978b) on AI's way of doing procedural semantics is also relevant here. Haugeland (1981), Searle (1980, 1984), and Putnam (1988) quite explicitly take LOTH to involve a program for providing a complete semantic account of mental symbols, which they then attack accordingly.[19]
It is also possible, in fact, quite natural, to combine conceptual role semantics (internalist) with causal/informational psychosemantics (externalist). The result is sometimes known as two-factor theories. If this turns out to be the right way to naturalize intentionality, then, given what is said above about the potential resources of LOTH in contributing to both factors, it is easy to see why many theorists who worry about naturalizing intentionality are attracted to LOTH.
As indicated previously, LOTH is almost completely silent about consciousness and the problem of qualia, the third mystery in Fodor's list in the quote above. But the naturalist's hope is that this problem too will be solved, if not by LOTH, then by something else. On the other hand, it is important to emphasize that LOTH is neutral about the naturalizability of consciousness/qualia. If it turns out that qualia cannot be naturalized, this would by no means show that LOTH is false or defective in some way. In fact, there are people who seem to think that LOTH may well turn out to be true even though qualia can perhaps not be naturalized (e.g., Block 1980, Chalmers 1996, McGinn 1991).
Finally, it should be emphasized that LOTH has no particular commitment to every symbolic activity's being conscious. Conscious thoughts and thinking may be the tip of a computational iceberg. Nevertheless, there are ways in which LOTH can be helpful for an account of state consciousness that seeks to explain a thought's being conscious in terms of a higher order thought which is about the first order thought. So, to the extent to which thought and thinking are conscious, to that extent LOTH can perhaps be viewed as providing some of the necessary resources for a naturalistic account of state consciousness—for elaboration see Rosenthal (1997) and Lycan (1997).
6. Arguments for LOTH
We have already seen two major arguments, perhaps historically the most important ones, for LOTH: First, we have noted that if LOTH is true then all the essential features of the common sense conception of propositional attitudes will be explicated in a naturalistic framework which is likely to be co-opted by scientific cognitive psychology, thus vindicating folk psychology. Second, we have discussed that, if true, LOTH would solve one of the mysteries about thinking minds: how is thinking (as characterized above) possible? How is rationality mechanically possible? Then we have also seen a third argument that LOTH would partially contribute to the project of naturalizing intentionality by offering an account of how the semantic properties of whole attitudes are fixed on the basis of their atomic constituents. But there have been many other arguments for LOTH. In this section, I will describe only those arguments that have been historically more influential and controversial.
When Fodor first formulated LOTH with significant elaboration in his (1975), he introduced his major argument for it along with its initial formulation in the first chapter. It was basically this: our best scientific theories and models of different aspects of higher cognition assume a framework that requires a computational/representational medium for them to be true. More specifically, he analyzed the basic form of the information processing models developed to account for three types of cognitive phenomena: perception as the fixation of perceptual beliefs, concept learning as hypothesis formation and confirmation, and decision making as a form of representing and evaluating the consequences of possible actions carried out in a situation with a preordered set of preferences. He rightly pointed out that all these psychological models treated mental processes as computational processes defined over representations. Then he drew what seems to be the obvious conclusion: if these models are right in at least treating mental processes as computational, even if not in detail, then there must be a LOT over which they are defined, hence LOTH.
In Fodor's (1975), the arguments for different aspects of LOTH are diffused and the emphasis, with the book's slogan “no computation without representation”, is put on the RTM rather than on (B) or (C). But all the elements are surely there.
People seem to be capable of entertaining an infinite number of thoughts, at least in principle, although they in fact entertain only a finite number of them. Indeed adults who speak a natural language are capable of understanding sentences they have never heard uttered before. Here is one: there is a big lake of melted gold on the dark side of the moon. I bet that you have never heard this sentence before, and yet, you have no difficulty in understanding it: it is one you in fact likely believe false. But this sentence was arbitrary, there are infinitely many such sentences I can in principle utter and you can in principle understand. But understanding a sentence is to entertain the thought/proposition it expresses. So there are in principle infinitely many thoughts you are capable of entertaining. This is sometimes expressed by saying that we have an unbounded competence in entertaining different thoughts, even though we have a bounded performance. But this unbounded capacity is to be achieved by finite means. For instance, storing an infinite number of representations in our heads is out of the question: we are finite beings. If human cognitive capacities (capacities to entertain an unbounded number of thoughts, or to have attitudes towards an unbounded number of propositions) are productive in this sense, how is this to be explained on the basis of finitary resources?
The explanation LOTH offers is straightforward: postulate a representational system that satisfies at least (B1). Indeed, recursion is the only known way to produce an infinite number of symbols from a finite base. In fact, given LOTH, productivity of thought as a competence mechanism seems to be guaranteed.[20]
Systematicity of thought consists in the empirical fact that the ability to entertain certain thoughts is intrinsically connected to the ability to entertain certain others. Which ones? Thoughts that are related in a certain way. In what way? There is a certain initial difficulty in answering such questions. I think, partly because of this, Fodor (1987) and Fodor and Pylyshyn (1988), who are the original defenders of this kind of argument, first argue for the systematicity of language production and understanding: the ability to produce/understand certain sentences is intrinsically connected to the ability to produce/understand certain others. Given that a mature speaker is able to produce/understand a certain sentence in her native language, by psychological law, there always appear to be a cluster of other sentences that she is able to produce/understand. For instance, we don't find speakers who know how to express in their native language the fact that John loves the girl but not the fact that the girl loves John. This is apparently so, moreover, for expressions of any n-place relation.
Fodor and Pylyshyn bring out the force of this psychological fact by comparing learning languages the way we actually do with learning a language by memorizing a huge phrase book. In the phrase book model, there is nothing to prevent someone learning how to say ‘John loves the girl’ without learning how to say ‘the girl loves John.’ In fact, that is exactly the way some information booklets prepared for tourists help them to cope with their new social environment. You might, for example, learn from a phrase book how to say ‘I'd like to have a cup of coffee with sugar and milk’ in Turkish without knowing how to say/understand absolutely anything else in Turkish. In other words, the phrase book model of learning a language allows arbitrarily punctate linguistic capabilities. In contrast, a speaker's knowledge of her native language is not punctate, it is systematic. Accordingly, we do not find, by nomological necessity, native speakers whose linguistic capacities are punctate.
Now, how is this empirical truth (in fact, a law-like generalization) to be explained? Obviously if this is a general nomological fact, then learning one's native language cannot be modeled on the phrase book model. What is the alternative? The alternative is well known. Native speakers master the grammar and vocabulary of their language. But this is just to say that sentences are not atomic, but have syntactic constituent structure. If you have a vocabulary, the grammar tells you how to combine systematically the words into sentences. Hence, in this way, if you know how to construct a particular sentence out of certain words, you automatically know how to construct many others. If you view all sentences as atomic, then, as Fodor and Pylyshyn say, the systematicity of language production/understanding is a mystery, but if you acknowledge that sentences have syntactic constituent structure, systematicity of linguistic capacities is what you automatically get; it is guaranteed. This is the orthodox explanation of linguistic systematicity.
From here, according to Fodor and Pylyshyn, establishing the systematicity of thought as a nomological fact is one step away. If it is a law that the ability to understand a sentence is systematically connected to the ability to understand many others, then it is similarly a law that the ability to think a thought is systematically connected to the ability to think many others. For to understand a sentence is just to think the thought/proposition it expresses. Since, according to RTM, to think a certain thought is just to token a representation in the head that expresses the relevant proposition, the ability to token certain representations is systematically connected to the ability to token certain others. But then, this fact needs an adequate explanation too. The classical explanation LOTH offers is to postulate a system of representations with combinatorial syntax exactly as in the case of the explanation of the linguistic systematicity. This is what (B1) offers.[21] This seems to be the only explanation that does not make the systematicity of thought a miracle, and thus argues for the LOT hypothesis.
However, thought is not only systematic but also compositional: systematically connected thoughts are also always semantically related in such a way that the thoughts so related seem to be composed out of the same semantic elements. For instance, the ability to think ‘John loves the girl’ is connected to the ability to think ‘the girl loves John’ but not to, say, ‘protons are made up of quarks’ or to ‘2+2=4.’ Why is this so? The answer LOTH gives is to postulate a combinatorial semantics in addition to a combinatorial syntax, where an atomic constituent of a mental sentence makes (approximately) the same semantic contribution to any complex mental expression in which it occurs. This is what Fodor and Pylyshyn call ‘the principle of compositionality’.[22]
In brief, it is an argument for LOTH that it offers a cogent and principled solution to the systematicity and compositionality of cognitive capacities by postulating a system of representations that has a combinatorial syntax and semantics, i.e., a system of representations that satisfies at least (B1).
6.4 Argument from the Systematicity of Thinking (Inferential Coherence)
Systematicity of thought does not seem to be restricted solely to the systematic ability to entertain certain thoughts. If the system of mental representations does have a combinatorial syntax, then there is a set of rules, psychosyntactic formation rules, so to speak, that govern the construction of well-formed expressions in the system. It is this fact, (B1), that guarantees that if you can form a mental sentence on the basis of certain rules, then you can also form many others on the basis of the same rules. The rules of combinatorial syntax determine the syntactic or formal structure of complex mental representations. This is the formative (or, formational) aspect of systematicity. But inferential thought processes ( i.e., thinking) seem to be systematic too: the ability to make certain inferences is intrinsically connected to the ability to make certain many others. For instance, you do not find minds that can infer ‘A’ from ‘A&B’ but cannot infer ‘C’ from ‘A&B&C.’ It seems to be a psychological fact that inferential capacities come in clusters that are homogeneous in certain aspects. How is this fact (i.e., the inferential or transformational systematicity) to be explained?
As we have seen, the explanation LOTH offers depends on the exploitation of the notion of logical form or syntactic structure determined by the combinatorial syntax postulated for the representational system. The combinatorial syntax not only gives us a criterion of well-formedness for mental expressions, but it also defines the logical form or syntactic structure for each well-formed expression. The classical solution to inferential systematicity is to make the mental operations on representations sensitive to their form or structure, i.e., to insist on (B2). Since, from a syntactic view point, similarly formed expressions will have similar forms, it is possible to define a single operation which will apply to only certain expressions that have a certain form, say, only to conjunctions, or conditionals. This allows the LOT theorist to give homogeneous explanations of what appear to be homogeneous classes of inferential capacities. This is one of the greatest virtues of LOTH, hence provides an argument for it.
The solution LOTH offers for what I called the problem of thinking, above, is connected to the argument here because the two phenomena are connected in a deep way. Thinking requires that the logico-semantic properties of a particular thought process be somehow causally implicated in the process (say, inferring that John is happy from knowing that if John is at the beach then John is happy and coming to realize that John is indeed at the beach). The systematicity of inferential thought processes then is based on the observation that if the agent is capable of making that particular inference, then she is capable of making many other somehow similarly organized inferences. But the idea of similar organization in this context seems to demand some sort of classification of thoughts independently of their particular content. But what can the basis of such a classification be? The only basis seems to be the logico-syntactic properties of thoughts, their form. Although it feels a little uneasy to talk about syntactic properties of thoughts common-sensically understood, it seems that they are forced upon us by the very attempt to understand their semantic properties: how, for instance, could we explain the semantic content of the thought that if John is at the beach then he is happy without somehow appealing to its being a conditional? This is the point of contact between the two phenomena. Especially when the demands of naturalism are added to this picture, inferring a LOT (= a representational system satisfying B) realized in the brain becomes almost irresistible. Indeed Rey (1995) doesn't resist and claims that, given the above observations, LOTH can be established on the basis of arguments that are not “merely empirical”. I leave it to the reader to evaluate whether mere critical reflection on our concepts of thought and thinking (along with certain mundane empirical observations about them) can be sufficient to establish LOTH.[23]
7. Objections to LOTH
There have been numerous arguments against LOTH. Some of them are directed more specifically against the Representational Theory of Mind (A), some against functionalist materialism (C). Here I will concentrate only on those arguments specifically targeting (B)—the most controversial component of LOTH.
These arguments rely on the explanations offered by LOTH defenders for certain aspects of natural languages. In particular, many LOT theorists advert to LOTH to explain (1) how natural languages are learned, (2) how natural languages are understood, or (3) how the utterances in such languages can be meaningful. For instance, according to Fodor (1975), natural languages are learned by forming and confirming hypotheses about the translation of natural language sentences into Mentalese such as: ‘Snow is white’ is true in English if and only if P, where ‘P’ is a sentence in one's LOT. But to be able to do that, one needs a representational medium in which to form and confirm hypotheses—at least to represent the truth-conditions of natural language sentences. The LOT is such a medium. Again, natural languages are understood because, roughly, such an understanding consists in translating their sentences into one's Mentalese. Similarly, natural language utterances are meaningful in virtue of the meanings of corresponding Mentalese sentences.
The basic complaint is that in each of these cases, either the explanations generate a regress because the same sort of explanations ought to be given for how the LOT is learned, understood or can be meaningful, or else they are gratuitous because if a successful explanation can be given for LOT that does not generate a regress then it could and ought to be given for the natural language phenomena without introducing a LOT (see, e.g., Blackburn 1984). Fodor's response in (1975) is (1) that LOT is not learned, it's innate; (2) that it's understood in a different sense than the sense involved in natural language comprehension; (3) that LOT sentences acquire their meanings not in virtue of another meaningful language but in a completely different way, perhaps by standing in some sort of causal relation to what they represent or by having certain computational profiles (see above, §5.3). For many who have a Wittgensteinian bent, these replies are not likely to be convincing. But here the issues tend to concern RTM rather than (B).
Laurence and Margolis (1997) point out that the regress arguments depend on the assumption that LOTH is introduced only to explain (1)-(3). If it can be shown that there are lots of other empirical phenomena for which the LOTH provides good explanations, then the regress arguments fail because LOTH then would not be gratuitous. In fact, as we have seen above, there are plenty of such phenomena. But still it is important to realize that the sort of explanations proposed for the understanding of one's LOT (computational use/activity of LOT sentences with certain meanings) and how LOT sentences can be meaningful (computational roles and/or nomic relations with the world) cannot be given for (1)-(3): it's unclear, for example, what it would be like to give a computational role and/or nomic relation account for the meanings of natural language utterances. (See Knowles 1998 for a reply to Laurence & Margolis 1997; Margolis & Laurence 1998 counterreplies to Knowles.)
Dennett in his review of Fodor's (1975) has raised the following objection (cf. Fodor 1987: 21–3 for a similar discussion):
In a recent conversation with the designer of a chess-playing program I heard the following criticism of a rival program: “it thinks it should get its queen out early.” This ascribes a propositional attitude to the program in a very useful and predictive way, for as the designer went on to say, one can usefully count on chasing that queen around the board. But for all the many levels of explicit representation to be found in that program, nowhere is anything roughly synonymous with “I should get my queen out early” explicitly tokened. The level of analysis to which the designer's remark belongs describes features of the program that are, in an entirely innocent way, emergent properties of the computational processes that have “engineering reality.” I see no reason to believe that the relation between belief-talk and psychological talk will be any more direct. (Dennett 1981: 107)
The objection, as Fodor (1987: 22) points out, isn't that the program has a dispositional, or potential, belief that it will get its queen out early. Rather, the program actually operates on this belief. There appear to be lots of other examples: e.g., in reasoning we pretty often follow certain inference rules like modus ponens, disjunctive syllogism, etc., without necessarily explicitly representing them.
The standard reply to such objections is to draw a distinction between rules on the basis of which Mentalese data-structures are manipulated, and the data-structures themselves (intuitively, the program/data distinction). LOTH is not committed to every rule's being explicitly represented. In fact, as a point of nomological fact, in a computational device not every rule can be explicitly represented: some have to be hard-wired and, thus, implicit in this sense. In other words, LOTH permits but doesn't require that rules be explicitly represented. On the other hand, data structures have to be explicitly represented: it is these that are manipulated formally by the rules. No causal manipulation is possible without explicit tokening of these structures. According to Fodor, if a propositional attitude is an actual episode in one's reasoning that plays a causal role, then LOTH is committed to explicit representation of its content, which is as per (A2 and B2) causally implicated in the physical process realizing that reasoning. Dispositional propositional attitudes can then be accounted for in terms of an appropriate principle of inferential closure of explicitly represented propositional attitudes (cf. Lycan 1986).
Dennett's chess program certainly involves explicit representations of the chess board, the pieces, etc. and perhaps some of the rules. Which rules are implicit and which are explicit depend on the empirical details of the program. Pointing to the fact that there may be some rules that are emergent out of the implementation of explicit rules and data-structures does not suffice to undermine LOTH.
In any sufficiently complex computational system, there are bound to be many symbol manipulations with no obviously corresponding description at the level of propositional attitudes. For instance, when a multiplication program is run through a standard conventional computer, the steps of the program are translated into the computer's machine language and executed there, but at this level the operations apply to 1's and 0's with no obvious way to map them onto the original numbers to be multiplied or to the multiplication operation. So it seems that at those levels that, according to Dennett, have engineering reality there are plenty of explicit tokenings of symbols with appropriate operations over them that don't correspond to anything like the propositional attitudes of folk psychology. In other words, there is plenty of symbolic activity which it would be wrong to say a person engages in. Rather, they are done by the person's subpersonal computational components as opposed to the person. How to rule out such cases? (cf. Fodor 1987: 23–6 for a similar discussion.)
They are ruled out by an appropriate reading of (A1) and (B1): (A1) says that the person herself must stand in an appropriate computational relation to a Mentalese sentence, which, as per (B1), has a suitable syntax and semantics. Only then will the sentence constitute the person's having a propositional attitude. Not all explicit symbols in one's LOT will satisfy this. In other words, not every computational routine will correspond to a processing appropriately described as storage in, e.g., the “belief-box”. Furthermore, as pointed out by Fodor (1987), LOTH would vindicate the common sense view of propositional attitudes if they turn out to be computational relations to Mentalese sentences. It may not be further required that every explicit representation correspond to a propositional attitude.
There have been many other objections to LOTH in recent years raised especially by connectionists: that LOT systems cannot handle certain cognitive tasks like perceptual pattern recognition, that they are too brittle and not sufficiently damage resistant, that they don't exhibit graceful degradation when physically damaged or as a response to noisy or degraded input, that they are too rigid, deterministic, so are not well-suited for modeling humans' capacity to satisfy multiple soft-constraints so gracefully, that they are not biologically realistic, and so on. (For useful discussions of these and many similar objections, see Rumelhart, McClelland and the PDP Research Group (1986), Fodor and Pylyshyn (1988), Horgan and Tienson (1996), Horgan (1997), McLaughlin and Warfield (1994), Bechtel and Abrahamsen (2002), Marcus (2002).)
8. The Connectionism/Classicism Debate
When Jerry Fodor published his influential book, The Language of Thought, in (1975), he called LOTH “the only game in town.” As we have seen, it was the philosophical articulation of the assumptions that underlay the new developments in “cognitive sciences” after the demise of behaviorism. Fodor argued for the truth of LOTH on the basis of the successes of the best scientific theories we had then. Indeed most of the scientific work in cognitive psychology, psycholinguistics, and AI assumed the framework of LOTH.
In the early 1980's, however, Fodor's claim that LOTH was the only game in town was beginning to be challenged by some who were working on so-called connectionist networks. They claimed that connectionism offered a new and radically different alternative to classicism in modeling cognitive phenomena. The name ‘classicism’ has since then become to be applied to the LOTH framework. On the other hand, many classicists like Fodor thought that connectionism was nothing but a slightly more sophisticated way with which the old and long dead associationism, whose roots could be traced back to early British empiricists, was being revived. In 1988 Fodor and Pylyshyn (F&P) published a long article, “Connectionism and Cognitive Architecture: A Critical Analysis”, in which they launched a formidable attack on connectionism, which largely set the terms for the ensuing debate between connectionists and classicists.
F&P's forceful criticism consists in posing a dilemma for connectionists: They either fail to explain the law-like cognitive regularities like systematicity and productivity in an adequate way or the connectionist models are nothing but mere implementation models of classical architectures; hence, they fail to provide a radically new paradigm as connectionists claim. This conclusion was also meant to be a challenge: Explain the cognitive regularities in question without postulating a LOT architecture.
First, let me present F&P's argument against connectionism in a somewhat reconstructed fashion. It will be helpful to characterize the debate by locating the issues according to the reactions many connectionists had to the premises of the argument.
F&P's Argument against Connectionism in their (1988) article:
Cognition essentially involves representational states and causal operations whose domain and range are these states; consequently, any scientifically adequate account of cognition should acknowledge such states and processes.
Higher cognition (specifically, thought and thinking with propositional content) conceived in this way, has certain empirically interesting properties: in particular, it is a law of nature that cognitive capacities are productive, systematic, and inferentially coherent.
Accordingly, the architecture of any proposed cognitive model is scientifically adequate only if it guarantees that cognitive capacities are productive, systematic, etc. This would amount to explaining, in the scientifically relevant and required sense, how it could be a law that cognition has these properties.
The only way (i.e., necessary condition) for a cognitive architecture to guarantee systematicity (etc.) is for it to involve a representational system for which (B) is true (see above). (Classical architectures necessarily satisfy (B).)
Either the architecture of connectionist models does satisfy (B), or it does not.
If it does, then connectionist models are implementations of the classical LOT architecture and have little new to offer (i.e., they fail to compete with classicism, and thus connectionism does not constitute a radically new way of modeling cognition).
If it does not, then (since connectionism does not then guarantee systematicity, etc., in the required sense) connectionism is empirically false as a theory of the cognitive architecture.
Therefore, connectionism is either true as an implementation theory, or empirically false as a theory of cognitive architecture.
The notion of cognitive architecture assumes special importance in this debate. F&P's characterization of the notion goes as follows:
The architecture of the cognitive system consists of the set of basic operations, resources, functions, principles, etc. (generally the sorts of properties that would be described in a “user's manual” for that architecture if it were available on a computer) whose domain and range are the representational states of the organism. (1988: 10)
Also, note that (B1) and (B2) are meta-architectural properties in that they are themselves conditions upon any specific architecture's being classical. They define classicism per se, but not any particular way of being classical. Classicism as such simply claims that whatever the particular cognitive architecture of the brain might turn out to be (whatever the specific grammar of Mentalese turns out to be), (B) must be true of it. F&P claim that this is the only way an architecture can be said to guarantee the nomological necessity of cognitive regularities like systematicity, etc. This seems to be the relevant and required sense in which a scientific explanation of cognition is required to guarantee the regularities—hence the third premise in their argument.
Connectionist responses have fallen into four classes:
Deny premise(i). The rejection of (i) commits connectionists to what is sometimes called radical or eliminativist connectionism. Premise (i), as F&P point out, draws a general line between eliminativism and representationalism (or, intentional realism). There has been some controversy as to whether connectionism constitutes a serious challenge to the fundamental tenets of folk psychology.[24] Although it may still be too early for assessment,[25] the connectionist research program has been overwhelmingly cognitivist: most connectionists do in fact advance their models as having causally efficacious representational states, and explicitly endorse F&P's first premise. So they seem to accept intentional realism.[26]
Accept the conclusion. This group may be seen as more or less accepting the cogency of the entire argument, and characterizes itself as implementationalist: they hold that connectionist networks will implement a classical architecture or language of thought. According to this group, the appropriate niche for neural networks is closer to neuroscience than to cognitive psychology. They seem to view the importance of the program in terms of its prospects of closing the gap between the neurosciences and high-level cognitive theorizing. In this, many seem content to admit premise (vi). (See Marcus 2001 for a discussion of the virtues of placing connectionist models closer to implementational level.)
Deny premise (ii) or (iv). Some connectionists reject (ii) or (iv),[27] holding that there are no lawlike cognitive regularities such as systematicity (etc.) to be explained, or that such regularities do not require a (B)-like architecture for their explanation. Those who question (ii) often question the empirical evidence for systematicity (etc.) and tend to ignore the challenge put forward by F&P. Those who question (iv) also often question (ii), or they argue that there can be very different sort of explanations for systematicity and the like (e.g. evolutionary explanations, see Braddon-Mitchell and Fitzpatrick 1990), or they question the very notion of explanation involved (e.g. Matthews 1994). There are indeed quite a number of different kinds of arguments in the literature against these premises.[28] For a sampling, see Aydede (1995) and McLaughlin (1993b), who partitions the debate similarly.
Deny premise (vi). The group of connectionists who have taken F&P's challenge most seriously has tended to reject premise (vi) in their argument, while accepting, on the face of it, the previous five premises (sometimes with reservations on the issue of productivity). They think that it is possible for connectionist representations to be syntactically structured in some sense without being classical. Prominent in this group are Smolensky (1990a, 1990b, 1995), van Gelder (1989, 1990, 1991), Chalmers (1990, 1993).[29] Some connectionists whose models give support to this line include Elman (1989), Hinton (1990), Touretzky (1990), Pollack (1990), Barnden and Srinivas (1991), Shastri and Ajjanagadde (1993), Plate (1998), Hummel et al. (2004), Van Der Velde and De Kamps (2006), Barrett et al. (2008), Sanjeevi and Bhattacharyya (2010).
Much of the recent debate between connectionists and classicists has focused on this option. How is it possible to reject premise (vi), which seems true by definition of classicism. The connectionists' answer, roughly put, is that when you devise a representational system whose satisfaction of (B) relies on a non-concatenative realization of structural/syntactic complexity of representations, you have a non-classical system. (See especially Smolensky 1990a and van Gelder 1990.) Interestingly, some classicists like Fodor and McLaughlin (1990) (F&M) seem to agree. F&M stipulate that you have a classical system only if the syntactic complexity of representations is realized concatenatively, or as it is sometimes put, explicitly:
We … stipulate that for a pair of expression types E1, E2, the first is a Classical constituent of the second only if the first is tokened whenever the second is tokened. (F&M 1990: 186)
The issues about how connectionists propose to obtain constituent structure non-concatenatively tend to be complex and technical. But they propose to exploit so called distributed representations in certain novel ways. The essential idea behind most of them is to use vector (and tensor) algebra (involving superimposition, multiplication, etc. of vectors) in composing and decomposing connectionist representations which consist in coding patterns of activity across neuron-like units which can be modeled as vectors. The result of such techniques is the production of representations that have in some interesting sense a complexity whose constituent structure is largely implicit in that the constituents are not tokened explicitly when the representations are tokened, but can be recovered by further operations upon them. The interested reader should consult some of the pioneering work by Elman (1989), Hinton (1990), Smolensky (1989, 1990, 1995), Touretzky (1990), Pollack (1990).
F&M's criticism, more specifically stated, however, is this. Connectionists with such techniques only satisfy (B1) in some “extended sense”, but they are incapable of satisfying (B2), precisely because their way of satisfying (B1) is committed to a non-concatenative realization of syntactic structures.
Some connectionists disagree (e.g., Chalmers 1993, Niklasson and van Gelder 1994—see also Browne 1998 and Browne and Sun 2001 for discussion and overview of models): they claim that you can have structure-sensitive transformations or operations defined over representations whose syntactic structure is non-concatenatively realized. So given the apparent agreement that non-concatenative realization is what makes a system non-classical, connectionists claim that they can and do perfectly satisfy (B) in its entirety with their connectionist models without implementing classical models.
The debate still continues and there is a growing literature built around the many issues raised by it. Aydede (1997a) offers an extensive analysis of the debate between classicists and this group of connectionists with special attention to the conceptual underpinnings of the debate. (See also Roth 2005 who argues that to the extent to which connectionist models can transform representations successfully according to an algorithmic function, to that extent they count as executing program in the sense relevant to classical program execution.) Aydede argues that both parties are wrong in assuming that concatenative realization is relevant to the characterization of LOTH. Part of the argument is that concatenative realization of (B) is just that—a realization. The attentive reader might have noticed that there is nothing in the characterization of (B) that requires concatenative realization. Indeed, when we look at all the major arguments for LOTH focused on the need for (B), none of them requires concatenation or explicit realization of syntactic structure. In fact, it is almost on the border of confusion to necessarily associate LOTH to such an implementational level issue. If anything, this class of connectionist networks, if successful and generalizable across all higher cognition, contributes to our understanding of how radically differently a LOTH architecture could be implemented in neural networks. Indeed, if these models prove to be adequate for explaining the full range of human cognitive capacities, they would show how syntactically structured representations and structure sensitive processes could be implemented in a radically new way. So research programs in this niche are by no means trivial or insignificant. But we need to be clear and careful about what minimally needs to be the case for LOTH to be true, and why.
On the other hand, it is by no means clear that these connectionist models are successful and generalizable (scalable). They all have proved to have serious limitations that seem to be tied to their particular ways of implementing variable binding (syntactic structure) and structure sensitive processing. For critical discussion, see Marcus (2001), Hadley (2009), Browne and Sun (2001). Marcus in particular makes a strong and largely empirical case for why classical symbol systems are needed for explaining human capacities of variable binding and generalizing, and why existing connectionist models aren't up to the job to match human capacities while remaining non-classical. Indeed the trend in the last fifteen years seems to be towards developing hybrid systems combining connectionist and classical symbol processing models—see, for instance, the articles in Wermter and Sun (2000).
Academic ToolsAristotle’s logic, especially his theory of the syllogism, has had an unparalleled influence on the history of Western thought. It did not always hold this position: in the Hellenistic period, Stoic logic, and in particular the work of Chrysippus, took pride of place. However, in later antiquity, following the work of Aristotelian Commentators, Aristotle’s logic became dominant, and Aristotelian logic was what was transmitted to the Arabic and the Latin medieval traditions, while the works of Chrysippus have not survived.
This unique historical position has not always contributed to the understanding of Aristotle’s logical works. Kant thought that Aristotle had discovered everything there was to know about logic, and the historian of logic Prantl drew the corollary that any logician after Aristotle who said anything new was confused, stupid, or perverse. During the rise of modern formal logic following Frege and Peirce, adherents of Traditional Logic (seen as the descendant of Aristotelian Logic) and the new mathematical logic tended to see one another as rivals, with incompatible notions of logic. More recent scholarship has often applied the very techniques of mathematical logic to Aristotle’s theories, revealing (in the opinion of many) a number of similarities of approach and interest between Aristotle and modern logicians.
This article is written from the latter perspective. As such, it is about Aristotle’s logic, which is not always the same thing as what has been called “Aristotelian” logic.The economic framework that each society has — its laws, institutions, policies, etc. — results in different distributions of economic benefits and burdens across members of the society. These economic frameworks are the result of human political processes and they constantly change both across societies and within societies over time. The structure of these frameworks is important because the economic distributions resulting from them fundamentally affect people's lives. Arguments about which frameworks and/or resulting distributions are morally preferable constitute the topic of distributive justice. Principles of distributive justice are therefore best thought of as providing moral guidance for the political processes and structures that affect the distribution of economic benefits and burdens in societies.
This entry is structured in the following way. After outlining the scope of the entry and the role of distributive principles, the first relatively simple principle of distributive justice examined is Strict Egalitarianism, which calls for the allocation of equal material goods to all members of society. John Rawls' alternative distributive principle, which he calls the Difference Principle, is examined next. The Difference Principle permits diverging from strict equality so long as the inequalities in question would make the least advantaged in society materially better off than they would be under strict equality. Some have thought that neither strict equality nor Rawls' Difference Principle capture the important moral roles of luck and responsibility in economic life. The “Luck Egalitarianism” literature comprises varying attempts to design distributive principles that are appropriately sensitive to considerations of responsibility and luck in economic life. Desert-based principles similarly emphasize the moral roles of responsibility and luck but are distinct because they approach these factors through claims about what people deserve because of their work.
Advocates of welfare-based principles (of which utilitarianism is the most famous) do not believe the primary distributive concern should be material goods and services. They argue that material goods and services have no intrinsic value but are valuable only in so far as they increase welfare. Hence, they argue, distributive principles should be designed and assessed according to how they affect welfare, either its maximization or distribution. Advocates of libertarian principles, by contrast to each of the principles so far mentioned, generally criticize any distributive ideal that requires the pursuit of economic ‘patterns’, such as maximization or equality of welfare or of material goods. They argue that the pursuit of such patterns conflicts with the more important moral demands of liberty or self-ownership. Finally, feminist critiques of existing distributive principles note that they tend to ignore the particular circumstances of women, so feminists tend to argue for principles which are more sensitive to facts such as that women often have primary responsibility for child-rearing and on average, spend less of their lifetimes than men in the market economy.
Related Entries
1. Scope and Role of Distributive Principles
Distributive principles vary in numerous dimensions. They vary in what is considered relevant to distributive justice (income, wealth, opportunities, jobs, welfare, utility, etc.); in the nature of the recipients of the distribution (individual persons, groups of persons, reference classes, etc.); and on what basis the distribution should be made (equality, maximization, according to individual characteristics, according to free transactions, etc.). In this entry, the focus is on principles designed to cover the distribution of benefits and burdens of economic activity among individuals in a society. Although principles of this kind have been the dominant source of Anglo-American debate about distributive justice over the last five decades, there are other important distributive justice questions, some of which are covered by other entries in the encyclopedia. These include questions of distributive justice at the global level rather than just at the national level (see justice: international), distributive justice across generations (see justice: intergenerational) and how the topic of distributive justice can be approached, not as a set of principles but as a virtue (see justice: as a virtue).
Although the numerous distributive principles vary along different dimensions, for simplicity, they are presented here in broad categories. Even though these are common classifications in the literature, it is important to keep in mind they necessarily involve over-simplification, particularly with respect to the criticisms of each of the groups of principles. Some criticisms may not apply equally to every principle in the group. The issue of how we are to understand and respond to criticisms of distributive principles is discussed briefly in the final section on methodology (see Methodology).
Throughout most of history, people were born into, and largely stayed in, a fairly rigid economic position. The distribution of economic benefits and burdens was normally seen as fixed, either by nature or by God. Only when there was a widespread realization that the distribution of economic benefits and burdens could be affected by government did distributive justice become a live topic. Now the topic is unavoidable. Governments continuously make and change laws and policies affecting the distribution of economic benefits and burdens in their societies. Almost all changes, whether they regard tax, industry, education, health, etc. have distributive effects. As a result, every society has a different distribution at any point in time and we are becoming increasingly more adept at measuring that distribution. More importantly, at every point in time now, each society is faced with a choice about whether to stay with current laws, policies, etc. or to modify them. The practical contribution of distributive justice theory is to provide moral guidance for these constant choices.
Partly because many writers on distributive justice tend to advocate their particular principles by describing or considering ideal societies operating under them, some readers may be misled to believe that discussions of distributive justice are merely exercises in ideal theory. This is unfortunate because, in the end, distributive justice theory is a practical enterprise. It is important to acknowledge that there has never been, and never will be, a purely libertarian society or Rawlsian society, or any society whose distribution conforms to one of the proposed principles, so rather than guiding ideal societies, distributive principles provide moral guidance for the choices that each society faces now and every year. So, for instance, advocates of Rawls' Difference Principle are arguing that we should change our institutions to improve the life prospects of the least advantaged in society. Others are arguing for changes to bring economic benefits and burdens more in accordance with what people deserve. Libertarians are arguing that reductions in government intervention in the economy will better respect liberty and/or self-ownership of its citizens. Sometimes a number of the theories will recommend the same changes; other times they will diverge.
Another popular misconception about distributive justice is that it is readily avoidable by the population and/or governments. This reveals a confusion about the nature of the choices always facing each society. To claim that we should not pursue any changes to our economic structures in light of a distributive justice argument is, by its very nature, to take a stand on the distributive justice of (or, if one prefers, the ‘morality’ of) the current distribution and structures in the society compared to any of the possible alternative distributions and structures practically available. At any particular moment the current economic framework is fundamentally affecting the economic and life prospects of everybody in the society. To assert that we should not change the current system is to take a substantive moral stance on that distribution — it is to say that it is morally preferable to any practical alternative proposed. The ‘should’ here has to be a moral should and if it is to be anything other than a bald assertion then the only type of argument that can back it up is one within the broad distributive justice tradition (which includes principles which do not use ‘justice’ per se, such as utilitarianism, but which are moral principles relating to distribution just the same). So societies cannot avoid taking positions about distributive justice.
A related point can be made when people assert that economic structures and policy should be left to economists, or when people assert that economic policy can be pursued without reference to distributive justice. These assertions reveal misconceptions about what distributive justice and economics are, and how they are related. Positive economics, at its best, can tell us about economic causes and effects. Positive economics is very important for distributive justice because it can give us guidance about which changes to pursue in order to better instantiate our moral principles. What it cannot do, in the absence of the principles, is tell us what we should do. This point is easily lost in everyday political discussion. When an economist says ‘The Central Bank should raise interest rates’ the general population often mistakenly believes the recommendation is purely coming from the science of economics. It is not. Moreover the ‘should’ is almost always a moral ‘should’. When economists make such a recommendation, they, sometimes unconsciously, have taken off their social scientific hat. They are employing alongside their positive economic theory, a moral principle. Often the moral evaluation being employed is not that controversial but suppressing that there are always moral arguments at play has had the effect of creating misconceptions about the role of distributive justice arguments in economic decision-making.
For instance, the raising of interest rates is typically thought by economists to have the dual effects of reducing inflation and reducing employment from what they otherwise would be. Other things being equal, most people think that reducing inflation is morally good and reducing employment is morally bad (it makes more people unemployed or underemployed). To get to a recommendation that the Central Bank should reduce interest rates involves not only empirical views about the relative sizes of the inflation and unemployment effects and their long-term impact on growth, etc. but also normative views about the relative moral importance of inflation, employment and growth. For economists, these normative views on economic policies come under the rubric of ‘normative’ economics, while philosophers would typically categorize them under ‘distributive justice’. But the rubrics are not important as basically the same area is covered under different names — the normative evaluation of economic policies/structure/institutions. The evaluations often look different because economists most commonly use utility as their fundamental moral concept while philosophers use a wider variety of moral concepts, but the task in which they are both engaged is the same. What is most important to understand here is that positive economics alone cannot, without the guidance of normative principles, recommend which policies/structures/institutions to pursue. The arguments and principles discussed in what follows aim to supply this kind of normative guidance.
2. Strict Egalitarianism
One of the simplest principles of distributive justice is that of strict, or radical, equality. The principle says that every person should have the same level of material goods and services. The principle is most commonly justified on the grounds that people are morally equal and that equality in material goods and services is the best way to give effect to this moral ideal.
Even with this ostensibly simple principle some of the difficult specification problems of distributive principles can be seen. The two main problems are the construction of appropriate indices for measurement (the index problem), and the specification of time frames. Because there are numerous proposed solutions to these problems, the ‘principle of strict equality’ is not a single principle but a name for a group of closely related principles. This range of possible specifications occurs with all the common principles of distributive justice.
The index problem arises primarily because the goods to be distributed need to be measured if they are going to be distributed according to some pattern (such as equality). The strict equality principle stated above says that there should be ‘the same level of material goods and services’. The problem is how to specify and measure levels. One way of solving the index problem in the strict equality case is to specify that everyone should have the same bundle of material goods and services rather than the same level (so everyone would have 4 oranges, 6 apples, 1 bike, etc.). The main objection to this solution is that it appears likely that there will be many other allocations of material goods and services which will make some people better off without making anybody else worse off. Such allocations are what are called ‘Pareto superior’ allocations (see equality for a more detailed discussion of Pareto efficiency). For instance, someone who prefers apples to oranges will be better off if she swaps some of her oranges for some of the apples belonging to a person who prefers oranges. That way, they are both better off and no one is worse off. Indeed, since most everyone will wish to trade something, requiring identical bundles will make virtually everybody materially worse off than they would be under an alternative allocation. So specifying that everybody must have the same bundle of goods does not seem to be a satisfactory way of solving the index problem. Some index for measuring the value of goods and services is required.
Money is an index for the value of material goods and services. It is an imperfect index whose pitfalls are documented in most economics textbooks. Moreover, once the goods to be allocated are extended beyond material ones to include goods such as opportunities, money must be combined with other indices. (For instance, John Rawls' index of primary goods — see Rawls 1971.) Nevertheless, using money, either in the form of income or wealth or both, as an index for the value of material goods and services is the most common response so far suggested to the index problem and is widely used in the specification and implementation of distributive principles.
The second main specification problem involves time frames. Many distributive principles identify and require that a particular pattern of distribution be achieved or at least aimed at. But they also need to specify when the pattern is required. One version of the principle of strict equality requires that all people should have the same wealth at some initial point, after which people are free to use their wealth in whatever way they choose, with the consequence that future outcomes are bound to be unequal. Principles specifying initial distributions after which the pattern need not be preserved are commonly called ‘starting-gate’ principles. (See Ackerman 1980, 53–59,168–170,180–186; Alstott and Ackerman 1999.)
Because ‘starting-gate’ principles may eventually lead to large inequalities, strict egalitarians do not usually favor them. The most common form of strict equality principle specifies that income (measured in terms of money) should be equal in each time-frame, though even this may lead to significant disparities in wealth if variations in savings are permitted. Hence, strict equality principles are commonly conjoined with some society-wide specification of just saving behavior (see justice: intergenerational). In practice, however, this principle and the starting-gate version might require more similar distributions than it first appears. This is because the structure of the family means the requirement to give people equal starts will often necessitate redistribution to parents, who due to bad luck, bad management, or simply their own choices, have been unsuccessful in accruing or holding onto material goods.
There are a number of direct moral criticisms made of strict equality principles: that they unduly restrict freedom, that they do not give best effect to the moral equality of persons, that they conflict with what people deserve, etc. (see the sections on Libertarian Principles, and Desert-Based Principles, and the entry on equality). But the most common criticism is a welfare-based one related to the Pareto efficiency requirement: that everyone can be materially better off if incomes are not strictly equal (Carens 1981). It is this criticism which partly inspired the Difference Principle.
3. The Difference Principle
The wealth of an economy is not a fixed amount from one period to the next. More wealth can be produced and indeed this has been the overwhelming feature of industrialized countries over the last couple of centuries. The dominant economic view is that wealth is most readily increased in systems where those who are more productive earn greater incomes. This economic view partly inspired the formulation of the Difference Principle.
The most widely discussed theory of distributive justice in the past four decades has been that proposed by John Rawls in A Theory of Justice, (Rawls 1971), and Political Liberalism, (Rawls 1993). Rawls proposes the following two principles of justice:
1. Each person has an equal claim to a fully adequate scheme of equal basic rights and liberties, which scheme is compatible with the same scheme for all; and in this scheme the equal political liberties, and only those liberties, are to be guaranteed their fair value.
2. Social and economic inequalities are to satisfy two conditions: (a) They are to be attached to positions and offices open to all under conditions of fair equality of opportunity; and (b), they are to be to the greatest benefit of the least advantaged members of society. (Rawls 1993, pp. 5–6. The principles are numbered as they were in Rawls' original A Theory of Justice.)
For (2b) Rawls uses an ‘index of primary goods’ to measure the benefits of people for the purposes of the second principle. Where the rules may conflict in practice, Rawls says that Principle (1) has lexical priority over Principle (2), and Principle (2a) has lexical priority over (2b). As a consequence of the priority rules, Rawls' principles do not permit sacrifices to basic liberties in order to generate greater equality of opportunity or a higher level of material goods, even for the worst off. While it is possible to think of Principle (1) as governing the distribution of liberties, it is not commonly considered a principle of distributive justice given that it is not governing the distribution of economic goods per se. Equality of opportunity is discussed in the next section. In this section, the primary focus will be on (2b), known as the Difference Principle.
The main moral motivation for the Difference Principle is similar to that for strict equality: equal respect for persons. Indeed the Difference Principle materially collapses to a form of strict equality under empirical conditions where differences in income have no effect on the work incentive of people. The overwhelming economic opinion though is that in the foreseeable future the possibility of earning greater income will bring forth greater productive effort. This will increase the total wealth of the economy and, under the Difference Principle, the wealth of the least advantaged. Opinion divides on the size of the inequalities which would, as a matter of empirical fact, be allowed by the Difference Principle, and on how much better off the least advantaged would be under the Difference Principle than under a strict equality principle. Rawls' principle however gives fairly clear guidance on what type of arguments will count as justifications for inequality. Rawls is not opposed in principle to a system of strict equality per se; his concern is about the absolute position of the least advantaged group rather than their relative position. If a system of strict equality maximizes the absolute position of the least advantaged in society, then the Difference Principle advocates strict equality. If it is possible to raise the absolute position of the least advantaged further by having some inequalities of income and wealth, then the Difference Principle prescribes inequality up to that point where the absolute position of the least advantaged can no longer be raised.
Because there has been such extensive discussion of the Difference Principle in the last 40 years, there have been numerous criticisms of it from the perspective of all the other theories of distributive justice outlined here. Briefly, the main criticisms are as follows.
Advocates of strict equality argue that inequalities permitted by the Difference Principle are unacceptable even if they do benefit the absolute position of the least advantaged. The problem for these advocates has been to explain convincingly why society should be prevented from materially benefiting the least advantaged when this benefit requires a deviation from strict equality.
For the strict egalitarian the relative position of people is all important and the absolute position is either not important at all or lexically inferior. For Rawls, at least with respect to the social and economic inequalities, the opposite is true. But there have been various plausible explanations given in reply to Rawls' proposed Difference Principle why relative position is a value that should be weighed against the value of the absolute position of the least advantaged rather than subordinated lexically to it. In an early reply to Rawls, Crocker explains the value of paying attention to the relative position as a way of understanding the value of solidarity. His approach fits into a set of views in which being materially equal, or striving towards it, is an important expression of the equality of persons.
Another set of views, in opposition to Rawls' Difference Principle, emphasizes the importance of relative position not as a value in itself but because of its effect on other relations. In particular, if some people are significantly better off materially than others then that can result in them having significant power over others. Rawls' response to this criticism appeals to the priority of his first principle: The inequalities consistent with the Difference Principle are only permitted so long as they do not compromise the fair value of the political liberties. So, for instance, very large wealth differentials may make it practically impossible for poor people to be elected to political office or to have their political views represented. These inequalities of wealth, even if they increase the material position of the least advantaged group, may need to be reduced in order for the first principle to be implemented. However, while this provides a partial reply to Rawls' critics, it does not seem to recognize that it is not just differential political power that can come from significant differences in economic position but also economic power and hence economic freedom. Virtual monopoly employers in regions of developing economies give a stark illustration of this phenomenon. Of course, Rawls can appeal in such cases to the empirical claim that such differentials do not maximize the long-term position of the least advantaged. The empirical question will be whether all such large differentials which result in large differences in economic power also demonstrably have the result of worsening the absolute position of the least advantaged.
The utilitarian objection to the Difference Principle is that it does not maximize utility. In A Theory of Justice, Rawls uses utilitarianism as the main theory for comparison with his own, and hence he offers a number of arguments in response to this utilitarian objection, some of which are outlined in the section on Welfare-Based Principles.
Libertarians object that the Difference Principle involves unacceptable infringements on liberty, property rights, or self-ownership. For instance, the Difference Principle may require redistributive taxation for the benefit of the poor, and libertarians commonly object that such taxation involves the immoral taking of just holdings (see Libertarian Principles).
The Difference Principle is also criticized as a primary distributive principle on the grounds that it mostly ignores claims that people deserve certain economic benefits in light of their actions. Advocates of desert-based principles argue that some may deserve a higher level of material goods because of their hard work or contributions even if their unequal rewards do not also function to improve the position of the least advantaged. Desert theorists as well as libertarians also argue that the explanation of how people come to be in more or less advantaged positions is morally relevant to their fairness, yet the Difference Principle ignores these explanations.
Like desert theorists, advocates of Luck Egalitarian principles argue that the Difference Principle does not fully capture the moral roles they believe luck and responsibility should play in principles of distributive justice. Indeed,‘ luck egalitarianism’ as a distinct approach in the distributive justice literature really developed in critical response to Rawls' theory of distributive justice. The reasons for that response are outlined in the next section.
4. Equality of Opportunity and Luck Egalitarianism
The distribution of material goods and services is not the only economic distribution which is important to people. The distribution of opportunities is also important. As noted in the previous section, John Rawls conjoined his Difference Principle with a principle of equality of opportunity. Endorsement of some form of equality of opportunity is very prevalent among distributive justice theorists and, indeed, among the general population, especially when combined with some form of market distributive mechanism. Equality of opportunity is often contrasted favorably with ‘equality of outcome’ or strict egalitarianism, by those who believe that we can show equal concern, respect, or treatment of people without them having the same material goods and services, so long as they have equal economic opportunities. What is the morally best interpretation of this equality of opportunity principle has been a significant focus of research, particularly among luck egalitarians.
In 1988, Brian Barry gave an interesting reconstruction of the reasoning which led John Rawls to his Equal Opportunity and Difference Principles. Barry's reconstruction and Ronald Dworkin's earlier discussion (which we will come to later), have been seminal in the luck egalitarian literature. A version of this argument is probably the best introduction to some of the relevant moral issues.
‘Formal’ equality of opportunity rules out formal discrimination on grounds such as a person's race, ethnicity, age or gender. What is the underlying concern, shared by most theorists and the general population, with a society lacking formal equality of opportunity? The concern seems to be rooted in the belief that traits such as a persons' gender or race are elements over which people have no control and, hence, a society in which people's race or gender have fundamental effects on their lifetime economic prospects treats people unfairly. In such societies, whether people were born as the favored gender or race, and hence were favored economically, would simply be a matter of luck. Rawls' claim is that structuring a society so that this ‘natural lottery’ has such fundamental effects on people's lives is immoral, when we have the option to structure it another way, with a system of formal equality of opportunity.
The foregoing is relatively uncontroversial, but what made Rawls' (and Barry's) arguments so interesting was their claim that this line of reasoning actually leads to much stronger requirements for social justice. They note that even with formal equality of opportunity, there will remain many factors over which people have no control but which will affect their lifetime economic prospects, such as whether a person's family can afford to purchase good quality educational opportunities or health care. A society therefore will have reasons to adopt a more substantial equality of opportunity principle, with equal opportunities for education, health care, etc. — the same reasons it had for adopting a merely formal equality of opportunity principle.
Following this line of reasoning further (and it certainly has appeared to many that we have no principled reason to stop here) seems to lead to more radical conclusions than those who agreed with formal equality of opportunity would have imagined. A society with a more substantial equality of opportunity principle in place will still not be providing equality of opportunity for all. People are born into more or less nurturing families and social circumstances. People are born into families and neighborhoods which are more or less encouraging of education and the development of economically advantageous talents. There are a whole range of social influences which have fundamental and unequal effects on children's economic prospects and for which they are in no way responsible — the influences children are exposed to are a matter of their luck in the ‘social lottery’. Moreover, the luck of the natural lottery is not just restricted to such characteristics as gender and race. Children are more or less fortunate in the distribution of natural talents as well.
A race where the starting line is arbitrarily staggered, where people's prospects for winning are not largely determined by factors for which they are responsible but rather largely by luck, is not considered a fair race. Similarly, if society is structured so that people's prospects for gaining more economic goods are not largely determined by factors for which they are responsible but rather largely by luck, then the society is open to the charge of being unfair. This is the challenging conclusion with which Barry, following Rawls, presents us.
In response to this challenge, Barry himself explores a number of avenues, including questioning whether economic distribution is really analogous to a race. Rawls, of course, responded to his own challenge by arguing that there is not a lot that can be done (morally) to make the social and natural opportunities more equal, so the fair response is to adopt the Difference Principle. Others, however, have taken this challenge in different directions.
Ronald Dworkin, (Dworkin 1981a, 1981b, 2000) provided one of the most detailed early responses to Rawls' challenge. In retrospect, Dworkin's theory is often identified as one of the earliest in the luck egalitarian literature, though Dworkin himself called his theory Resource Egalitarianism. Dworkin presented his key insight (i.e., what distinguishes him from Rawls) in terms of a distinction between ‘ambitions’ and ‘endowments’. Dworkin uses the term ‘ambitions’ to cover the realm of our choices and what results from our choices, such as the choice to work hard, or to spend money on expensive luxuries. His term ‘endowments’ refers to the results of brute luck, or those things over which we have no control, such as one's genetic inheritance, or unforeseeable bad luck. Dworkin agrees with Rawls that natural inequalities are not distributed according to people's choices, nor are they justified by reference to some other morally relevant fact about people, so people should not end up worse off as a result of bad luck in the natural lottery. However, Dworkin argues the Difference Principle fails to deliver on this ideal, since its formulation in terms of primary goods fails to recognize that those who are very unlucky, such as the severely ill or disabled, may need considerably greater shares of primary goods than others in order to achieve a reasonable life. Dworkin also argued that just economic distributions should be more responsive than the Difference Principle to the consequences of people's choices.
Dworkin proposed that people begin with equal resources but be allowed to end up with unequal economic benefits as a result of their own choices. What constitutes a just material distribution is to be determined by the result of a thought experiment designed to model fair distribution. Suppose that everyone is given the same purchasing power and each uses that purchasing power to bid, in a fair auction, for resources best suited to their life plans. They are then permitted to use those resources as they see fit. Although people may end up with different economic benefits, none of them is given less consideration than another in the sense that if they wanted somebody else's resource bundle they could have bid for it instead.
In Dworkin's proposal we see his attitudes to ‘ambitions’ and ‘endowments’ which have become a central feature of luck egalitarianism (though under a wide variety of alternative names and further subset-distinctions). In terms of sensitivity to ‘ambitions’, Dworkin and many other luck egalitarians argue that provided people have an ‘equal’ starting point (in Dworkin's case, resources) they should live with the consequences of their choices. They argue, for instance, that people who choose to work hard to earn more income should not be required to subsidize those choosing more leisure and hence less income.
With respect to ‘endowments’, Dworkin proposes a hypothetical compensation scheme in which he supposes that, before the hypothetical auction described above, people do not know their own natural endowments. However, they are able to buy insurance against being disadvantaged in the natural distribution of talents and they know that their payments will provide an insurance pool to compensate those people who are unlucky in the ‘natural lottery’.
Dworkin's early proposals were very hypothetical and it was somewhat difficult to see what they meant in practice. Later luck egalitarians have tried to tease out the practical implications of their theories in more detail, though much of the debate still remains at the theoretical level. They agree with Dworkin's recommendation, against Rawls' Difference Principle approach, that those with unequal natural endowments should receive compensation. For instance, people born with handicaps, or ill-health, who have not brought these circumstances upon themselves, can be explicitly compensated so that they are not disadvantaged in their economic prospects. Under Rawls' Difference Principle, though, no such explicit compensation is forthcoming — as Rawls says, the Difference Principle is not the principle of redress (Rawls 1971, 101). Of course, for the subset of people with long-term handicaps or ill-health who are also in the least advantaged group (variously defined by Rawls, but most commonly defined as the lowest socio-economic grouping) the Difference Principle will help. But the help will not be proportionate to their needs arising from their handicaps or ill-health.
Luck egalitarians continue to refine such aspects of their theories as (a) what they believe is the relevant conception of equality of opportunity, (b) how much of a role luck should play in the distribution of economic benefits and (c) what is the best conception of 'luck' (Arneson 1990 and 2001, Fleurbaey 2001, Swift 2008, Sher 2010). Relatedly, they continue to explore what role responsibility should play in the distribution of economic goods (Sen 1985, Cohen 1997, Valentyne 1997, Knight 2011).
Because the luck egalitarian proposals have a similar motivation to the Difference Principle the moral criticisms of them tend to be variations on those leveled against the Difference Principle. However, as noted above, what is practically required of a society operating under the Difference Principle is relatively straightforward. How the theoretical concerns of luck egalitarians are to be practically implemented is often not so clear. For instance, it has seemed impossible to measure differences in people's natural talents — unfortunately, people's talents do not neatly divide into the natural and those for which people can be held responsible. A system of special assistance to the physically and mentally handicapped and to the ill would be a partial implementation of the compensation system, but most natural inequalities would be left untouched by such assistance while the theories commonly require compensation for such inequalities. Exploring how in practical ways the economic systems can be refined to track responsibility while mitigating certain types of pure luck will be an ongoing challenge for luck egalitarians.
5. Welfare-Based Principles
Welfare-based principles are motivated by the idea that what is of primary moral importance is the level of welfare of people. Advocates of welfare-based principles view the concerns of other theories — material equality, the level of primary goods of the least advantaged, resources, desert-claims, or liberty — as derivative concerns. They are only valuable in so far as they affect welfare, so that all distributive questions should be settled entirely by how the distribution affects welfare. However, there are many ways that welfare can be used in answering these distributive questions, so welfare-theorists need to specify what welfare function they believe should be maximized. The welfare functions proposed vary according to what will count as welfare and the weighting system for that welfare. Economists defending some form of welfarism normally state the explicit functional form, while philosophers often avoid this formality, concentrating on developing their theories in answer to two questions: 1) the question of what has intrinsic value, and 2) the question of what actions or policies would maximize the intrinsic value. Moreover, philosophers tend to restrict themselves to a small subset of the available welfare functions. Although there are a number of advocates of alternative welfare functions (such as ‘equality of well-being’), most philosophical activity has concentrated on a variant known as utilitarianism. This theory can be used to illustrate most of the main characteristics of welfare-based principles.
Historically, utilitarians have used the term ‘utility’ rather than ‘welfare’ and utility has been defined variously as pleasure, happiness, or preference-satisfaction. Jeremy Bentham, the historical father of utilitarianism, argued that the experience of pleasure was the only thing with intrinsic value, and all other things had intstrumental value insofar as they contribute to the experience of pleasure or the avoidance of pain. His student John Stuart Mill broadened this theory of intrinsic value to include happiness, or fulfillment. Modern philosophers since Kenneth Arrow, though, tend to argue that intrinsic value consists in preference-satisfaction, i.e. in individuals' having what they want. So, for instance, the principle for distributing economic benefits for preference utilitarians is to distribute them so as to maximize preference-satisfaction. The welfare function for such a principle has a relatively simple theoretical form which requires choosing the distribution which maximizes the arithmetic sum of all satisfied preferences (unsatisfied preferences being negative), weighted for the intensity of those preferences. To accommodate uncertainty with respect to outcomes the function can be modified so that expected utility, rather than utility, is maximized (see consequentialism).
The basic theory of utilitarianism is one of the simplest to state and understand. Much of the work on the theory therefore has been directed towards defending it against moral criticisms, particularly from the point of view of ‘commonsense’ morality. The criticisms and responses have been widely discussed in the literature on utilitarianism as a general moral theory (see consequentialism). Two of the most widely discussed criticisms will be mentioned here.
The first, which was famously articulated by John Rawls (1971), is that utilitarianism fails to take seriously the distinctness of persons. Maximization of preference-satisfaction is often taken as prudent in the case of individuals — people may take on greater burdens, suffering or sacrifice at certain periods of their lives so that their lives are overall better. The complaint against utilitarianism is that it takes this principle, commonly described as prudent for individuals, and uses it on an entity, society, unlike individuals in important ways. While it may be acceptable for a person to choose to suffer at some period in her life (be it a day, or a number of years) so that her overall life is better, it is often argued against utilitarianism that it is immoral to make some people suffer so that there is a net gain for other people. In the individual case, there is a single entity experiencing both the sacrifice and the gain. Also, the individuals, who suffer or make the sacrifices, choose to do so in order to gain some benefit they deem worth their sacrifice. In the case of society as a whole, there is no single experiential entity — some people suffer or are sacrificed so that others may gain. Furthermore, under utilitarianism, there is no requirement for people to consent to the suffering or sacrifice, nor is there necessarily a unified belief in the society that the outcome is worth the cost.
A related criticism of utilitarianism involves the way it treats individual preferences about other peoples' welfare or holdings. For instance, some people may have a preference that the members of some minority racial group have less material benefits. Under utilitarian theories, in their classical form, this preference or interest counts like any other in determining the best distribution. Hence, if racial preferences are widespread and are not outweighed by the minority's contrary preferences (perhaps because the minority is relatively few in number compared to the majority), utilitarianism will recommend an inegalitarian distribution based on race if there is not some other utility-maximizing alternative on offer.
utilitarians have responded to these criticisms in a number of ways. utilitarians may believe that even more welfare in the long run can be achieved by re-educating the majority so that racist preferences weaken or disappear over time, leading to a more harmonious and happier world. However, the utilitarian must supply an account of why racist or sexist preferences should be discouraged if the same level of total long term utility could be achieved by encouraging the less powerful to be content with a lower position. Utilitarians have also argued that the empirical conditions are such that utility maximizing will rarely require racial minorities to sacrifice or suffer for the benefit of others, or to satisfy the prejudices of others. But if their theory on rare occasions does require people sacrifice or suffer in these ways, utilitarians have defended this unintuitive consequence on the grounds that our judgments about what is wrong provide us with ‘rules of thumb’ which are useful at the level of commonsense morality but ultimately mistaken at the level of ‘critical theory’. More recently, some utilitarians have drawn on institutional theory or game theory in defence, or in modification, of utilitarianism (see Hardin 1988, Goodin 1995, Bailey 1997). Noting that the consequences of individual actions are rarely determined in isolation, but rather in conjunction with the actions of many others, these Institutional utilitarians argue that morally intuitive institutions such as constitutional rights, human rights and various property rights would be endorsed by utilitarianism, and would forbid the morally horrible outcomes critics have feared utilitarianism could sanction.
Utilitarian distribution principles, like the other principles described here, have problems with specification and implementation. Most formulations of utilitarianism require interpersonal comparisons of utility. This means, for instance, that we must be able to compare the utility one person gains from eating an apple with that another gains from eating an apple. Furthermore, utilitarianism requires that differences in utility be measured and summed for widely disparate goods (so, for instance, the amount of utility a particular person gains from playing football is measured and compared with the amount of utility another gains from eating a gourmet meal). Some critics have argued that such interpersonal utility comparisons are impossible, even in theory, because even if the already significant challenge of combining all the diverse goods into a single index of ‘utility’ could be met for an individual, there is no conceptually adequate way of calibrating such a measure among individuals (see Elster 1991).
Utilitarians face a greater problem than this theoretical one in determining what material distribution is prescribed by their theory. Those who share similar utilitarian theoretical principles frequently recommend very different material distributions to implement the principle. This problem occurs for other theories, but appears worse for utilitarian and welfare-based distribution principles. Recommendations for distributions or economic structures to implement other distributive principles commonly vary among advocates with similar theoretical principles, but the advocates tend to cluster around particular recommendations. This is not the case for utilitarianism, with adherents dispersed in their recommendations across the full range of possible distributions and economic structures. For instance, many preference utilitarians believe their principle prescribes strongly egalitarian structures with lots of state intervention while other preference utilitarians believe it prescribes a laissez faire style of capitalism.
There is an explanation for why utilitarians are faced with greater difficulties in implementation. Other distributive principles can rule out, relatively quickly, some practical policies on the grounds that they clearly violate the guiding principle, but utilitarians must examine, in great detail, all the policies on offer. For each policy, they must determine the distribution of goods and services yielded by the policy and at least three other factors: the identity of each person in the distribution (if individuals' utility functions differ); the utility of each person from the goods and services distributed to them; the utility of each person from the policy itself. The size of the information requirements make this task impossible. Hence, broad assumptions must be made and each different set of assumptions will yield a different answer, and so the answers range across the full set of policies on offer. Moreover, there is no obvious way to arbitrate between the different sets of assumptions. For instance, suppose three utilitarians agree on the same utilitarian distributive principle. Utilitarian 1, for example, may assert that the population's utility function conforms to function A (e.g. people's marginal utility is linear in the goods and services they consume) and is maximized by Policy 1; Utilitarian 2, however, asserts that half the population's utility function conforms to function A and half to function B (e.g. people's marginal utility is diminishing) and is maximized by Policy 2; Utilitarian 3, furthermore, asserts Utilitarian 2 is correct about the utility functions of the population but claims that Policy 3 will maximize utility. What seems impossible for advocates of utilitarian-distribution principles to answer is how we would arbitrate these claims. If utilitarian principles are to play a role in debates about distributive justice then this is the most important question to answer.
6. Desert-Based Principles
Another complaint against welfarism is that it ignores, and in fact cannot even make sense of, claims that people deserve certain economic benefits in light of their actions (Feinberg, Lamont 1997). The complaint is often motivated by the concern that various forms of welfarism treat people as mere containers for well-being, rather than purposeful beings, responsible for their actions and creative in their environments.
The different desert-based principles of distribution differ primarily according to what they identify as the basis for deserving. While Aristotle proposed virtue, or moral character, to be the best desert-basis for economic distribution, contemporary desert theorists have proposed desert-bases that are more practically implemented in complex modern societies. Most contemporary desert theorists have pursued John Locke's lead in this respect. Locke argued people deserve to have those items produced by their toil and industry, the products (or the value thereof) being a fitting reward for their effort (see Miller 1989). Locke's underlying idea was to guarantee to individuals the fruits of their own labor and abstinence. Most contemporary proposals for desert-bases fit into one of three broad categories:
Contribution: People should be rewarded for their work activity according to the value of their contribution to the social product. (Miller 1976, Miller 1989, Riley 1989)
Effort: People should be rewarded according to the effort they expend in their work activity (Sadurski 1985a,b, Milne 1986).
Compensation: People should be rewarded according to the costs they incur in their work activity (Dick 1975, Lamont 1997).
According to the contemporary desert theorist, people freely apply their abilities and talents, in varying degrees, to socially productive work. People come to deserve varying levels of income by providing goods and services desired by others (Feinberg 1970). Distributive systems are just insofar as they distribute incomes according to the different levels earned or deserved by the individuals in the society for their productive labors, efforts, or contributions.
Contemporary desert-principles all share the value of raising the standard of living — collectively, ‘the social product’. Under each principle, only activity directed at raising the social product will serve as a basis for deserving income. The concept of desert itself does not yield this value of raising the social product; it is a value societies hold independently. Hence, desert principles identifying desert-bases tied to socially productive activity (productivity, compensation, and effort all being examples of such bases) do not do so because the concept of desert requires this. They do so because societies value higher standards of living, and therefore choose the raising of living standards as the primary value relevant to desert-based distribution. This means that the full development of desert-based principles requires specification (and defense) of those activities which will or will not count as socially productive, and hence as deserving of remuneration (Lamont 1994).
It is important to distinguish desert-payments from entitlements. For desert theorists a well-designed institutional structure will make it so that many of the entitlements people have are deserved. But entitlements and just deserts can come apart. As Feinberg notes, a person can be entitled to assume the presidential office without deserving it (Feinberg 1970, 86) and a person who accidentally apprehends a criminal may be entitled to a reward but not deserve it. Conversely, a team may deserve to win the championship prize but not entitled to it or a person may deserve an economic benefit but not be entitled to it. These instances of desert and entitlements coming apart are typical fertile grounds for a desert theorist to argue for institutional reform.
Payments designed to give people incentives are a form of entitlement particularly worth distinguishing from desert-payments as they are commonly confused (Barry 1965, 111–112). Incentive-payments are ‘forward-looking’ in that they are set up to create a situation in the future, while desert-payments are ‘backwards-looking’in that they are justified with reference to work in the present or past. Even though it is possible for the same payment to be both deserved and an incentive, incentives and desert provide distinct rationales for income and should not be conflated (Lamont 1997).
While some have sought to justify current capitalist distributions via desert-based distributive principles, John Stuart Mill and many since have forcefully argued the contrary claim — that the implementation of a productivity principle would involve dramatic changes in modern market economies and would greatly reduce the inequalities characteristic of them. It is important to note, though, that contemporary desert-based principles are rarely complete distributive principles. They usually are only designed to cover distribution among working adults, leaving basic welfare needs to be met by other principles.
The specification and implementation problems for desert-based distribution principles revolve mainly around the desert-bases: it is difficult to identify what is to count as a contribution, an effort or a cost, and it is even more difficult to measure these in a complex modern economy.
The main moral objection to desert-based principles is that they make economic benefits depend on factors over which people have little control. John Rawls has made one of the most widely discussed arguments to this effect (Rawls 1971), and while the strong form of this argument has been clearly refuted (Zaitchik, Sher), it remains a problem for desert-based principles. The problem is most pronounced in the case of productivity-based principles — a person's productivity seems clearly to be influenced by many factors over which the person has little control.
It is interesting to note that under most welfare-based principles, it is also the case that people's level of economic benefits depends on factors beyond their control. But welfarists view this as a virtue of their theory, since they think the only morally relevant characteristic of any distribution is the welfare resulting from it. Whether the distribution ties economic benefits to matters beyond our control is morally irrelevant from the welfarist point of view. (As it happens, welfarists often hold the empirical claim that people have little control over their contributions to society anyway.) However, for people's benefits to depend on factors beyond their control is a more awkward result for desert theorists who emphasize the responsibility of people in choosing to engage in more or less productive activities.
7. Libertarian Principles
Most contemporary versions of the principles discussed so far allow some role for the market as a means of achieving the desired distributive pattern — the Difference Principle uses it as a means of helping the least advantaged; utilitarian principles commonly use it as a means of achieving the distributive pattern maximizing utility; desert-based principles rely on it to distribute goods according to desert, etc. In contrast, advocates of libertarian distributive principles rarely see the market as a means to some desired pattern, since the principle(s) they advocate do not ostensibly propose a ‘pattern’ at all, but instead describe the sorts of acquisitions or exchanges which are just in their own right. The market will be just, not as a means to some pattern, but insofar as the exchanges permitted in the market satisfy the conditions of just acquisition and exchange described by the principles. For libertarians, just outcomes are those arrived at by the separate just actions of individuals; a particular distributive pattern is not required for justice. Robert Nozick has advanced this version of libertarianism (Nozick 1974), and is its best known contemporary advocate.
Nozick proposes a 3-part “Entitlement Theory”.
If the world were wholly just, the following definition would exhaustively cover the subject of justice in holdings:
A person who acquires a holding in accordance with the principle of justice in acquisition is entitled to that holding.
A person who acquires a holding in accordance with the principle of justice in transfer, from someone else entitled to the holding, is entitled to the holding.
No one is entitled to a holding except by (repeated) applications of (a) and (b).
The complete principle of distributive justice would say simply that a distribution is just if everyone is entitled to the holdings they possess under the distribution (Nozick, p.151).
The statement of the Entitlement Theory includes reference to the principles of justice in acquisition and transfer. (For details of these principles see Nozick, pp.149–182.) The principle of justice in transfer is the least controversial and is designed to specify fair contracts while ruling out stealing, fraud, etc. The principle of justice in acquisition is more complicated and more controversial. The principle is meant to govern the gaining of exclusive property rights over the material world. For the justification of these rights, Nozick takes his inspiration from John Locke's idea that everyone ‘owns’ themselves and, by mixing one's labors with the world, self-ownership can generate ownership of some part of the material world. However, of Locke's mixing metaphor, Nozick legitimately asks: ‘...why isn't mixing what I own with what I don't own a way of losing what I own rather than a way of gaining what I don't? If I own a can of tomato juice and spill it in the sea so its molecules... mingle evenly throughout the sea, do I thereby come to own the sea, or have I foolishly dissipated my tomato juice?’ (Nozick 1974, p.174) Nozick concludes that what is significant about mixing our labor with the material world is that in doing so, we tend to increase the value of it, so that self-ownership can lead to ownership of the external world in such cases (Nozick 1974, pp. 149–182).
The obvious objection to this claim is that it is not clear why the first people to acquire some part of the material world should be able to exclude others from it (and, for instance, be the land owners while the later ones become the wage laborers). In response to this objection, Nozick follows Locke in recognizing the need for a qualification on just acquisition. According to the Lockean Proviso, an exclusive acquisition of the external world is just, if, after the acquisition, there is ‘enough and as good left in common for others’. One of the main challenges for libertarians has been to formulate a morally plausible interpretation of this proviso. According to Nozick's weaker version of Locke's Proviso, “a process normally giving rise to a permanent bequeathable property right in a previously unowned thing will not do so if the position of others no longer at liberty to use the thing is thereby worsened” (Nozick, 1974, p. 178). For Nozick's critics, his proviso is unacceptably weak. This is because it fails to consider the position others may have achieved under alternative distributions and thereby instantiates the morally dubious criterion of whoever is first gets the exclusive spoils. For example, one can satisfy Nozick's proviso by ‘acquiring’ a beach and charging $1 admission to those who previously were able to use the beach for free, so long as one compensates them with a benefit they deem equally valuable, such as a clean-up or life-guarding service on the beach. However, the beach-goers would have been even better off had the more efficient organizer among them acquired the beach, charging only 50 cents for the same service, but this alternative is never considered under Nozick's proviso (Cohen, 1995).
Will Kymlicka has given a summary of the steps in Nozick's self-ownership argument:
People own themselves.
The world is initially unowned.
You can acquire absolute rights over a disproportionate share of the world, if you do not worsen the condition of others.
It is relatively easy, without worsening the condition of others, to acquire absolute rights over a disproportionate share of the world. Therefore:
Once private property has been appropriated, a free market in capital and labor is morally required (Kymlicka 1990, p. 112).
The assessment of this argument is quite complex, but the difficulties mentioned above with the proviso call into question claims (3) and (4).
The challenge for libertarians then is to find a plausible reading of (3) which will yield (4). Moreover, Nozick extends the operation of the proviso to apply both to acquisitions and transfers, compounding the problem,(Nozick, 1974, p. 174).
Of course, many existing holdings are the result of acquisitions or transfers which at some point did not satisfy the principles of justice for acquisitions or transfers. Hence, Nozick must supplement those principles with a principle of rectification for past injustice. Although he does not specify this principle he does describe its purpose:
This principle uses historical information about previous situations and injustices done in them... and information about the actual course of events that flowed from these injustices, until the present, and it yields a description (or descriptions) of holdings in the society. The principle of rectification presumably will make use of its best estimate of subjunctive information about what would have occurred... if the injustice had not taken place. If the actual description of holdings turns out not to be one of the descriptions yielded by the principle, then one of the descriptions yielded must be realized. (Nozick 1974, pp. 152–153)
Nozick does not make an attempt to provide a principle of rectification. The absence of such a principle is much worse for a historical theory than for a patterned theory. Past injustices systematically undermine the justice of every subsequent distribution in historical theories. Nozick is clear that his historical theory cannot be used to evaluate the justice of actual societies until such a theory of rectification is given or no considerations of rectification of injustice could apply to justify the distribution in the actual society:
In the absence of [a full treatment of the principle of rectification] applied to a particular society, one cannot use the analysis and theory presented here to condemn any particular scheme of transfer payments, unless it is clear that no considerations of rectification of injustice could apply to justify it. (Nozick 1974, p.231)
Unfortunately for the theory, it would seem that no such treatment will ever be forthcoming because the task is, for all practical purposes, impossible. The numbers of injustices perpetrated throughout history, both within nations and between them, are enormous and the necessary details of the vast majority of injustices are unavailable. Even if the details of the injustices were available, the counterfactual causal chains could not be reliably determined. As Derek Parfit has pointed out, in a different context, even the people who would have been born would have been different (Parfit 1986). As a consequence, it is difficult to see how Nozick's entitlement theory could provide guidance as to what the current distribution of material holdings should be or what distributions or redistributions are legitimate or illegitimate. (Indeed Nozick suggests, for instance, the Difference Principle may be the best implementation of the principle of rectification.) Although Nozick is fairly candid about this consequence, many of his supporters and critics have ignored it and have carried on a vigorous debate as though, contrary to Nozick's own statement, his theory can be used to evaluate the justice of current economic distributions.
Libertarians usually advocate a system in which there are exclusive property rights, with the role of the government restricted to the protection of these property rights. The property rights commonly rule out taxation for purposes other than raising the funds necessary to protect property rights. One of the strongest critiques of any attempt to institute such a system of legally protected strong property rights comes, as we have seen, from Nozick's theory itself — there seems no obvious reason to give strong legal protection to property rights which have arisen through violations of the just principles of acquisition and transfer. But putting this critique to one side for a moment, what other arguments are made in favor of exclusionary property rights?
As already noted, Nozick argues that because people own themselves and hence their talents, they own whatever they can produce with these talents. Moreover, it is possible in a free market to sell the products of exercising one's talents. Any taxation of the income from such selling, according to Nozick, ‘institute[s] (partial) ownership by others of people and their actions and labor’ (Nozick, p. 172). People, according to this argument, have these exclusive rights of ownership. Taxation then, simply involves violating these rights and allowing some people to own (partially) other people. Moreover, it is argued, any system not legally recognizing these rights violates Kant's maxim to treat people always as ends in themselves and never merely as a means. The two main difficulties with this argument have been: (1) to show that self-ownership is only compatible with having such strong exclusive property rights; and (2) that a system of exclusive property rights is the best system for treating people with respect, as ends in themselves.
Nozick candidly accepts that he does not himself give a systematic moral justification of the exclusionary property rights he advocates: ‘This book does not present a precise theory of the moral basis of individual rights.’ (Nozick, p.xiv) But others have tried to provide more systematic justifications of similar rights (Lomasky, Steiner) or to develop, more fully, justifications to which Nozick alludes.
In addition to the arguments from self-ownership, and the requirement to treat people as ends in themselves, the most common other route for trying to justify exclusive property rights has been to argue that they are required for the maximization of freedom and/or liberty or the minimization of violations of these (Hayek 1960). As an empirical claim though, this appears to be false. If we compare countries with less exclusionary property rights (e.g. more taxation) with countries with more exclusionary property regimes, we see no systematic advantage in freedoms/liberties enjoyed by people in the latter countries. (Of course, we do see a difference in distribution of such freedoms/liberties in the latter countries, the richer have more and the poorer less, while in the former they are more evenly distributed.) Now if libertarians restrict what counts as a valuable freedom/liberty (and discount other freedoms/liberties people value), it will follow that exclusionary property rights are required to maximize freedom/liberty or to minimize violations of these. But the challenge for these libertarians is to show why only their favored liberties and freedoms are valuable, and not those which are weakened by a system of exclusive property rights.
8. Feminist Principles
There is no one feminist conception of distributive justice; theorists who name themselves feminists defend positions across the political spectrum. Hence, feminists offer distinctive versions of all the theories considered so far as well as others. One way of thinking about what unifies many feminist theorists is an interest in what difference, if any, the practical experience of gender makes to the subject matter or study of justice; how different feminists answer this question distinguishes them from each other and from those alternative distributive principles that most inspire their thinking.
The distributive principles so far outlined, with the exception of strict egalitarianism, are often described as falling under the broad classification of liberalism — they both inform, and are the product of, the liberal democracies which have emerged over the last two centuries. Lumping them together this way, though clumsy, makes the task of understanding the emergence of feminist critiques (and the subsequent positive theories) much easier.
John Stuart Mill in The Subjection of Women (1869) gives one of the clearest early feminist critiques of the political and distributive structures of the emerging liberal democracies. His writings provide the starting point for many contemporary liberal feminists. Mill argued that the principles associated with the developing liberalism of his time required equal political status for women. The principles Mill explicitly mentions include a rejection of the aristocracy of birth, equal opportunity in education and in the marketplace, equal rights to hold property, a rejection of the man as the legal head of the household, and equal rights to political participation. Feminists who follow Mill believe that a proper recognition of the position of women in society requires that women be given equal and the same rights as men have, and that these primarily protect their liberty and their status as equal persons under the law. Thus, government regulation should not prevent women from competing on equal terms with men in educational, professional, marketplace and political institutions. From the point of view of other feminisms, the liberal feminist position is a conservative one, in the sense that it requires the proper inclusion for women of the rights, protections, and opportunities previously secured for men, rather than a fundamental change to the traditional liberal position. The problem for women, on this view, is not liberalism but the failure of society and the State to properly instantiate liberal principles.
One phrase or motto around which a whole range of feminists have rallied, however, marks a significant break with Mill's liberalism: ‘the personal is political.’ Feminists have offered a variety of interpretations of this motto, many of which take the form of a critique of liberal theories. Mill was crucial in developing the liberal doctrine of limiting the state's intervention in the private lives of citizens. Many contemporary feminists have argued that the resulting liberal theories of justice have fundamentally been unable to accommodate the injustices that have their origins in this ‘protected’ private sphere. This particular feminist critique has also been a primary source of inspiration for the broader multicultural critique of liberalism. The liberal commitments to government neutrality and to a protected personal sphere of liberty, where the government must not interfere, have been primary critical targets.
While issues about neutrality and personal liberty go beyond debates about distributive justice they also have application within these debates. The feminist critics recognize that liberalism correctly identifies the government as one potential source of oppression against individuals, and therefore recommends powerful political protections of individual liberty. They argue, however, that liberal theories of distributive justice are unable to address the oppression which surfaces in the so-called private sphere of government non-interference. Susan Moller Okin, for example, documents the effects of the institution of the nuclear family, arguing that the consequence of this institution is a position of systematic material and political inequality for women. Standard liberal theories, committed to neutrality in the private sphere, seem powerless to address (or sometimes even recognize) striking and lasting inequalities for women, minorities, or historically oppressed racial groups, when these are merely the cumulative effect of individuals' free behavior. Okin and others demonstrate, for example, that women have substantial disadvantages in competing in the market because of childrearing responsibilities which are not equally shared with men. As a consequence, any theory relying on market mechanisms, including most liberal theories, will yield systems which result in women systematically having less income and wealth than men. Thus, feminists have challenged contemporary political theorists to rethink the boundaries of political authority in the name of securing a just outcome for women and other historically oppressed groups.
While the political effects of personal freedom pose a serious challenge to contemporary liberal theories of distributive justice, the feminist critiques are somewhat puzzling because, as Jean Hampton puts it, many feminists appear to complain in the name of liberal values. In other words, their claims about the fundamental flaws of liberalism at the same time leave in tact the various ideals of liberty and equality which inspire the liberal theories of justice. Moreover, the task of defining feasible pathways for modifying the structure of liberal democracies without undermining their virtues and protections has proved more difficult than setting out the criticisms of liberalism. Indeed, despite a legitimate feminist worry about the effects of so-called government neutrality on women's material status, the relative neutrality of liberal democracies compared to non-liberal societies has been one of the significant contributing factors both to the flourishing of feminist theory and to the many significant practical gains women in liberal democracies have made relative to women in other parts of the world. The challenge, being taken up by many, is to navigate both a coherent theoretical and practical path in response to the best feminist critiques available (see the entry on feminist ethics).
9. Methodology and Empirical Beliefs about Distributive Justice
How are we to go about choosing between the different distributive principles on offer, and respond to criticisms of the principles? Unfortunately, few philosophers explicitly discuss the methodology they are using. The most notable exception is John Rawls (1971, 1974) who explicitly brought the method of wide reflective equilibrium to political philosophy. This method has been brilliantly discussed by Norman Daniels over the years and the reader is strongly encouraged to refer to his entry (see reflective equilibrium) to understand how to evaluate, revise and choose between normative principles. While there is no point in reiterating the method here there are some supplementary issues worth noting.
Empirical data on the beliefs of the population about distributive justice was not available when Rawls published A Theory of Justice (Rawls 1971) but much empirical work has since been completed. Swift (1995, 1999) and Miller (1999, chaps. 3–4) have provided surveys of this literature and arguments for why those committed to the method of reflective equilibrium in distributive justice should take the beliefs of the population seriously, though not uncritically. Indeed, some go even further, arguing that the distributive decisions arising through the legitimate application of particular democratic processes might even, at least in part, constitute distributive justice (Walzer 1984). Data on people's beliefs about distributive justice is also useful for addressing the necessary intersection between philosophical and political processes. Such beliefs put constraints on what institutional and policy reforms are practically achievable in any generation — especially when the society is committed to democratic processes.
Two final methodological issues need to be noted. The first concerns the distinctive role counterexamples play in debates about distributive justice. As noted above, the overarching methodological concern of the distributive justice literature must be, in the first instance, the pressing choice of how the benefits and burdens of economic activity should be distributed, rather than the mere uncovering of abstract truth. Principles are to be implemented in real societies with the problems and constraints inherent in such application. Given this, pointing out that the application of any particular principle will have some, perhaps many, immoral results will not by itself constitute a fatal counterexample to any distributive theory. Such counter-evidence to a theory would only be fatal if there were an alternative, or improved, version of the theory, which, if fully implemented, would yield a morally preferable society overall. So, it is at least possible that the best distributive theory, when implemented, might yield a system which still has many injustices and/or negative consequences. This practical aspect partly distinguishes the role of counterexamples in distributive justice theory from many other philosophical areas. Given that distributive justice is about what to do now, not just what to think, alternate distributive theories must, in part, compete as comprehensive systems which take into account the practical constraints we face.
The second and related methodological point is that the evaluation of alternate distributive principles requires us (and their advocates) to consider the application of the distributive principle in the world. If it is uncertain or indeterminate how a particular distributive principle might in practice apply to the ordering of real societies, then this principle is not yet a serious candidate for our consideration. This is also true of principles whose implementation is practically impossible given the institutional/psychological/informational/administrative/technical constraints of a society. Distributive justice is not an area where we can say an idea is good in theory but not in practice. If it is not good in practice, then it is not good in theory either.
